{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).\n",
    "\n",
    "Category: Prototype based, Density based, Hierarchical, Graph-based \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "##### K-Mean Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Mean++ Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Mode Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Median Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Medoids Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Medoids Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Hierarchy Clustering (Agglomerative) Bottom up - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Hierarchy Clustering (Divisive) Top Down - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Fuzzy Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### DBSCAN Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### OPTICS Clustering - \n",
    "Pros: Doesn’t assume cluster’s shape; Allow ‘noise’ which doesn’t belong to any cluster (robust); identify clusters with different density (DBSCAN can’t); \n",
    "\n",
    "Cons: More complicate so slower than DBSCAN; \n",
    "\n",
    "##### Expectation Maximization (EM) - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Non Negative Matrix Factorization - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Latent Dirichlet Allocation (LDA) - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- K-Mean Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n",
    "\n",
    "Lloyd -> Given any set of k centers Z, for each center z in Z, let V(z) denote its neighborhood. That is the set of data points for which z is the nearest neighbor. Each stage of Lloyd's algorithm moves every center point z to the centroid of V(z) and then updates V(z) by recomputing the distance from each point to its nearest center. These steps are repeated until convergence. Note that Lloyd's algorithm can get stuck in locally minimal solutions that are far from the optimal. For this reason it is common to consider heuristics based on local search, in which centers are swapped in and out of an existing solution (typically at random). Such a swap is accepted only if it decreases the average distortion, otherwise it is ignored.\n",
    "\n",
    "Forgy -> Forgy's algorithm is a simple alternating least-squares algorithm consisting of the following steps:\n",
    "Initialize the codebook vectors. (Suppose that when processing a given training case, N cases have been previously assigned to the winning codebook vector.)\n",
    "Repeat the following two steps until convergence:\n",
    "Read the data, assigning each case to the nearest (using Euclidean distance) codebook vector.\n",
    "Replace each codebook vector with the mean of the cases that were assigned to it.\n",
    "\n",
    "MacQueen -> This algorithm works by repeatedly moving all cluster centers to the mean of their respective Voronoi sets.\n",
    "\n",
    "Hartigan and Wong -> Given n objects with p variables measured on each object x(i,j) for i = 1,2,...,n; j = 1,2,...,p; K-means allocates each object to one of K groups or clusters to minimize the within-cluster sum of squares:\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "Given fixed centroids, it minizes the distance between in-cluster Xs and centroids by choosing the cluster labels for each X.(Repeat after each movement of centroids)\n",
    "#### Process Flow: \n",
    "Initiate K centroids, assign Xs(observations) to the closest(distance metric) centroid. Then, calculate the Avg(x) for each cluster which grouped by the initial assignment. Use the Avg(x) as the new position of the centroids. Repeating this process.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "data <- iris[, 3:4]\n",
    "set.seed(20)\n",
    "kmean.cluster <- kmeans(data,# - numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns). \n",
    "                        centers=3,# - either the number of clusters, say k, or a set of initial (distinct) cluster centres. If a number, a random set of (distinct) rows in x is chosen as the initial centres. \n",
    "                        iter.max = 300,# - the maximum number of iterations allowed. \n",
    "                        nstart = 1,# - if centers is a number, how many random sets should be chosen? \n",
    "                        algorithm = c(\"Hartigan-Wong\", \"Lloyd\", \"Forgy\", \"MacQueen\"), \n",
    "                        trace=FALSE)# - only used in the default method (\"Hartigan-Wong\"): if positive (or true), tracing information on the progress of the algorithm is produced.\n",
    "\n",
    "kmean.cluster$cluster # A vector of integers (from 1:k) indicating the cluster to which each point is allocated.\n",
    "kmean.cluster$centers # A matrix of cluster centres.\n",
    "kmean.cluster$totss # The total sum of squares.\n",
    "kmean.cluster$withinss # Vector of within-cluster sum of squares, one component per cluster\n",
    "kmean.cluster$tot.withinss # Total within-cluster sum of squares, i.e. sum(withinss).\n",
    "kmean.cluster$betweenss # The between-cluster sum of squares, i.e. totss-tot.withinss.\n",
    "kmean.cluster$size # The number of points in each cluster.\n",
    "kmean.cluster$iter # The number of (outer) iterations.\n",
    "kmean.cluster$ifault # integer: indicator of a possible algorithm problem – for experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------- Python Code\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = KMeans(n_clusters=3, # k\n",
    "            init='random', # initate points\n",
    "            n_init=10, # runs to choose lowest SSE #\n",
    "            max_iter=300, # iters in each run\n",
    "            tol=1e-04, # define converge\n",
    "            random_state=0)\n",
    "Y_km = km.fit_predict(X) # arrary of labels of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Mean ++ Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In classic k-means algorithm that uses a random seed to place the initial centroids, which can sometimes result in bad clusterings or slow convergence if the initial centroids are choosen poorly. So, in K-Mean++ clustering, it places the initial controids far aways from each others. The k-means++ algorithm addresses the second of these obstacles by specifying a procedure to initialize the cluster centers before proceeding with the standard k-means optimization iterations. With the k-means++ initialization, the algorithm is guaranteed to find a solution that is O(log k) competitive to the optimal k-means solution.\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "Given fixed centroids, it minizes the distance between in-cluster Xs and centroids by choosing the cluster labels for each X.(Repeat after each movement of centroids)\n",
    "#### Process Flow: \n",
    "Initiate K centroids randomly, assign Xs(observations) to the closest(distance metric) centroid. Find the minimum squared distance between (X,centroid) for each centroid. Randomly select the next centroid using a weighted probability (one minium dist / sum of all minium dists). Repeat the process until k centroids choosen (create k centroids which are far aways from each other) --> Proceed to the classic K-mean process... \n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = KMeans(n_clusters=3, # k\n",
    "            init='k-means++', # initiate points far away from each other\n",
    "            n_init=10, # runs to choose lowest SSE #\n",
    "            max_iter=300, # iters in each run\n",
    "            tol=1e-04, # define converge\n",
    "            random_state=0)\n",
    "Y_km = km.fit_predict(X) # arrary of labels of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Mode Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "K-modes, an algorithm extending the k-means paradigm to categorical domain is introduced ??. New dissimilarity measures to deal with categorical data is conducted to replace means with modes, and a frequency based method is used to update modes in the clustering process to minimize the clustering cost function.\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.packages(\"klaR\")\n",
    "library(klaR)\n",
    "install.packages(\"vcd\")\n",
    "library(vcd)\n",
    "\n",
    "data <- as.data.frame(Arthritis)\n",
    "data <- as.matrix(data, ncol=5, nrow=84)\n",
    "\n",
    "kmode.cluster <- kmodes(data,# - A matrix or data frame of categorical data. Objects have to be in rows, variables in columns.\n",
    "                        modes=3,# - Either the number of modes or a set of initial (distinct) cluster modes. If a number, a random set of (distinct) rows in data is chosen as the initial modes.\n",
    "                        iter.max = 10,# - The maximum number of iterations allowed.\n",
    "                        weighted = FALSE)# - Whether usual simple-matching distance between objects is used, or a weighted version of this distance.\n",
    "\n",
    "kmode.cluster$cluster # A vector of integers indicating the cluster to which each object is allocated\n",
    "kmode.cluster$size # The number of objects in each cluster.\n",
    "kmode.cluster$modes # A matrix of cluster modes.\n",
    "kmode.cluster$withindiff # The within-cluster simple-matching distance for each cluster.\n",
    "kmode.cluster$iterations # The number of iterations the algorithm has run.\n",
    "kmode.cluster$weighted # Whether weighted distances were used or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "pip install kmodes\n",
    "pip install --upgrade kmodes\n",
    "\n",
    "# random categorical data\n",
    "data = np.random.choice(20, (100, 10))\n",
    "\n",
    "km = kmodes.KModes(n_clusters=4, init='Huang', n_init=5, verbose=1)\n",
    "\n",
    "clusters = km.fit_predict(data)\n",
    "\n",
    "# Print the cluster centroids\n",
    "print(km.cluster_centroids_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Medians Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the square of the 2-norm distance metric (which k-means does.)\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# pip install pyclustering\n",
    "\n",
    "import pyclustering\n",
    "from pyclustering.utils import read_sample;\n",
    "# load list of points for cluster analysis\n",
    "data = ; \n",
    "# create instance of K-Medians algorithm\n",
    "kmedians_instance = kmedians(data, # (list): Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "                             [ [0.0, 0.1], [2.5, 2.6] ], # (list): Initial coordinates of medians of clusters that are represented by list: [center1, center2, ...].\n",
    "                            tolerance = 0.25, # (double): Stop condition: if maximum value of change of centers of clusters is less than tolerance than algorithm will stop processing\n",
    "                            ccore = False); # (bool): Defines should be CCORE library (C++ pyclustering library) used instead of Python code or not.\n",
    " \n",
    "# run cluster analysis and obtain results\n",
    "kmedians_instance.process();\n",
    "kmedians_instance.get_clusters(); \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Medoids Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary metrics of distances between datapoints. k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.packages(\"cluster\")\n",
    "library(cluster)\n",
    "\n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/pam.html\n",
    "kmedoids <- pam(x,\n",
    "                k,\n",
    "                diss = inherits(x, \"dist\"),\n",
    "                metric = \"euclidean\", \n",
    "                medoids = NULL, \n",
    "                stand = FALSE, \n",
    "                cluster.only = FALSE,\n",
    "                do.swap = TRUE,\n",
    "                keep.diss = !diss && !cluster.only && n < 100,\n",
    "                keep.data = !diss && !cluster.only,\n",
    "                pamonce = FALSE, \n",
    "                trace.lev = 0)\n",
    "\n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clara.html\n",
    "# Handle large dataset\n",
    "kmedoids <- clara(x, \n",
    "                  k, \n",
    "                  metric = \"euclidean\", \n",
    "                  stand = FALSE, \n",
    "                  samples = 5,\n",
    "                  sampsize = min(n, 40 + 2 * k), \n",
    "                  trace = 0, \n",
    "                  medoids.x = TRUE,\n",
    "                  keep.data = medoids.x, \n",
    "                  rngR = FALSE, \n",
    "                  pamLike = FALSE, \n",
    "                  correct.d = TRUE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# load list of points for cluster analysis\n",
    "data = read_sample(path);\n",
    " \n",
    "# create instance of K-Medoids algorithm\n",
    "kmedians_instance = kmedians(data, # (list): Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "                             [1, 10], # (list): Indexes of intial medoids (indexes of points in input data).\n",
    "                             tolerance = 0.25, # (double): Stop condition: if maximum value of distance change of medoids of clusters is less than tolerance than algorithm will stop processing.\n",
    "                             ccore = False ); # (bool): If specified than CCORE library (C++ pyclustering library) is used for clustering instead of Python code.\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedians_instance.process();\n",
    "kmedians_instance.get_clusters();  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Hierarchy Clustering (Agglomerative) Bottom up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root) (see figure below). The result is a tree which can be plotted as a dendrogram.\n",
    "\n",
    "In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical) ~ Distance Metric: Euclidean distance, Squared Euclidean distance, Manhattan distance, maximum distance, Mahalanobis distance, Hamming distance(cate), Levenshtein distance(cate).\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "Maximum or complete linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters.\n",
    "\n",
    "Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, “loose” clusters.\n",
    "\n",
    "Mean or average linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters.\n",
    "\n",
    "Centroid linkage clustering: It computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.\n",
    "\n",
    "Ward’s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.\n",
    "#### Process Flow: \n",
    "Created distance matrix by choosing distance metrics, then choose a linkage method to start 'merge' or 'dive' from top down or down top. When done, define the 'cut' for the number of groups\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# - http://www.sthda.com/english/wiki/hierarchical-clustering-essentials-unsupervised-machine-learning\n",
    "data(\"USArrests\")\n",
    "df <- na.omit(USArrests)\n",
    "df <- scale(df)\n",
    "# Dissimilarity matrix\n",
    "d <- dist(df, method = \"euclidean\")\n",
    "\n",
    "# - hclust package\n",
    "# Hierarchical clustering using Ward's method\n",
    "# - http://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html\n",
    "hier.cluster <- hclust(d, method = \"ward.D2\" )\n",
    "# Plot the obtained dendrogram\n",
    "plot(res.hc, cex = 0.6, hang = -1)\n",
    "# Cut tree into 4 groups\n",
    "grp <- cutree(res.hc, k = 4)\n",
    "# Number of members in each cluster\n",
    "table(grp)\n",
    "# Get the names for the members of cluster 1\n",
    "rownames(df)[grp == 1]\n",
    "# Plot dendrogram with color boxes cover the 4 groups\n",
    "plot(res.hc, cex = 0.6)\n",
    "rect.hclust(res.hc, k = 4, border = 2:5)# border control color, 2,3,4,5\n",
    "# plot 'cut tree' again 1,2nd pca\n",
    "library(\"factoextra\")\n",
    "fviz_cluster(list(data = df, cluster = grp))\n",
    "\n",
    "\n",
    "# - agnes packages \n",
    "library(\"cluster\")\n",
    "# Compute agnes()\n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/agnes.html\n",
    "res.agnes <- agnes(df, method = \"ward\")\n",
    "# Plot the tree using pltree()\n",
    "pltree(res.agnes, cex = 0.6, hang = -1,\n",
    "       main = \"Dendrogram of agnes\") \n",
    "# plot.hclust() - convert obj using plot as above\n",
    "plot(as.hclust(res.agnes), cex = 0.6, hang = -1)\n",
    "# Agglomerative coefficient; amount of clustering structure found\n",
    "res.agnes$ac\n",
    "# Cut agnes() tree into 4 groups\n",
    "cutree(res.agnes, k = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "import sklearn.cluster\n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "AgglomerativeClustering_instance = AgglomerativeClustering(n_clusters=2, \n",
    "                                                           affinity='euclidean', \n",
    "                                                           memory=Memory(cachedir=None), \n",
    "                                                           connectivity=None, \n",
    "                                                           compute_full_tree='auto', \n",
    "                                                           linkage='ward', \n",
    "                                                           pooling_func=<function mean>)\n",
    "# Fit the hierarchical clustering on the data\n",
    "AgglomerativeClustering_instance.fit(x) # X : array-like, shape = [n_samples, n_features]\n",
    "# Performs clustering on X and returns cluster labels.\n",
    "AgglomerativeClustering_instance.fit_predict(X, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Hierarchy Clustering (Divisive) Top Down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Divisive hierarchical clustering: It’s also known as DIANA (Divise Analysis) and it works in a top-down manner. The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster (see figure below).\n",
    "\n",
    "In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical) ~ Distance Metric: Euclidean distance, Squared Euclidean distance, Manhattan distance, maximum distance, Mahalanobis distance, Hamming distance(cate), Levenshtein distance(cate).\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "Created distance matrix by choosing distance metrics, then choose a linkage method to start 'merge' or 'dive' from top down or down top. When done, define the 'cut' for the number of groups\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# - http://www.sthda.com/english/wiki/hierarchical-clustering-essentials-unsupervised-machine-learning\n",
    "data(\"USArrests\")\n",
    "df <- na.omit(USArrests)\n",
    "df <- scale(df)\n",
    "# Dissimilarity matrix\n",
    "d <- dist(df, method = \"euclidean\")\n",
    "\n",
    "# - diana packages \n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/diana.html\n",
    "library(\"cluster\")\n",
    "diana(df, metric = \"euclidean\", stand = FALSE)\n",
    "# Plot the tree\n",
    "pltree(res.diana, cex = 0.6, hang = -1,\n",
    "       main = \"Dendrogram of diana\")\n",
    "# plot.hclust()\n",
    "plot(as.hclust(res.diana), cex = 0.6, hang = -1)\n",
    "# Divise coefficient; amount of clustering structure found\n",
    "res.diana$dc\n",
    "# Cut diana() tree into 4 groups\n",
    "cutree(as.hclust(res.diana), k = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Fuzzy Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "Fuzzy clustering (also referred to as soft clustering) is a form of clustering in which each data point can belong to more than one cluster. In non-fuzzy clustering (also known as hard clustering), data is divided into distinct clusters, where each data point can only belong to exactly one cluster. In fuzzy clustering, data points can potentially belong to multiple clusters. Membership grades are assigned to each of the data points. These membership grades indicate the degree to which data points belong to each cluster. Thus, points on the edge of a cluster, with lower membership grades, may be in the cluster to a lesser degree than points in the center of cluster.\n",
    "\n",
    "K-means clustering also attempts to minimize the objective function shown above. This method differs from the k-means objective function by the addition of the membership values {\\displaystyle w_{ij}} {\\displaystyle w_{ij}} and the fuzzifier, {\\displaystyle m\\in R} {\\displaystyle m\\in R} , with {\\displaystyle m\\geq 1} {\\displaystyle m\\geq 1}. The fuzzifier {\\displaystyle m} m determines the level of cluster fuzziness. A large {\\displaystyle m} m results in smaller membership values, {\\displaystyle w_{ij}} {\\displaystyle w_{ij}}, and hence, fuzzier clusters. In the limit {\\displaystyle m=1} {\\displaystyle m=1}, the memberships, {\\displaystyle w_{ij}} {\\displaystyle w_{ij}} , converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge, {\\displaystyle m} m is commonly set to 2. The algorithm minimizes intra-cluster variance as well, but has the same problems as k-means; the minimum is a local minimum, and the results depend on the initial choice of weights.\n",
    "#### Input Data: \n",
    "X(Numeric) \n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "The fuzzy c-means algorithm is very similar to the k-means algorithm: Choose a number of clusters, Assign randomly to each point coefficients for being in the clusters, Repeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than 'threshold' , the given sensitivity threshold) :\n",
    "[1] Compute the centroid for each cluster (shown below). [2] For each point, compute its coefficients of being in the clusters.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# - http://pro1.unibz.it/projects/Clustering_Methods_2014/Ferraro.pdf\n",
    "\n",
    "install.packages(\"fclust\")\n",
    "library(fclust)\n",
    "## McDonald's data\n",
    "data(Mc)\n",
    "names(Mc)\n",
    "## data normalization by dividing the nutrition facts by the Serving Size (column 1)\n",
    "for (j in 2:(ncol(Mc)-1))\n",
    "  Mc[,j]=Mc[,j]/Mc[,1]\n",
    "## removing the column Serving Size\n",
    "Mc=Mc[,-1]\n",
    "\n",
    "## - Fuzzy k-means (excluded the factor column Type (last column))\n",
    "# - https://www.rdocumentation.org/packages/fclust/versions/1.1.2/topics/FKM\n",
    "clust=FKM(Mc[,1:(ncol(Mc)-1)],k=6,m=1.5,stand=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1] It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.[2]\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "MinPts (controls minimum size of a cluster, low – like linkage, high – all noise, expect # for one cluster, like 10, 20, 500)\n",
    "\n",
    "Radius (the radius of the distance comparsion) Kind of subjective to define\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "In DBSCAN, a special label is assigned as core point if at least a specified number (MinPts) of neighboring points fall within the specified radius:\n",
    "[Core point] - if at least a specified number of (MinPts) of beighboring points fall within the specified radius\n",
    "[Border point] - has fewer neighbors than (MinPts) with radius but lies within the radius of a core point\n",
    "[Noise points] - Niether Core points or Border points\n",
    "After labeling the points as core, border, or noise points, the DBSCAN can be summarized:\n",
    "1.Form a separate cluster for each core point or a connected group of core points(core points are connected if they no farther away than radius\n",
    "2.Assign each border point to the cluster of its corresponding core point.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Does not assume the clusters have a spherical shape, removing noise points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.apckages(\"dbscan\")\n",
    "library(dbscan)\n",
    "# - https://cran.r-project.org/web/packages/dbscan/dbscan.pdf\n",
    "dbscan.cluster <- dbscan(x, eps, minPts = 5, weights = NULL,\n",
    "                         borderPoints = TRUE, search = \"kdtree\", bucketSize = 10,\n",
    "                         splitRule = \"suggest\", approx = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Fitting the model\n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- OPTICS Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2] Its basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. In order to do so, the points of the database are (linearly) ordered such that points which are spatially closest become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that needs to be accepted for a cluster in order to have both points belong to the same cluster.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters:\n",
    "MinPts (controls minimum size of a cluster, low – like linkage, high – all noise, expect # for one cluster, like 10, 20, 500) \n",
    "#### Cost Function:\n",
    "#### Process Flow: \n",
    "#-http://fogo.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf\n",
    "\n",
    "Like DBSCAN, OPTICS requires two parameters: ε, which describes the maximum distance (radius) to consider, and MinPts, describing the number of points required to form a cluster. A point p is a core point if at least MinPts points are found within its ε -neighborhood N ε   ( p )   Contrary to DBSCAN, OPTICS also considers points that are part of a more densely packed cluster, so each point is assigned a core distance that describes the distance to the MinPtsth closest point. The reachability-distance of another point o from a point p is either the distance between o and p, or the core distance of p, whichever is bigger. The basic idea to overcome these problems is to run an algorithm which produces a special order of the database with respect to its density-based clustering structure containing the information about every clustering level of the data set (up to a “generating distance” e), and is very easy to analyze.\n",
    "#### Evaluation Methods:\n",
    "#### Tips:\n",
    "Does not assume the clusters have a spherical shape, removing noise points, it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# - https://cran.r-project.org/web/packages/dbscan/dbscan.pdf\n",
    "Install.packages(“dbscan”)\n",
    "Library(dbscan)\n",
    "Optics.cluster <- optics(x, \n",
    "                         eps, \n",
    "                         minPts = 5, \n",
    "                         eps_ci, xi, \n",
    "                         search = “kdtree”, \n",
    "                         bucketSize = 10, \n",
    "                         splitRule = “suggest”, \n",
    "                         approx. = 0)\n",
    "Optics_cut(x, eps_ci); \n",
    "opticsXi(object, xi = 0.001, minimum = F, nocorrect = F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# - http://pythonhosted.org/pyclustering/classpyclustering_1_1cluster_1_1optics_1_1optics.html\n",
    "# Read sample for clustering from some file\n",
    "sample = read_sample(path_sample);\n",
    "# Create OPTICS algorithm for cluster analysis\n",
    "optics_instance = optics(sample, 0.5, 6);\n",
    "# Run cluster analysis\n",
    "optics_instance.process();\n",
    "# Obtain results of clustering\n",
    "clusters = optics_instance.get_clusters();\n",
    "noise = optics_instance.get_noise();\n",
    "# Obtain rechability-distances\n",
    "ordering = optics_instance.get_cluster_ordering();\n",
    "# Visualization of cluster ordering in line with reachability distance.\n",
    "indexes = [i for i in range(0, len(ordering))];\n",
    "plt.bar(indexes, ordering);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Non Negative Matrix Factorization (NMF) - Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "#- http://web.stanford.edu/group/mmds/slides2012/s-park.pdf\n",
    "Non-negative Matrix Factorization, a technique which makes use of an algorithm based on decomposition by parts of an extensive data matrix into a small number of relevant metagenes. NMF’s ability to identify expression patterns and make class discoveries has been shown to able to have greater robustness over popular clustering techniques such as HCL and SOM. Non-negative matrix factorization (NMF) finds a small number of metagenes, each defined as a positive linear combination of the genes in the expression data. NMF is most frequently used to make class discoveries through identification of molecular patterns. The module can also be used to cluster genes, generating metasamples rather than metagenes.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters:\n",
    "K_Genre(K dimensions in factorization) \n",
    "#### Cost Function:\n",
    "#### Process Flow:\n",
    "#- http://www.cc.gatech.edu/~hpark/papers/GT-CSE-08-01.pdf\n",
    "MeV’s NMF uses a multiplicative update algorithm, introduced by Lee and Seung in 2001, to factor a non-negative data matrix into two factor matrices referred to as W and H. Associated with each factorization is a user-specified rank. This represents the columns in W, the rows in H, and the number of clusters to which the samples are to be assigned. Starting with randomly seeded matrices and using an iterative approach with a specified cost measurement we can reach a locally optimal solution for these factor matrices. H and W can then be evaluated as metagenes and metagenes expression patterns, respectively. Using a “winner-take-all” approach, samples can be assigned to clusters based on their highest metagenes expression. Multiple iterations of this process allow us to see the robustness of the cluster memberships. Additionally, running multiple ranks consecutively can allow for the comparison between differing numbers of classes using cophenetic correlation. \n",
    "#### Evaluation Methods:\n",
    "#### Tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# - https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf\n",
    "Install.packages(“NMF”)\n",
    "Library(NMF)\n",
    "method  <- nmfAlgorithm(“brunet”) # choose algorithm\n",
    "seed <- nmfSeed(“nndsvd”) # choose a seeding method\n",
    "nmf.cluster <- nmf(x, \n",
    "                   rank, \n",
    "                   method, \n",
    "                   seed) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "import numpy as np\n",
    "X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "model.fit(X) \n",
    "\n",
    "NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,\n",
    "  n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,\n",
    "  solver='cd', sparseness=None, tol=0.0001, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Latent Dirichlet Allocation (LDA) - Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Expectation Maximization (EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
