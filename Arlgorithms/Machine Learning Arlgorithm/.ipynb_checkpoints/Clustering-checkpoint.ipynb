{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).\n",
    "\n",
    "Category: Prototype based, Density based, Hierarchical\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "##### K-Mean Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Mean++ Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Mode Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Median Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Medoids Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### K-Medoids Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Hierarchy Clustering (Agglomerative) Bottom up - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Hierarchy Clustering (Divisive) Top Down - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Fuzzy Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### DBSCAN Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### OPTICS Clustering - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Expectation Maximization (EM) - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Non Negative Matrix Factorization - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "##### Latent Dirichlet Allocation (LDA) - Pros: xxxx | Cons: xxxx\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- K-Mean Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n",
    "\n",
    "Lloyd -> Given any set of k centers Z, for each center z in Z, let V(z) denote its neighborhood. That is the set of data points for which z is the nearest neighbor. Each stage of Lloyd's algorithm moves every center point z to the centroid of V(z) and then updates V(z) by recomputing the distance from each point to its nearest center. These steps are repeated until convergence. Note that Lloyd's algorithm can get stuck in locally minimal solutions that are far from the optimal. For this reason it is common to consider heuristics based on local search, in which centers are swapped in and out of an existing solution (typically at random). Such a swap is accepted only if it decreases the average distortion, otherwise it is ignored.\n",
    "\n",
    "Forgy -> Forgy's algorithm is a simple alternating least-squares algorithm consisting of the following steps:\n",
    "Initialize the codebook vectors. (Suppose that when processing a given training case, N cases have been previously assigned to the winning codebook vector.)\n",
    "Repeat the following two steps until convergence:\n",
    "Read the data, assigning each case to the nearest (using Euclidean distance) codebook vector.\n",
    "Replace each codebook vector with the mean of the cases that were assigned to it.\n",
    "\n",
    "MacQueen -> This algorithm works by repeatedly moving all cluster centers to the mean of their respective Voronoi sets.\n",
    "\n",
    "Hartigan and Wong -> Given n objects with p variables measured on each object x(i,j) for i = 1,2,...,n; j = 1,2,...,p; K-means allocates each object to one of K groups or clusters to minimize the within-cluster sum of squares:\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "Given fixed centroids, it minizes the distance between in-cluster Xs and centroids by choosing the cluster labels for each X.(Repeat after each movement of centroids)\n",
    "#### Process Flow: \n",
    "Initiate K centroids, assign Xs(observations) to the closest(distance metric) centroid. Then, calculate the Avg(x) for each cluster which grouped by the initial assignment. Use the Avg(x) as the new position of the centroids. Repeating this process.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "data <- iris[, 3:4]\n",
    "set.seed(20)\n",
    "kmean.cluster <- kmeans(data,# - numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns). \n",
    "                        centers=3,# - either the number of clusters, say k, or a set of initial (distinct) cluster centres. If a number, a random set of (distinct) rows in x is chosen as the initial centres. \n",
    "                        iter.max = 300,# - the maximum number of iterations allowed. \n",
    "                        nstart = 1,# - if centers is a number, how many random sets should be chosen? \n",
    "                        algorithm = c(\"Hartigan-Wong\", \"Lloyd\", \"Forgy\", \"MacQueen\"), \n",
    "                        trace=FALSE)# - only used in the default method (\"Hartigan-Wong\"): if positive (or true), tracing information on the progress of the algorithm is produced.\n",
    "\n",
    "kmean.cluster$cluster # A vector of integers (from 1:k) indicating the cluster to which each point is allocated.\n",
    "kmean.cluster$centers # A matrix of cluster centres.\n",
    "kmean.cluster$totss # The total sum of squares.\n",
    "kmean.cluster$withinss # Vector of within-cluster sum of squares, one component per cluster\n",
    "kmean.cluster$tot.withinss # Total within-cluster sum of squares, i.e. sum(withinss).\n",
    "kmean.cluster$betweenss # The between-cluster sum of squares, i.e. totss-tot.withinss.\n",
    "kmean.cluster$size # The number of points in each cluster.\n",
    "kmean.cluster$iter # The number of (outer) iterations.\n",
    "kmean.cluster$ifault # integer: indicator of a possible algorithm problem â€“ for experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------- Python Code\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = KMeans(n_clusters=3, # k\n",
    "            init='random', # initate points\n",
    "            n_init=10, # runs to choose lowest SSE #\n",
    "            max_iter=300, # iters in each run\n",
    "            tol=1e-04, # define converge\n",
    "            random_state=0)\n",
    "Y_km = km.fit_predict(X) # arrary of labels of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Mean ++ Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In classic k-means algorithm that uses a random seed to place the initial centroids, which can sometimes result in bad clusterings or slow convergence if the initial centroids are choosen poorly. So, in K-Mean++ clustering, it places the initial controids far aways from each others. The k-means++ algorithm addresses the second of these obstacles by specifying a procedure to initialize the cluster centers before proceeding with the standard k-means optimization iterations. With the k-means++ initialization, the algorithm is guaranteed to find a solution that is O(log k) competitive to the optimal k-means solution.\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "Given fixed centroids, it minizes the distance between in-cluster Xs and centroids by choosing the cluster labels for each X.(Repeat after each movement of centroids)\n",
    "#### Process Flow: \n",
    "Initiate K centroids randomly, assign Xs(observations) to the closest(distance metric) centroid. Find the minimum squared distance between (X,centroid) for each centroid. Randomly select the next centroid using a weighted probability (one minium dist / sum of all minium dists). Repeat the process until k centroids choosen (create k centroids which are far aways from each other) --> Proceed to the classic K-mean process... \n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = KMeans(n_clusters=3, # k\n",
    "            init='k-means++', # initiate points far away from each other\n",
    "            n_init=10, # runs to choose lowest SSE #\n",
    "            max_iter=300, # iters in each run\n",
    "            tol=1e-04, # define converge\n",
    "            random_state=0)\n",
    "Y_km = km.fit_predict(X) # arrary of labels of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Mode Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "K-modes, an algorithm extending the k-means paradigm to categorical domain is introduced ??. New dissimilarity measures to deal with categorical data is conducted to replace means with modes, and a frequency based method is used to update modes in the clustering process to minimize the clustering cost function.\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.packages(\"klaR\")\n",
    "library(klaR)\n",
    "install.packages(\"vcd\")\n",
    "library(vcd)\n",
    "\n",
    "data <- as.data.frame(Arthritis)\n",
    "data <- as.matrix(data, ncol=5, nrow=84)\n",
    "\n",
    "kmode.cluster <- kmodes(data,# - A matrix or data frame of categorical data. Objects have to be in rows, variables in columns.\n",
    "                        modes=3,# - Either the number of modes or a set of initial (distinct) cluster modes. If a number, a random set of (distinct) rows in data is chosen as the initial modes.\n",
    "                        iter.max = 10,# - The maximum number of iterations allowed.\n",
    "                        weighted = FALSE)# - Whether usual simple-matching distance between objects is used, or a weighted version of this distance.\n",
    "\n",
    "kmode.cluster$cluster # A vector of integers indicating the cluster to which each object is allocated\n",
    "kmode.cluster$size # The number of objects in each cluster.\n",
    "kmode.cluster$modes # A matrix of cluster modes.\n",
    "kmode.cluster$withindiff # The within-cluster simple-matching distance for each cluster.\n",
    "kmode.cluster$iterations # The number of iterations the algorithm has run.\n",
    "kmode.cluster$weighted # Whether weighted distances were used or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "pip install kmodes\n",
    "pip install --upgrade kmodes\n",
    "\n",
    "# random categorical data\n",
    "data = np.random.choice(20, (100, 10))\n",
    "\n",
    "km = kmodes.KModes(n_clusters=4, init='Huang', n_init=5, verbose=1)\n",
    "\n",
    "clusters = km.fit_predict(data)\n",
    "\n",
    "# Print the cluster centroids\n",
    "print(km.cluster_centroids_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Medians Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the square of the 2-norm distance metric (which k-means does.)\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# pip install pyclustering\n",
    "\n",
    "import pyclustering\n",
    "from pyclustering.utils import read_sample;\n",
    "# load list of points for cluster analysis\n",
    "data = ; \n",
    "# create instance of K-Medians algorithm\n",
    "kmedians_instance = kmedians(data, # (list): Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "                             [ [0.0, 0.1], [2.5, 2.6] ], # (list): Initial coordinates of medians of clusters that are represented by list: [center1, center2, ...].\n",
    "                            tolerance = 0.25, # (double): Stop condition: if maximum value of change of centers of clusters is less than tolerance than algorithm will stop processing\n",
    "                            ccore = False); # (bool): Defines should be CCORE library (C++ pyclustering library) used instead of Python code or not.\n",
    " \n",
    "# run cluster analysis and obtain results\n",
    "kmedians_instance.process();\n",
    "kmedians_instance.get_clusters(); \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Medoids Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary metrics of distances between datapoints. k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.packages(\"cluster\")\n",
    "library(cluster)\n",
    "\n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/pam.html\n",
    "kmedoids <- pam(x,# data matrix or data frame, or dissimilarity matrix or object, depending on the value of the diss argument.\n",
    "                k,# positive integer specifying the number of clusters, less than the number of observations. \n",
    "                diss = inherits(x, \"dist\"),# logical flag: if TRUE (default for dist or dissimilarity objects), then x will be considered as a dissimilarity matrix. If FALSE, then x will be considered as a matrix of observations by variables.\n",
    "                metric = \"euclidean\", \n",
    "                medoids = NULL, \n",
    "                stand = FALSE, \n",
    "                cluster.only = FALSE,\n",
    "                do.swap = TRUE,\n",
    "                keep.diss = !diss && !cluster.only && n < 100,\n",
    "                keep.data = !diss && !cluster.only,\n",
    "                pamonce = FALSE, \n",
    "                trace.lev = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# load list of points for cluster analysis\n",
    "data = read_sample(path);\n",
    " \n",
    "# create instance of K-Medoids algorithm\n",
    "kmedians_instance = kmedians(data, # (list): Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "                             [1, 10], # (list): Indexes of intial medoids (indexes of points in input data).\n",
    "                             tolerance = 0.25, # (double): Stop condition: if maximum value of distance change of medoids of clusters is less than tolerance than algorithm will stop processing.\n",
    "                             ccore = False ); # (bool): If specified than CCORE library (C++ pyclustering library) is used for clustering instead of Python code.\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedians_instance.process();\n",
    "kmedians_instance.get_clusters();  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Hierarchy Clustering (Agglomerative) Bottom up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Agglomerative: This is a \"bottom up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "\n",
    "Divisive: This is a \"top down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical) ~ Distance Metric: Euclidean distance, Squared Euclidean distance, Manhattan distance, maximum distance, Mahalanobis distance, Hamming distance(cate), Levenshtein distance(cate).\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "Linkage ~ Minimum or single-linkage clustering, Maximum or complete-linkage clustering, Mean or average linkage clustering, or UPGMA, Centroid linkage clustering, or UPGMC, Minimum energy clustering\n",
    "#### Process Flow: \n",
    "Created distance matrix by choosing distance metrics, then choose a linkage method to start 'merge' or 'dive' from top down or down top. When done, define the 'cut' for the number of groups\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Hierarchy Clustering (Divisive) Top Down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Agglomerative: This is a \"bottom up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "\n",
    "Divisive: This is a \"top down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical) ~ Distance Metric: Euclidean distance, Squared Euclidean distance, Manhattan distance, maximum distance, Mahalanobis distance, Hamming distance(cate), Levenshtein distance(cate).\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "Linkage ~ Minimum or single-linkage clustering, Maximum or complete-linkage clustering, Mean or average linkage clustering, or UPGMA, Centroid linkage clustering, or UPGMC, Minimum energy clustering\n",
    "#### Process Flow: \n",
    "Created distance matrix by choosing distance metrics, then choose a linkage method to start 'merge' or 'dive' from top down or down top. When done, define the 'cut' for the number of groups\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Fuzzy Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- OPTICS Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Non Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Expectation Maximization (EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
