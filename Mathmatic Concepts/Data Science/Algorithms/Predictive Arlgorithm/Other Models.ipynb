{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Models\n",
    "Other Un-categorized Models ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Support Vector Machine (SVM)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Good for text categorization since can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings; Good for image classification; Accuracy; Works well on smaller cleaner datasets; It can be more efficient because it uses a subset of training points;\n",
    "\n",
    "Cons: Isn’t suited to larger datasets as the training time with SVMs can be high; Less effective on noisier datasets with overlapping classes; \n",
    "\n",
    "\n",
    "\n",
    "#### Page Rank\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes. SVMs are more commonly used in classification problems. SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes, as shown in the image below. \n",
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set.\n",
    "\n",
    "- What is a hyperplane? \n",
    "\n",
    "As a simple example, for a classification task with only two features (like the image above), you can think of a hyperplane as a line that linearly separates and classifies a set of data. Intuitively, the further from the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it. So when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it.\n",
    "\n",
    "- How do we find the right hyperplane?\n",
    "\n",
    "Or, in other words, how do we best segregate the two classes within the data? The distance between the hyperplane and the nearest data point from either set is known as the margin. The goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set, giving a greater chance of new data being classified correctly.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical) – Y(Categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "Parameter (Cost) controls the tolerance of misclassification;\n",
    "\n",
    "Choice of ‘kernal’ function to transform the data – “linear”, “polynomial”, “radial”;\n",
    "#### Cost Function: \n",
    "Maximize the total margin (perpendicular distance) from observations (from both sides) to the hyper-plan \n",
    "#### Process Flow: \n",
    "- Kernel transform\n",
    "\n",
    "X ~ {X1i, X2i, … , Xpi} with i in n -> Choose a kernel – K(xi, xj) to transform X -> XT {…}\n",
    "Y ~ {A,B,B, …, A} with total n\n",
    "\n",
    "- Estimate parameters\n",
    "\n",
    "Since Yi(beta0 + beta1XT1i + … + betapXTpi) > 0, margin positive, = 0, on the hyper-plan;\n",
    "\n",
    "Maximize -> Margin(1-error i) =< Yi(beta0 + beta1XT1i + … + betapXTpi) \n",
    "\n",
    " Subject to -> SUM(beta1-p^2) = 1, error i >= 0, SUM(error 1-n) =< C (Tuning parameter 0 – infinite)\n",
    " \n",
    "Solve -> [beta0, beta1, …, beta p, error 1, error 2, …, error n]\n",
    "\n",
    "*Kernel – linear -> nonlinear classification; *C – 0: maximal margin classifier (variance), large: svm (Bias) \n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n",
    "# https://cran.r-project.org/web/packages/e1071/e1071.pdf\n",
    "\n",
    "Install.packages(“e1071”)\n",
    "Library(e1071)\n",
    "Set.seed(1)\n",
    "\n",
    "X <- matrix(rnorm(20 * 2), ncol=2)\n",
    "Y <- c(rep(-1, 10), rep(1,10))\n",
    "X[y==1,] <- X[Y==1,] + 1\n",
    "Plot(X, col=(3-Y))\n",
    "\n",
    "Data.class <- data.frame(x=X, y=as.factor(Y))\n",
    "Table.svm <- svm(y~. , data=Data.class, kernel=”linear”, cost=10, scale=FALSE) # kernel: polynomial, radial\n",
    "Summary(Table.svm); \n",
    "\n",
    "Table.tune.cv <- tune(svm, y~. , data=Data.class, kernel=”radial”, ranges=list(cost=c(1,2,3,4,6), gamma=c(0.1,0.2)))\n",
    "Summary(Table.tune.cv); best.mod <- table.tune.cv$best.model; summary(best.mod)\n",
    "Svm.perdict <- predict(best.mod, test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------- Python\n",
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "# load lib {numpy, sklearn}\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# loading datasets\n",
    "isir = datasets.load_iris()\n",
    "X = iris.data[:,[2,3]]\n",
    "y = iris.target\n",
    "\n",
    "# Spliting datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler() # - define scaler object\n",
    "sc.fit(X_train) # fit the object with data to get meansure\n",
    "X_train_std = sc.transform(X_train) # scale data\n",
    "X_test_std = sc.transform(X_test) # scale data\n",
    "\n",
    "# fitting the model\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=0) # linear kernel SVM / 'C' torlent to misclassification\n",
    "svm = SVC(kernel='rbf', C=10.0, random_state=0, gamma=0.10) # Radius kernel SVM / 'C' torlent to misclassification / gamma: large over fit, small under fit\n",
    "\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "# SGD implementation \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svm = SGDClassifier(loss='hinge') # ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- Page Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "\n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "\n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
