{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Distance Metric Learning \n",
    "\n",
    "\n",
    "### Distance Between Data Points \n",
    "A metric or distance function is a function d(x,y) that defines the distance between elements of a set as a non-negative real number. If the distance is zero, both elements are equivalent under that specific metric. Distance functions thus provide a way to measure how close two elements are, where elements do not have to be numbers but can also be vectors, matrices or arbitrary objects. Distance functions are often used as error or cost functions to be minimized in an optimization problem. L1 - |x-y| ; L2 – (x-y)^2\n",
    "\n",
    "Sum of Absolute Difference (SAD) \n",
    "\n",
    "Sun of Squared Difference (SSD)\n",
    "\n",
    "Mean-Absolute Error (MAE) \n",
    "\n",
    "Mean-Squared Error (MSE)\n",
    "\n",
    "Euclidean / Euclidean Squared Distance\n",
    "\n",
    "Manhattan (City-block, Taxicab) Distance \n",
    "\n",
    "Pearson Correlation Distance \n",
    "\n",
    "Pearson Squared Distance \n",
    "\n",
    "Chebychev Distance (Chessboard Distance) \n",
    "\n",
    "Spearman Distance\n",
    "\n",
    "Minkowski Distance (Family of Distance by L1, L2)\n",
    "\n",
    "Canberra Distance\n",
    "\n",
    "Cosine Distance\n",
    "\n",
    "Hamming Distance\n",
    "\n",
    "Steinhaus Coefficients\n",
    "\n",
    "Mismatch Value\n",
    "\n",
    "Jaccard distance\n",
    "\n",
    "Sørensen–Dice coefficient \n",
    "\n",
    "\n",
    "### Distance Between Clusters \n",
    "\n",
    "Single Linkage: The single linkage is good at handling non-elliptical shapes, but is sensitive to noise and outliers.\n",
    "\n",
    "Complete Linkage: Complete linkage is less susceptible to noise and outlier, but it can break large clusters and it favers globular shapes.\n",
    "\n",
    "Group Average Linkage: Group Average linkage is an intermediate approach between the single and complete approaches.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Type Of Clustering \n",
    "\n",
    "Hierarchical vs Partitional: Nested subsections / one dimensional separated sub-sections \n",
    "\n",
    "Exclusive vs Overlapping vs Fuzzy: Exclusive memberships / multiple memberships / all memberships\n",
    "\n",
    "Complete vs Partial: All data assigned memberships / Allow outliers no membership assigned\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Type Of Clusters (Data) \n",
    "\n",
    "Well-Separated: A cluster is a set of objects in which each object is closer to every other object in the cluster than any object not in the cluster. \n",
    "\n",
    "Prototype-Based: A cluster is a set of objects in which each object is closer to the prototype (Centroid) that defines the cluster than the prototype of any other cluster.\n",
    "\n",
    "Graph-Based: If the data is represented as a graph, where the nodes are objects and the links represents connections among objects, then a cluster can be defined as connected component.\n",
    "\n",
    "Density-Based: A cluster is a dense region of objects that is surrounded by a region of low density.\n",
    "\n",
    "Shared-Property (Conceptual Cluster): A cluster of objects that share some property which may encompresses all the previous definitions of a cluster.\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "### Model Pros & Cons\n",
    "\n",
    "##### K-Mean Clustering - \n",
    "\n",
    "Pros: Simple and fast (Good for large data);  \n",
    "\n",
    "Cons: Start randomly so result not consistent; Requires prior knowledge of K; Require to choose good initial centroids; Very sensitive to outliers/noises; Assume Cluster shape is spherical; \n",
    "##### K-Mean++ Clustering - \n",
    "\n",
    "Pros: Simple and fast (Good for large data); assume Cluster shape is spherical; Choose centroids far away(more table result); Resolve outliers issues; \n",
    "\n",
    "Cons: Requires prior knowledge of K; Assume Cluster shape is spherical; \n",
    "##### X-Mean Clustering - \n",
    "\n",
    "Pros: Simple and fast (Good for large data); Auto-select K; \n",
    "\n",
    "Cons: Start randomly so result not consistent; Require to choose good initial centroids; Very sensitive to outliers/noises; Assume Cluster shape is spherical; \n",
    "##### K-Mode Clustering - \n",
    "\n",
    "Pros: Simple and fast (Good for large data); Handle categorical data;  \n",
    "\n",
    "Cons: Start randomly so result not consistent; Requires prior knowledge of K; Require to choose good initial centroids; Very sensitive to outliers/noises; Assume Cluster shape is spherical; \n",
    "##### K-Median Clustering - \n",
    "\n",
    "Pros: Simple and fast (Good for large data); Less sensitive to outlier/noise; This makes the algorithm more reliable for discrete or even binary data sets. \n",
    "\n",
    "Cons: Start randomly so result not consistent; Requires prior knowledge of K; Require to choose good initial centroids; Assume Cluster shape is spherical; \n",
    "##### K-Medoids Clustering - \n",
    "\n",
    "Pros: Simple and fast (Good for large data); Robust to outliers; \n",
    "\n",
    "Cons: Start randomly so result not consistent; Requires prior knowledge of K; Require to choose good initial centroids; Assume Cluster shape is spherical; \n",
    "##### Hierarchy Clustering (Agglomerative) Bottom up - \n",
    "\n",
    "Pros: Good on small data; Consistent results; Provide multiple cluster number levels same time; Handle clusters with arbitrary shapes; No assumption regarding # clusters needed; Good performance when data is in a meaningful taxonomy; Robust on local minimum; No problem with initial points choices;\n",
    "\n",
    "Cons: Slow or failed on large data; The fact that all merges are final can also cause trouble for noisy, high-dimensional data; [Can somehow solve by apply partial clustering with K-mean before load] \n",
    "##### Hierarchy Clustering (Divisive) Top Down - \n",
    "\n",
    "Pros: Good on small data; Consistent results; Provide multiple cluster number levels same time; Handle clusters with arbitrary shapes; No assumption regarding # clusters needed; More efficient if we do not generate a complete hierarchy all the way down to individual document leaves; Top-down clustering benefits from complete information about the global distribution when making top-level partitioning decisions; Good performance when data is in a meaningful taxonomy; Robust on local minimum; No problem with initial points choices;\n",
    "\n",
    "Cons: Slow or failed on large data; \n",
    "##### Fuzzy Clustering - \n",
    "\n",
    "Pros: Allow mulitiple clusters assignments on one which is more real (Usually involves multiple factors);  \n",
    "\n",
    "Cons: Need to define C the number of clusters; Need to determine memebership cut-off value; Need to choose good initial centroids;\n",
    "##### DBSCAN Clustering - \n",
    "\n",
    "Pros: Handle clusters with arbitrary shapes; Robust to outliers; No assumption regarding # clusters needed; Consistent result; \n",
    "\n",
    "Cons: Require sufficient high density (No Sparse data points); Data sets with varying densities are problematics; Slow on large data; Not good with high-dimensional data;\n",
    "##### OPTICS Clustering - \n",
    "\n",
    "Pros: Allow ‘noise’ which doesn’t belong to any cluster (robust); identify clusters with different density (DBSCAN can’t); Handle clusters with arbitrary shapes; No assumption regarding # clusters needed; Consistent result;\n",
    "\n",
    "Cons: More complicate so slower than DBSCAN; More slow on large data;\n",
    "##### Expectation Maximization (EM) - \n",
    "\n",
    "Pros: Handle clusters may be overlapping areas; each point gives p() distribution of memberships for each group rather than the exact assignment [Soft Clustering];  \n",
    "\n",
    "Cons: Assume elliptical shape of cluster (Multi-Gaussian) which still more flexible than K-mean; \n",
    "##### Non Negative Matrix Factorization - \n",
    "\n",
    "Pros: Good for topic modeling; Good if topic probabilities should remain fixed per document (oftentimes unlikely)—or in small data settings in which the additional variability coming from the hyperpriors is too much; \n",
    "\n",
    "Cons: Less stable than LDA\n",
    "##### Latent Dirichlet Allocation (LDA) - \n",
    "\n",
    "Pros: Good for topic modeling; Good in identifying coherent topics where as NMF usually gives incoherent topics;  \n",
    "\n",
    "Cons: Makes proior assumptions as Dirichlet distribution; \n",
    "##### Affinity Propagation (AP) - \n",
    "\n",
    "Pros: xxxx \n",
    "\n",
    "Cons: xxxx\n",
    "##### Mean Shift Clustering - \n",
    "\n",
    "Pros: xxxx \n",
    "\n",
    "Cons: xxxx\n",
    "\n",
    "##### Spectral Clustering - \n",
    "\n",
    "Pros: xxxx \n",
    "\n",
    "Cons: xxxx\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- K-Mean Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n",
    "\n",
    "Lloyd -> Given any set of k centers Z, for each center z in Z, let V(z) denote its neighborhood. That is the set of data points for which z is the nearest neighbor. Each stage of Lloyd's algorithm moves every center point z to the centroid of V(z) and then updates V(z) by recomputing the distance from each point to its nearest center. These steps are repeated until convergence. Note that Lloyd's algorithm can get stuck in locally minimal solutions that are far from the optimal. For this reason it is common to consider heuristics based on local search, in which centers are swapped in and out of an existing solution (typically at random). Such a swap is accepted only if it decreases the average distortion, otherwise it is ignored.\n",
    "\n",
    "Forgy -> Forgy's algorithm is a simple alternating least-squares algorithm consisting of the following steps:\n",
    "Initialize the codebook vectors. (Suppose that when processing a given training case, N cases have been previously assigned to the winning codebook vector.)\n",
    "Repeat the following two steps until convergence:\n",
    "Read the data, assigning each case to the nearest (using Euclidean distance) codebook vector.\n",
    "Replace each codebook vector with the mean of the cases that were assigned to it.\n",
    "\n",
    "MacQueen -> This algorithm works by repeatedly moving all cluster centers to the mean of their respective Voronoi sets.\n",
    "\n",
    "Hartigan and Wong -> Given n objects with p variables measured on each object x(i,j) for i = 1,2,...,n; j = 1,2,...,p; K-means allocates each object to one of K groups or clusters to minimize the within-cluster sum of squares:\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "Given fixed centroids, it minizes the distance between in-cluster Xs and centroids by choosing the cluster labels for each X.(Repeat after each movement of centroids)\n",
    "#### Process Flow: \n",
    "Initiate K centroids, assign Xs(observations) to the closest(distance metric) centroid. Then, calculate the Avg(x) for each cluster which grouped by the initial assignment. Use the Avg(x) as the new position of the centroids. Repeating this process.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "data <- iris[, 3:4]\n",
    "set.seed(20)\n",
    "kmean.cluster <- kmeans(data,# - numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns). \n",
    "                        centers=3,# - either the number of clusters, say k, or a set of initial (distinct) cluster centres. If a number, a random set of (distinct) rows in x is chosen as the initial centres. \n",
    "                        iter.max = 300,# - the maximum number of iterations allowed. \n",
    "                        nstart = 1,# - if centers is a number, how many random sets should be chosen? \n",
    "                        algorithm = c(\"Hartigan-Wong\", \"Lloyd\", \"Forgy\", \"MacQueen\"), \n",
    "                        trace=FALSE)# - only used in the default method (\"Hartigan-Wong\"): if positive (or true), tracing information on the progress of the algorithm is produced.\n",
    "\n",
    "kmean.cluster$cluster # A vector of integers (from 1:k) indicating the cluster to which each point is allocated.\n",
    "kmean.cluster$centers # A matrix of cluster centres.\n",
    "kmean.cluster$totss # The total sum of squares.\n",
    "kmean.cluster$withinss # Vector of within-cluster sum of squares, one component per cluster\n",
    "kmean.cluster$tot.withinss # Total within-cluster sum of squares, i.e. sum(withinss).\n",
    "kmean.cluster$betweenss # The between-cluster sum of squares, i.e. totss-tot.withinss.\n",
    "kmean.cluster$size # The number of points in each cluster.\n",
    "kmean.cluster$iter # The number of (outer) iterations.\n",
    "kmean.cluster$ifault # integer: indicator of a possible algorithm problem – for experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------- Python Code\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = KMeans(n_clusters=3, # k\n",
    "            init='random', # initate points\n",
    "            n_init=10, # runs to choose lowest SSE #\n",
    "            max_iter=300, # iters in each run\n",
    "            tol=1e-04, # define converge\n",
    "            random_state=0)\n",
    "Y_km = km.fit_predict(X) # arrary of labels of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Mean ++ Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In classic k-means algorithm that uses a random seed to place the initial centroids, which can sometimes result in bad clusterings or slow convergence if the initial centroids are choosen poorly. So, in K-Mean++ clustering, it places the initial controids far aways from each others. The k-means++ algorithm addresses the second of these obstacles by specifying a procedure to initialize the cluster centers before proceeding with the standard k-means optimization iterations. With the k-means++ initialization, the algorithm is guaranteed to find a solution that is O(log k) competitive to the optimal k-means solution.\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "Given fixed centroids, it minizes the distance between in-cluster Xs and centroids by choosing the cluster labels for each X.(Repeat after each movement of centroids)\n",
    "#### Process Flow: \n",
    "Initiate K centroids randomly, assign Xs(observations) to the closest(distance metric) centroid. Find the minimum squared distance between (X,centroid) for each centroid. Randomly select the next centroid using a weighted probability (one minium dist / sum of all minium dists). Repeat the process until k centroids choosen (create k centroids which are far aways from each other) --> Proceed to the classic K-mean process... \n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://cran.r-project.org/web/packages/flexclust/flexclust.pdf\n",
    " \n",
    "km <- kcca(Nclus, k=4, family=kccaFamily(\"kmeans\"), control=list(initcent=\"kmeanspp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = KMeans(n_clusters=3, # k\n",
    "            init='k-means++', # initiate points far away from each other\n",
    "            n_init=10, # runs to choose lowest SSE #\n",
    "            max_iter=300, # iters in each run\n",
    "            tol=1e-04, # define converge\n",
    "            random_state=0)\n",
    "Y_km = km.fit_predict(X) # arrary of labels of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Mode Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "K-modes, an algorithm extending the k-means paradigm to categorical domain is introduced ??. New dissimilarity measures to deal with categorical data is conducted to replace means with modes, and a frequency based method is used to update modes in the clustering process to minimize the clustering cost function.\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.packages(\"klaR\")\n",
    "library(klaR)\n",
    "install.packages(\"vcd\")\n",
    "library(vcd)\n",
    "\n",
    "data <- as.data.frame(Arthritis)\n",
    "data <- as.matrix(data, ncol=5, nrow=84)\n",
    "\n",
    "kmode.cluster <- kmodes(data,# - A matrix or data frame of categorical data. Objects have to be in rows, variables in columns.\n",
    "                        modes=3,# - Either the number of modes or a set of initial (distinct) cluster modes. If a number, a random set of (distinct) rows in data is chosen as the initial modes.\n",
    "                        iter.max = 10,# - The maximum number of iterations allowed.\n",
    "                        weighted = FALSE)# - Whether usual simple-matching distance between objects is used, or a weighted version of this distance.\n",
    "\n",
    "kmode.cluster$cluster # A vector of integers indicating the cluster to which each object is allocated\n",
    "kmode.cluster$size # The number of objects in each cluster.\n",
    "kmode.cluster$modes # A matrix of cluster modes.\n",
    "kmode.cluster$withindiff # The within-cluster simple-matching distance for each cluster.\n",
    "kmode.cluster$iterations # The number of iterations the algorithm has run.\n",
    "kmode.cluster$weighted # Whether weighted distances were used or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "pip install kmodes\n",
    "pip install --upgrade kmodes\n",
    "\n",
    "# random categorical data\n",
    "data = np.random.choice(20, (100, 10))\n",
    "\n",
    "km = kmodes.KModes(n_clusters=4, init='Huang', n_init=5, verbose=1)\n",
    "\n",
    "clusters = km.fit_predict(data)\n",
    "\n",
    "# Print the cluster centroids\n",
    "print(km.cluster_centroids_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Medians Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the square of the 2-norm distance metric (which k-means does.)\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# pip install pyclustering\n",
    "\n",
    "import pyclustering\n",
    "from pyclustering.utils import read_sample;\n",
    "# load list of points for cluster analysis\n",
    "data = ; \n",
    "# create instance of K-Medians algorithm\n",
    "kmedians_instance = kmedians(data, # (list): Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "                             [ [0.0, 0.1], [2.5, 2.6] ], # (list): Initial coordinates of medians of clusters that are represented by list: [center1, center2, ...].\n",
    "                            tolerance = 0.25, # (double): Stop condition: if maximum value of change of centers of clusters is less than tolerance than algorithm will stop processing\n",
    "                            ccore = False); # (bool): Defines should be CCORE library (C++ pyclustering library) used instead of Python code or not.\n",
    " \n",
    "# run cluster analysis and obtain results\n",
    "kmedians_instance.process();\n",
    "kmedians_instance.get_clusters(); \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- K-Medoids Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary metrics of distances between datapoints. k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "K(Number of clusters)\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.packages(\"cluster\")\n",
    "library(cluster)\n",
    "\n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/pam.html\n",
    "kmedoids <- pam(x,\n",
    "                k,\n",
    "                diss = inherits(x, \"dist\"),\n",
    "                metric = \"euclidean\", \n",
    "                medoids = NULL, \n",
    "                stand = FALSE, \n",
    "                cluster.only = FALSE,\n",
    "                do.swap = TRUE,\n",
    "                keep.diss = !diss && !cluster.only && n < 100,\n",
    "                keep.data = !diss && !cluster.only,\n",
    "                pamonce = FALSE, \n",
    "                trace.lev = 0)\n",
    "\n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clara.html\n",
    "# Handle large dataset\n",
    "kmedoids <- clara(x, \n",
    "                  k, \n",
    "                  metric = \"euclidean\", \n",
    "                  stand = FALSE, \n",
    "                  samples = 5,\n",
    "                  sampsize = min(n, 40 + 2 * k), \n",
    "                  trace = 0, \n",
    "                  medoids.x = TRUE,\n",
    "                  keep.data = medoids.x, \n",
    "                  rngR = FALSE, \n",
    "                  pamLike = FALSE, \n",
    "                  correct.d = TRUE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# load list of points for cluster analysis\n",
    "data = read_sample(path);\n",
    " \n",
    "# create instance of K-Medoids algorithm\n",
    "kmedians_instance = kmedians(data, # (list): Input data that is presented as list of points (objects), each point should be represented by list or tuple.\n",
    "                             [1, 10], # (list): Indexes of intial medoids (indexes of points in input data).\n",
    "                             tolerance = 0.25, # (double): Stop condition: if maximum value of distance change of medoids of clusters is less than tolerance than algorithm will stop processing.\n",
    "                             ccore = False ); # (bool): If specified than CCORE library (C++ pyclustering library) is used for clustering instead of Python code.\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedians_instance.process();\n",
    "kmedians_instance.get_clusters();  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Hierarchy Clustering (Agglomerative) Bottom up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root) (see figure below). The result is a tree which can be plotted as a dendrogram.\n",
    "\n",
    "In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical) ~ Distance Metric: Euclidean distance, Squared Euclidean distance, Manhattan distance, maximum distance, Mahalanobis distance, Hamming distance(cate), Levenshtein distance(cate).\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "Maximum or complete linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters.\n",
    "\n",
    "Minimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, “loose” clusters.\n",
    "\n",
    "Mean or average linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters.\n",
    "\n",
    "Centroid linkage clustering: It computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.\n",
    "\n",
    "Ward’s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.\n",
    "#### Process Flow: \n",
    "Created distance matrix by choosing distance metrics, then choose a linkage method to start 'merge' or 'dive' from top down or down top. When done, define the 'cut' for the number of groups\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# - http://www.sthda.com/english/wiki/hierarchical-clustering-essentials-unsupervised-machine-learning\n",
    "data(\"USArrests\")\n",
    "df <- na.omit(USArrests)\n",
    "df <- scale(df)\n",
    "# Dissimilarity matrix\n",
    "d <- dist(df, method = \"euclidean\")\n",
    "\n",
    "# - hclust package\n",
    "# Hierarchical clustering using Ward's method\n",
    "# - http://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html\n",
    "hier.cluster <- hclust(d, method = \"ward.D2\" )\n",
    "# Plot the obtained dendrogram\n",
    "plot(res.hc, cex = 0.6, hang = -1)\n",
    "# Cut tree into 4 groups\n",
    "grp <- cutree(res.hc, k = 4)\n",
    "# Number of members in each cluster\n",
    "table(grp)\n",
    "# Get the names for the members of cluster 1\n",
    "rownames(df)[grp == 1]\n",
    "# Plot dendrogram with color boxes cover the 4 groups\n",
    "plot(res.hc, cex = 0.6)\n",
    "rect.hclust(res.hc, k = 4, border = 2:5)# border control color, 2,3,4,5\n",
    "# plot 'cut tree' again 1,2nd pca\n",
    "library(\"factoextra\")\n",
    "fviz_cluster(list(data = df, cluster = grp))\n",
    "\n",
    "\n",
    "# - agnes packages \n",
    "library(\"cluster\")\n",
    "# Compute agnes()\n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/agnes.html\n",
    "res.agnes <- agnes(df, method = \"ward\")\n",
    "# Plot the tree using pltree()\n",
    "pltree(res.agnes, cex = 0.6, hang = -1,\n",
    "       main = \"Dendrogram of agnes\") \n",
    "# plot.hclust() - convert obj using plot as above\n",
    "plot(as.hclust(res.agnes), cex = 0.6, hang = -1)\n",
    "# Agglomerative coefficient; amount of clustering structure found\n",
    "res.agnes$ac\n",
    "# Cut agnes() tree into 4 groups\n",
    "cutree(res.agnes, k = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "import sklearn.cluster\n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "AgglomerativeClustering_instance = AgglomerativeClustering(n_clusters=2, \n",
    "                                                           affinity='euclidean', \n",
    "                                                           memory=Memory(cachedir=None), \n",
    "                                                           connectivity=None, \n",
    "                                                           compute_full_tree='auto', \n",
    "                                                           linkage='ward', \n",
    "                                                           pooling_func=<function mean>)\n",
    "# Fit the hierarchical clustering on the data\n",
    "AgglomerativeClustering_instance.fit(x) # X : array-like, shape = [n_samples, n_features]\n",
    "# Performs clustering on X and returns cluster labels.\n",
    "AgglomerativeClustering_instance.fit_predict(X, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Hierarchy Clustering (Divisive) Top Down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Divisive hierarchical clustering: It’s also known as DIANA (Divise Analysis) and it works in a top-down manner. The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster (see figure below).\n",
    "\n",
    "In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical) ~ Distance Metric: Euclidean distance, Squared Euclidean distance, Manhattan distance, maximum distance, Mahalanobis distance, Hamming distance(cate), Levenshtein distance(cate).\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "Created distance matrix by choosing distance metrics, then choose a linkage method to start 'merge' or 'dive' from top down or down top. When done, define the 'cut' for the number of groups\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# - http://www.sthda.com/english/wiki/hierarchical-clustering-essentials-unsupervised-machine-learning\n",
    "data(\"USArrests\")\n",
    "df <- na.omit(USArrests)\n",
    "df <- scale(df)\n",
    "# Dissimilarity matrix\n",
    "d <- dist(df, method = \"euclidean\")\n",
    "\n",
    "# - diana packages \n",
    "# - https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/diana.html\n",
    "library(\"cluster\")\n",
    "diana(df, metric = \"euclidean\", stand = FALSE)\n",
    "# Plot the tree\n",
    "pltree(res.diana, cex = 0.6, hang = -1,\n",
    "       main = \"Dendrogram of diana\")\n",
    "# plot.hclust()\n",
    "plot(as.hclust(res.diana), cex = 0.6, hang = -1)\n",
    "# Divise coefficient; amount of clustering structure found\n",
    "res.diana$dc\n",
    "# Cut diana() tree into 4 groups\n",
    "cutree(as.hclust(res.diana), k = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Fuzzy Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Wiki Definitation: \n",
    "Fuzzy clustering (also referred to as soft clustering) is a form of clustering in which each data point can belong to more than one cluster. In non-fuzzy clustering (also known as hard clustering), data is divided into distinct clusters, where each data point can only belong to exactly one cluster. In fuzzy clustering, data points can potentially belong to multiple clusters. Membership grades are assigned to each of the data points. These membership grades indicate the degree to which data points belong to each cluster. Thus, points on the edge of a cluster, with lower membership grades, may be in the cluster to a lesser degree than points in the center of cluster.\n",
    "\n",
    "K-means clustering also attempts to minimize the objective function shown above. This method differs from the k-means objective function by the addition of the membership values {\\displaystyle w_{ij}} {\\displaystyle w_{ij}} and the fuzzifier, {\\displaystyle m\\in R} {\\displaystyle m\\in R} , with {\\displaystyle m\\geq 1} {\\displaystyle m\\geq 1}. The fuzzifier {\\displaystyle m} m determines the level of cluster fuzziness. A large {\\displaystyle m} m results in smaller membership values, {\\displaystyle w_{ij}} {\\displaystyle w_{ij}}, and hence, fuzzier clusters. In the limit {\\displaystyle m=1} {\\displaystyle m=1}, the memberships, {\\displaystyle w_{ij}} {\\displaystyle w_{ij}} , converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge, {\\displaystyle m} m is commonly set to 2. The algorithm minimizes intra-cluster variance as well, but has the same problems as k-means; the minimum is a local minimum, and the results depend on the initial choice of weights.\n",
    "#### Input Data: \n",
    "X(Numeric) \n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "The fuzzy c-means algorithm is very similar to the k-means algorithm: Choose a number of clusters, Assign randomly to each point coefficients for being in the clusters, Repeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than 'threshold' , the given sensitivity threshold) :\n",
    "[1] Compute the centroid for each cluster (shown below). [2] For each point, compute its coefficients of being in the clusters.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Choosing K based on either business knowledage or 'elbow method' on cost function value. Silhouette.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# - http://pro1.unibz.it/projects/Clustering_Methods_2014/Ferraro.pdf\n",
    "\n",
    "install.packages(\"fclust\")\n",
    "library(fclust)\n",
    "## McDonald's data\n",
    "data(Mc)\n",
    "names(Mc)\n",
    "## data normalization by dividing the nutrition facts by the Serving Size (column 1)\n",
    "for (j in 2:(ncol(Mc)-1))\n",
    "  Mc[,j]=Mc[,j]/Mc[,1]\n",
    "## removing the column Serving Size\n",
    "Mc=Mc[,-1]\n",
    "\n",
    "## - Fuzzy k-means (excluded the factor column Type (last column))\n",
    "# - https://www.rdocumentation.org/packages/fclust/versions/1.1.2/topics/FKM\n",
    "clust=FKM(Mc[,1:(ncol(Mc)-1)],k=6,m=1.5,stand=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1] It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.[2]\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "MinPts (controls minimum size of a cluster, low – like linkage, high – all noise, expect # for one cluster, like 10, 20, 500)\n",
    "\n",
    "Radius (the radius of the distance comparsion) Kind of subjective to define\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "In DBSCAN, a special label is assigned as core point if at least a specified number (MinPts) of neighboring points fall within the specified radius:\n",
    "[Core point] - if at least a specified number of (MinPts) of beighboring points fall within the specified radius\n",
    "[Border point] - has fewer neighbors than (MinPts) with radius but lies within the radius of a core point\n",
    "[Noise points] - Niether Core points or Border points\n",
    "After labeling the points as core, border, or noise points, the DBSCAN can be summarized:\n",
    "1.Form a separate cluster for each core point or a connected group of core points(core points are connected if they no farther away than radius\n",
    "2.Assign each border point to the cluster of its corresponding core point.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "Does not assume the clusters have a spherical shape, removing noise points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "install.apckages(\"dbscan\")\n",
    "library(dbscan)\n",
    "# - https://cran.r-project.org/web/packages/dbscan/dbscan.pdf\n",
    "dbscan.cluster <- dbscan(x, eps, minPts = 5, weights = NULL,\n",
    "                         borderPoints = TRUE, search = \"kdtree\", bucketSize = 10,\n",
    "                         splitRule = \"suggest\", approx = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Fitting the model\n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- OPTICS Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2] Its basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. In order to do so, the points of the database are (linearly) ordered such that points which are spatially closest become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that needs to be accepted for a cluster in order to have both points belong to the same cluster.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters:\n",
    "MinPts (controls minimum size of a cluster, low – like linkage, high – all noise, expect # for one cluster, like 10, 20, 500) \n",
    "#### Cost Function:\n",
    "#### Process Flow: \n",
    "http://fogo.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf\n",
    "\n",
    "Like DBSCAN, OPTICS requires two parameters: ε, which describes the maximum distance (radius) to consider, and MinPts, describing the number of points required to form a cluster. A point p is a core point if at least MinPts points are found within its ε -neighborhood N ε   ( p )   Contrary to DBSCAN, OPTICS also considers points that are part of a more densely packed cluster, so each point is assigned a core distance that describes the distance to the MinPtsth closest point. The reachability-distance of another point o from a point p is either the distance between o and p, or the core distance of p, whichever is bigger. The basic idea to overcome these problems is to run an algorithm which produces a special order of the database with respect to its density-based clustering structure containing the information about every clustering level of the data set (up to a “generating distance” e), and is very easy to analyze.\n",
    "#### Evaluation Methods:\n",
    "#### Tips:\n",
    "Does not assume the clusters have a spherical shape, removing noise points, it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# - https://cran.r-project.org/web/packages/dbscan/dbscan.pdf\n",
    "Install.packages(“dbscan”)\n",
    "Library(dbscan)\n",
    "Optics.cluster <- optics(x, \n",
    "                         eps, \n",
    "                         minPts = 5, \n",
    "                         eps_ci, xi, \n",
    "                         search = “kdtree”, \n",
    "                         bucketSize = 10, \n",
    "                         splitRule = “suggest”, \n",
    "                         approx. = 0)\n",
    "Optics_cut(x, eps_ci); \n",
    "opticsXi(object, xi = 0.001, minimum = F, nocorrect = F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# - http://pythonhosted.org/pyclustering/classpyclustering_1_1cluster_1_1optics_1_1optics.html\n",
    "# Read sample for clustering from some file\n",
    "sample = read_sample(path_sample);\n",
    "# Create OPTICS algorithm for cluster analysis\n",
    "optics_instance = optics(sample, 0.5, 6);\n",
    "# Run cluster analysis\n",
    "optics_instance.process();\n",
    "# Obtain results of clustering\n",
    "clusters = optics_instance.get_clusters();\n",
    "noise = optics_instance.get_noise();\n",
    "# Obtain rechability-distances\n",
    "ordering = optics_instance.get_cluster_ordering();\n",
    "# Visualization of cluster ordering in line with reachability distance.\n",
    "indexes = [i for i in range(0, len(ordering))];\n",
    "plt.bar(indexes, ordering);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Non Negative Matrix Factorization (NMF) - Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "http://web.stanford.edu/group/mmds/slides2012/s-park.pdf\n",
    "Non-negative Matrix Factorization, a technique which makes use of an algorithm based on decomposition by parts of an extensive data matrix into a small number of relevant metagenes. NMF’s ability to identify expression patterns and make class discoveries has been shown to able to have greater robustness over popular clustering techniques such as HCL and SOM. Non-negative matrix factorization (NMF) finds a small number of metagenes, each defined as a positive linear combination of the genes in the expression data. NMF is most frequently used to make class discoveries through identification of molecular patterns. The module can also be used to cluster genes, generating metasamples rather than metagenes.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters:\n",
    "K_Genre(K dimensions in factorization) \n",
    "#### Cost Function:\n",
    "#### Process Flow:\n",
    "http://www.cc.gatech.edu/~hpark/papers/GT-CSE-08-01.pdf\n",
    "MeV’s NMF uses a multiplicative update algorithm, introduced by Lee and Seung in 2001, to factor a non-negative data matrix into two factor matrices referred to as W and H. Associated with each factorization is a user-specified rank. This represents the columns in W, the rows in H, and the number of clusters to which the samples are to be assigned. Starting with randomly seeded matrices and using an iterative approach with a specified cost measurement we can reach a locally optimal solution for these factor matrices. H and W can then be evaluated as metagenes and metagenes expression patterns, respectively. Using a “winner-take-all” approach, samples can be assigned to clusters based on their highest metagenes expression. Multiple iterations of this process allow us to see the robustness of the cluster memberships. Additionally, running multiple ranks consecutively can allow for the comparison between differing numbers of classes using cophenetic correlation. \n",
    "#### Evaluation Methods:\n",
    "#### Tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# - https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf\n",
    "Install.packages(“NMF”)\n",
    "Library(NMF)\n",
    "method  <- nmfAlgorithm(“brunet”) # choose algorithm\n",
    "seed <- nmfSeed(“nndsvd”) # choose a seeding method\n",
    "nmf.cluster <- nmf(x, \n",
    "                   rank, \n",
    "                   method, \n",
    "                   seed) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "import numpy as np\n",
    "X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "model.fit(X) \n",
    "\n",
    "NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,\n",
    "  n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,\n",
    "  solver='cd', sparseness=None, tol=0.0001, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Latent Dirichlet Allocation (LDA) - Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics.\n",
    "\n",
    "Latent Dirichlet allocation is a way of automatically discovering topics that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like\n",
    "\n",
    "•Sentences 1 and 2: 100% Topic A\n",
    "\n",
    "•Sentences 3 and 4: 100% Topic B\n",
    "\n",
    "•Sentence 5: 60% Topic A, 40% Topic B\n",
    "\n",
    "•Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, ... (at which point, you could interpret topic A to be about food)\n",
    "\n",
    "•Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, ... (at which point, you could interpret topic B to be about cute animals)\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters:\n",
    "->Bayes' rule with p(theta|x) = p(x|theta)p(theta|alpha)/p(x|alpha)\n",
    "\n",
    "->posterior probability = likelihood x prior probability / marginal likelihood\n",
    "\n",
    "K_topic(choose#)\n",
    "\n",
    "Alpha(hidden para)\n",
    "\n",
    "Beta(hidden para)\n",
    "#### Cost Function:\n",
    "#### Process Flow:\n",
    "https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation\n",
    "\n",
    "LDA assumes that each document k is generated by:\n",
    "\n",
    "1.From our Dirichlet distribution for k, sample a random  distribution of topics. That is, pick a place on that triangle that is  associated with a certain probability of generating each topic. If we   choose a place very close to the \"sports article\" edge, we have a higher  probability of picking \"sports article\". The probability of picking a  particular place on the triangle is described by the pdf of the  Dirichlet distribution (the placement of the purple mound).\n",
    "\n",
    "2.For each topic, pick a distribution of words for that topic from the Dirichlet for that topic.\n",
    "\n",
    "3.For each word in document k,\n",
    "\n",
    "a.From the distribution of topics selected for k, sample a topic, like \"sports article\".\n",
    "\n",
    "b.From the distribution selected for \"sports article\", pick the current word.\n",
    "\n",
    "So let's say your first four words all come from baseball and your  document maybe starts off \"average the bat bat\". If that's not how you  tend to write, that's okay. All models are wrong.\n",
    "The important thing to understand is that your Dirichlet priors are distribution of distributions, which are selected to generate each word.\n",
    "\n",
    "We're generally not just making these distributions for the heck of it  or to actually generate documents. We want to figure out what topic was  probably used for each word by our lazy writer who randomly generates  each word. Maybe it's been a while since you took probability, but do  you remember this guy? (Conditional Probability)\n",
    "This is the Bayes Theorem. We already know the probability of generating  a particular word given a topic according to our model. That's the  probability of sampling that word from the topic's word distribution. So  that's P(B|A) where B is the event of \"average\" being generated for the current word and A is the event of picking the topic \"sports article\". P(A) is the probability of \"sports article\" being picked from the document's topic distribution. P(B) is the probability of \"average\" being generated at all, which is the sum over all topic selections A of P(B|A)P(A). Now we can use Bayes to find P(A|B), the probability that topic A generated word B.\n",
    "\n",
    "So now we know how to figure out the probability of each topic per word.  Now we already know that our documents are assumed to be a mix of  topics, but we want to find the most likely dominant topic. The lazy  writer generates each word independently of each other word, so the  overall probability of a topic throughout the document is the product of  P(A|B) at each word B. Just pick the most likely topic across the words.\n",
    "#### Evaluation Methods:\n",
    "#### Tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf\n",
    "Install.packages(“topicmodels”)\n",
    "library(topicmodels)\n",
    "x # - x is a suitable document-term matrix with nonnegative integer count entries, t\n",
    "#  ypically a \"DocumentTermMatrix\" as obtained from package tm. Internally, topicmodels uses the simple triplet \n",
    "#   matrix representation of package slam (Hornik, Meyer, and Buchta 2011) (which, similar to the \\coordinate list\" \n",
    "#  (COO) sparse matrix format, stores the information about non-zero entries xij in the form of (i; j; xij) triplets).\n",
    "#  x can be any object coercible to such simple triplet matrices (with count entries), in particular objects obtained \n",
    "#  from readers for commonly employed document-term matrix storage formats. For example the reader \n",
    "#  read_dtm_Blei_et_al() available in package tm allows to read in data provided in the format used for the code by \n",
    "#  Blei and co-authors.\n",
    "control_LDA_VEM <-  list(estimate.alpha = TRUE, alpha = 50/k, estimate.beta = TRUE,\n",
    "                         verbose = 0, prefix = tempfile(), save = 0, keep = 0,\n",
    "                         seed = as.integer(Sys.time()), nstart = 1, best = TRUE,\n",
    "                         var = list(iter.max = 500, tol = 10^-6),\n",
    "                         em = list(iter.max = 1000, tol = 10^-4),\n",
    "                         initialize = \"random\")\n",
    "control_LDA_Gibbs <- list(alpha = 50/k, estimate.beta = TRUE,\n",
    "                          verbose = 0, prefix = tempfile(), save = 0, keep = 0,\n",
    "                          seed = as.integer(Sys.time()), nstart = 1, best = TRUE,\n",
    "                          delta = 0.1,\n",
    "                          iter = 2000, burnin = 0, thin = 2000)\n",
    "\n",
    "LDA(x, k, method = \"VEM\", control = NULL, model = NULL, ...)\n",
    "\n",
    "control_CTM_VEM <- list(estimate.beta = TRUE,\n",
    "                        verbose = 0, prefix = tempfile(), save = 0, keep = 0,\n",
    "                        seed = as.integer(Sys.time()), nstart = 1L, best = TRUE,\n",
    "                        var = list(iter.max = 500, tol = 10^-6),\n",
    "                        em = list(iter.max = 1000, tol = 10^-4),\n",
    "                        initialize = \"random\",\n",
    "                        cg = list(iter.max = 500, tol = 10^-5))\n",
    "\n",
    "CTM(x, k, method = \"VEM\", control = NULL, model = NULL, ...) \n",
    "# The correlated topics model (CTM; Blei and La ",
    "erty 2007) is an extension of the LDA model where correlations \n",
    "# between topics are allowed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "From sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_topics=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None)\n",
    "LDA.fit(X,y=none)\n",
    "# https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Expectation Maximization (EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters:\n",
    "X(Numeric)/X(Categorical) – Observed vars\n",
    "K groups – Unobserved var (No need to assign)\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "http://stats.stackexchange.com/questions/72774/numerical-example-to-understand-expectation-maximization\n",
    "\n",
    "EM is an an iterative unsupervised clustering method. To grossly oversimplify, EM takes a set of data (\\( \\bf X \\)) and attempts to find the number of clusters represented in \\( \\bf X \\) and finds clusters in the data. It finds clusters by first assigning each point a probability of belonging to a each cluster and then re-estimating the probability distribution for each cluster. This process continues until the error has converged at which time the points are all assigned to a given cluster with some degree of uncertainty. The process repeats for various counts of clusters. The quality of each model is accessed using something like the Bayesian information criterion and the model with the number of clusters that maximizes the quality of the model is returned.\n",
    "\n",
    "The EM (expectation maximization) technique is similar to the K-Means technique. The basic operation of K-Means clustering algorithms is relatively simple: Given a fixed number of k clusters, assign observations to those clusters so that the means across clusters (for all variables) are as different from each other as possible. The EM algorithm extends this basic approach to clustering in two important ways: \n",
    "\n",
    "•Instead of assigning examples to clusters to maximize the differences in means for continuous variables, the EM clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions. The goal of the clustering algorithm then is to maximize the overall probability or likelihood of the data, given the (final) clusters.\n",
    "\n",
    "•Unlike the classic implementation of k-means clustering, the general EM algorithm can be applied to both continuous and categorical variables (note that the classic k-means algorithm can also be modified to accommodate categorical variables).\n",
    "\n",
    "Expectation Maximization algorithmThe basic approach and logic of this clustering method is as follows. Suppose you measure a single continuous variable in a large sample of observations. Further, suppose that the sample consists of two clusters of observations with different means (and perhaps different standard deviations); within each sample, the distribution of values for the continuous variable follows the normal distribution. The goal of EM clustering is to estimate the means and standard deviations for each cluster so as to maximize the likelihood of the observed data (distribution). Put another way, the EM algorithm attempts to approximate the observed distributions of values based on mixtures of different distributions in different clusters. The results of EM clustering are different from those computed by k-means clustering. The latter will assign observations to clusters to maximize the distances between clusters. The EM algorithm does not compute actual assignments of observations to clusters, but classification probabilities. In other words, each observation belongs to each cluster with a certain probability. Of course, as a final result you can usually review an actual assignment of observations to clusters, based on the (largest) classification probability.\n",
    "#### Evaluation Methods:\n",
    "\n",
    "#### Tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# >> Numeric data\n",
    "# https://www.r-bloggers.com/maximize-your-expectations/\n",
    "nSample <- 2000  # this is equal to the size of each class (0.5*total)\n",
    "\n",
    "# x1a and x2a are features x1 and x2 for z=0 (x11 just seemed a tad much)\n",
    "x1a <- rnorm(nSample, 10, 3)\n",
    "x2a <- rnorm(nSample, 10, 3)\n",
    "# x1b and x2b are features x1 and x2 for z=1\n",
    "x1b <- rnorm(nSample, 16, 3)\n",
    "x2b <- rnorm(nSample, 16, 3)\n",
    "\n",
    "library(mclust)\n",
    "## Package 'mclust' version 4.0\n",
    "# put the data into a matrix just to make using Mclust a bit easier\n",
    "clusterMatrix <- matrix(NA, nrow = nSample * 2, ncol = 2)\n",
    "clusterMatrix[, 1] <- c(x1a, x1b)\n",
    "clusterMatrix[, 2] <- c(x2a, x2b)\n",
    "# fits the EM model to the data\n",
    "# package details - https://cran.r-project.org/web/packages/mclust/mclust.pdf\n",
    "model <- Mclust(clusterMatrix)\n",
    "summary(model)\n",
    "\n",
    "## ----------------------------------------------------\n",
    "## Gaussian finite mixture model fitted by EM algorithm \n",
    "## ----------------------------------------------------\n",
    "## \n",
    "## Mclust EII (spherical, equal volume) model with 2 components:\n",
    "## \n",
    "##  log.likelihood    n df    BIC\n",
    "##          -22111 4000  6 -44271\n",
    "## \n",
    "## Clustering table:\n",
    "##    1    2 \n",
    "## 2052 1948\n",
    "\n",
    "# No surprise there, we expected and we found 2 clusters. Lets take a look at those clusters now.\n",
    "clusterPlots <- data.frame(x1 = clusterData$x1, x2 = clusterData$x2, cluster = factor(model$classification, \n",
    "                           levels = c(1, 2), labels = c(\"z = 0\", \"z = 1\")), uncertainity = model$uncertainty, \n",
    "                           logUncertainity = log(model$uncertainty))\n",
    "clusterPlots.gg <- ggplot(clusterPlots)\n",
    "clusterPlots.gg + geom_point(aes(x = x1, y = x2, color = cluster))\n",
    "\n",
    "# One of the nice things about EM is it quantifies how uncertain it is in a points class.\n",
    "clusterPlots.gg + geom_point(aes(x = x1, y = x2, color = logUncertainity))\n",
    "\n",
    "# More - https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Expectation_Maximization_(EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n",
    "From sklearn.mixture import \n",
    "\n",
    "GaussianMixture(n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, \n",
    "                n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, \n",
    "                random_state=None, warm_start=False, verbose=0, verbose_interval=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- X-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "A way to deal with this problem is to include some penalty for a larger number of clusters. So, we are now trying to minimize not only the error, but error + penalty. The error will just converge towards zero as we increase the number of clusters, but the penalty will grow. At some points, the gain from adding another cluster will be less than the introduced penalty, and we’ll have the optimal result. A solution that uses Bayesian Information Criterion (BIC) for this purpose is called X-Means [Pelleg and Moore, 2000].\n",
    "#### Input Data:\n",
    "X(Numeric)\n",
    "#### Initial Parameters:\n",
    "K(Number of clusters)\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "http://stats.stackexchange.com/questions/13103/x-mean-algorithm-bic-calculation-question\n",
    "#### Evaluation Methods:\n",
    "\n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Affinity Propagation (AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "In statistics and data mining, affinity propagation (AP) is a clustering algorithm based on the concept of \"message passing\" between data points.[1] Unlike clustering algorithms such as k-means or k-medoids, affinity propagation does not require the number of clusters to be determined or estimated before running the algorithm. Similar to k-medoids, affinity propagation finds \"exemplars\", members of the input set that are representative of clusters.[1]\n",
    "#### Input Data:\n",
    "X(Numeric)\n",
    "#### Initial Parameters:\n",
    "K(Number of clusters)\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "https://en.wikipedia.org/wiki/Affinity_propagation\n",
    "\n",
    "The main drawbacks of K-Means and similar algorithms are having to select the number of clusters, and choosing the initial set of points. Affinity Propagation, instead, takes as input measures of similarity between pairs of data points, and simultaneously considers all data points as potential exemplars. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges.\n",
    "As an input, the algorithm requires us to provide two sets of data:\n",
    "1.Similarities between data points, representing how well-suited a point is to be another one’s exemplar. If there’s no similarity between two points, as in they cannot belong to the same cluster, this similarity can be omitted or set to -Infinity depending on implementation.\n",
    "2.Preferences, representing each data point’s suitability to be an exemplar. We may have some a priori information which points could be favored for this role, and so we can represent it through preferences.\n",
    "Both similarities and preferences are often represented through a single matrix, where the values on the main diagonal represent preferences. Matrix representation is good for dense datasets. Where connections between points are sparse, it is more practical not to store the whole n x n matrix in memory, but instead keep a list of similarities to connected points. Behind the scene, ‘exchanging messages between points’ is the same thing as manipulating matrices, and it’s only a matter of perspective and implementation.\n",
    "The algorithm then runs through a number of iterations, until it converges. Each iteration has two message-passing steps:\n",
    "1.Calculating responsibilities: Responsibility r(i, k) reflects the accumulated evidence for how well-suited point k is to serve as the exemplar for point i, taking into account other potential exemplars for point i. Responsibility is sent from data point i to candidate exemplar point k.\n",
    "2.Calculating availabilities: Availability a(i, k) reflects the accumulated evidence for how appropriate it would be for point i to choose point k as its exemplar, taking into account the support from other points that point k should be an exemplar. Availability is sent from candidate exemplar point k to point i.\n",
    "In order to calculate responsibilities, the algorithm uses original similarities and availabilities calculated in the previous iteration (initially, all availabilities are set to zero). Responsibilities are set to the input similarity between point i and point k as its exemplar, minus the largest of the similarity and availability sum between point i and other candidate exemplars. The logic behind calculating how suitable a point is for an exemplar is that it is favored more if the initial a priori preference was higher, but the responsibility gets lower when there is a similar point that considers itself a good candidate, so there is a ‘competition’ between the two until one is decided in some iteration.\n",
    "Calculating availabilities, then, uses calculated responsibilities as evidence whether each candidate would make a good exemplar. Availability a(i, k) is set to the self-responsibility r(k, k) plus the sum of the positive responsibilities that candidate exemplar k receives from other points.\n",
    "Finally, we can have different stopping criteria to terminate the procedure, such as when changes in values fall below some threshold, or the maximum number of iterations is reached. At any point through Affinity Propagation procedure, summing Responsibility (r) and Availability (a) matrices gives us the clustering information we need: for point i, the k with maximum r(i, k) + a(i, k) represents point i’s exemplar. Or, if we just need the set of exemplars, we can scan the main diagonal. If r(i, i) + a(i, i) > 0, point i is an exemplar.\n",
    "#### Evaluation Methods:\n",
    "\n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Mean Shift Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2] \n",
    "#### Input Data:\n",
    "X(Numeric)\n",
    "#### Initial Parameters:\n",
    "K(Number of clusters)\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "\n",
    "#### Evaluation Methods:\n",
    "\n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "http://www.datasciencecentral.com/profiles/blogs/spectral-clustering-how-math-is-redefining-decision-making?utm_content=buffer68e80&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer\n",
    "\n",
    "In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters:\n",
    "\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "\n",
    "#### Evaluation Methods:\n",
    "\n",
    "#### Tips:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Indices ------------------------------------ [True Label Exists]\n",
    "An external index is a measure of agreement between two partitions where the first partition is the a priori known clustering structure, and the second results from the clustering procedure. For external indices, we evaluate the results of a clustering algorithm based on a known cluster structure of a data set (or cluster labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Adjust Rand Index\n",
    "https://en.wikipedia.org/wiki/Rand_index\n",
    "\n",
    "The Rand index[1] or Rand measure (named after William M. Rand) in statistics, and in particular in data clustering, is a measure of the similarity between two data clusterings. A form of the Rand index may be defined that is adjusted for the chance grouping of elements, this is the adjusted Rand index. From a mathematical standpoint, Rand index is related to the accuracy, but is applicable even when class labels are not used.\n",
    "\n",
    "##### Pros:\n",
    "Random (uniform) label assignments have a ARI score close to 0.0 for any value of n_clusters and n_samples (which is not the case for raw Rand index or the V-measure for instance); Bounded range [-1, 1]: negative values are bad (independent labelings), similar clusterings have a positive ARI, 1.0 is the perfect match score; No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with “folded” shapes.\n",
    "##### Cons:\n",
    "Contrary to inertia, ARI requires knowledge of the ground truth classes while is almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting); However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "# http://svitsrv25.epfl.ch/R-doc/library/flexclust/html/randIndex.html\n",
    "## no class correlations: corrected Rand almost zero\n",
    "g1 <- sample(1:5, size=1000, replace=TRUE)\n",
    "g2 <- sample(1:5, size=1000, replace=TRUE)\n",
    "tab <- table(g1, g2)\n",
    "randIndex(tab)\n",
    "\n",
    "## uncorrected version will be large, because there are many points\n",
    "## which are assigned to different clusters in both cases\n",
    "randIndex(tab, correct=FALSE)\n",
    "\n",
    "## let pairs (g1=1,g2=1) and (g1=3,g2=3) agree better\n",
    "k <- sample(1:1000, size=200)\n",
    "g1[k] <- 1\n",
    "g2[k] <- 1\n",
    "k <- sample(1:1000, size=200)\n",
    "g1[k] <- 3\n",
    "g2[k] <- 3\n",
    "tab <- table(g1, g2)\n",
    "\n",
    "## the index should be larger than before\n",
    "randIndex(tab)\n",
    "randIndex(tab, correct=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
    ">>>1.0\n",
    "adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
    ">>>1.0\n",
    "adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
    ">>>0.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Jaccard index\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Fowlkes-Mallows (FM) index\n",
    "The Fowlkes-Mallows index (sklearn.metrics.fowlkes_mallows_score) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall: \"FMI = TP / square_root((TP + FP)/(TP + FN))\"\n",
    "\n",
    "Where TP is the number of True Positive (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), FP is the number of False Positive (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and FN is the number of False Negative (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).\n",
    "##### Pros:\n",
    "Random (uniform) label assignments have a FMI score close to 0.0 for any value of n_clusters and n_samples (which is not the case for raw Mutual Information or the V-measure for instance); Bounded range [0, 1]: Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, values of exactly 0 indicate purely independent label assignments and a AMI of exactly 1 indicates that the two label assignments are equal (with or without permutation); No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with “folded” shapes.\n",
    "##### Cons:\n",
    "Contrary to inertia, FMI-based measures require the knowledge of the ground truth classes while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n",
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n",
    ">>> 0.47140...\n",
    "# One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:\n",
    "labels_pred = [1, 1, 0, 0, 3, 3]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n",
    ">>> 0.47140...\n",
    "# Perfect labeling is scored 1.0:\n",
    "labels_pred = labels_true[:]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n",
    ">>> 1.0\n",
    "# Bad (e.g. independent labelings) have zero scores:\n",
    "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n",
    "labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n",
    ">>> 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Indices ------------------------------------ [True Label Not Exisits]\n",
    "Internal indices are used to measure the goodness of a clustering structure without external information. For internal indices, we evaluate the results using quantities and features inherent in the data set. The optimal number of clusters is usually determined based on an internal validity index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Silhouette index\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "The more even thickness of each bar the better (equal size), the clusters below average coefficient is bad partition, the closer to 1 the better the cluster\n",
    "\n",
    "##### Pros:\n",
    "The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\n",
    "##### Cons:\n",
    "The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# http://www.sthda.com/english/wiki/clustering-validation-statistics-4-vital-things-everyone-should-know-unsupervised-machine-learning#silhouette-plot-for-k-means-clustering\n",
    "# Silhouette coefficient of observations\n",
    "library(\"cluster\")\n",
    "sil <- silhouette(km.res$cluster, dist(iris.scaled))\n",
    "head(sil[, 1:3], 10)\n",
    "##       cluster neighbor sil_width\n",
    "##  [1,]       1        3 0.7341949\n",
    "##  [2,]       1        3 0.5682739\n",
    "##  [3,]       1        3 0.6775472\n",
    "##  [4,]       1        3 0.6205016\n",
    "##  [5,]       1        3 0.7284741\n",
    "##  [6,]       1        3 0.6098848\n",
    "##  [7,]       1        3 0.6983835\n",
    "##  [8,]       1        3 0.7308169\n",
    "##  [9,]       1        3 0.4882100\n",
    "## [10,]       1        3 0.6315409\n",
    "# Silhouette plot\n",
    "plot(sil, main =\"Silhouette plot - K-means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "...                                                      \n",
    "0.55..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Davies-Bouldin index\n",
    "https://www.mathworks.com/help/stats/clustering.evaluation.daviesbouldinevaluation-class.html?requestedDomain=www.mathworks.com\n",
    "\n",
    "The Davies-Bouldin criterion is based on a ratio of within-cluster and between-cluster distances. The Davies-Bouldin index is defined as “Di,j as SUM(avgDi +avgDj)/Dij -> DB = SUM(MAX(Di,j))/k for all pairs of i and j clusters”. The optimal clustering solution has the smallest Davies-Bouldin index value.\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://artax.karlin.mff.cuni.cz/r-help/library/clusterSim/html/index.DB.html\n",
    "# Example 1\n",
    "library(clusterSim)\n",
    "data(data_ratio)\n",
    "cl1 <- pam(data_ratio, 4)\n",
    "d<-dist(data_ratio)\n",
    "print(index.DB(data_ratio, cl1$clustering,d, centrotypes=\"medoids\"))\n",
    "\n",
    "# Example 2\n",
    "library(clusterSim)\n",
    "data(data_ratio)\n",
    "cl2 <- pam(data_ratio, 5)\n",
    "print(index.DB(data_ratio, cl2$clustering, centrotypes=\"centroids\"))\n",
    "\n",
    "# Example 3\n",
    "library(clusterSim)\n",
    "data(data_ratio)\n",
    "md <- dist(data_ratio, method=\"euclidean\")\n",
    "# nc - number_of_clusters\n",
    "min_nc=2\n",
    "max_nc=8\n",
    "res <- array(0, c(max_nc-min_nc+1, 2))\n",
    "res[,1] <- min_nc:max_nc\n",
    "clusters <- NULL\n",
    "for (nc in min_nc:max_nc)\n",
    "{\n",
    "hc <- hclust(md, method=\"complete\")\n",
    "cl2 <- cutree(hc, k=nc)\n",
    "res[nc-min_nc+1, 2] <- DB <- index.DB(data_ratio, cl2, centrotypes=\"centroids\")$DB\n",
    "clusters <- rbind(clusters, cl2)\n",
    "}\n",
    "print(paste(\"min DB for\",(min_nc:max_nc)[which.min(res[,2])],\"clusters=\",min(res[,2])))\n",
    "print(\"clustering for min DB\")\n",
    "print(clusters[which.min(res[,2]),])\n",
    "write.table(res,file=\"DB_res.csv\",sep=\";\",dec=\",\",row.names=TRUE,col.names=FALSE)\n",
    "plot(res, type=\"p\", pch=0, xlab=\"Number of clusters\", ylab=\"DB\", xaxt=\"n\")\n",
    "axis(1, c(min_nc:max_nc))\n",
    "\n",
    "# Example 4\n",
    "library(clusterSim)\n",
    "data(data_ordinal)\n",
    "md <- dist.GDM(data_ordinal, method=\"GDM2\")\n",
    "# nc - number_of_clusters\n",
    "min_nc=2\n",
    "max_nc=6\n",
    "res <- array(0, c(max_nc-min_nc+1, 2))\n",
    "res[,1] <- min_nc:max_nc\n",
    "clusters <- NULL\n",
    "for (nc in min_nc:max_nc)\n",
    "{\n",
    "hc <- hclust(md, method=\"complete\")\n",
    "cl2 <- cutree(hc, k=nc)\n",
    "res[nc-min_nc+1,2] <- DB <- index.DB(data_ordinal,cl2,d=md,centrotypes=\"medoids\")$DB\n",
    "clusters <- rbind(clusters, cl2)\n",
    "}\n",
    "print(paste(\"min DB for\",(min_nc:max_nc)[which.min(res[,2])],\"clusters=\",min(res[,2])))\n",
    "print(\"clustering for min DB\")\n",
    "print(clusters[which.min(res[,2]),])\n",
    "write.table(res,file=\"DB_res.csv\",sep=\";\",dec=\",\",row.names=TRUE,col.names=FALSE)\n",
    "plot(res, type=\"p\", pch=0, xlab=\"Number of clusters\", ylab=\"DB\", xaxt=\"n\")\n",
    "axis(1, c(min_nc:max_nc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Calinski-Harabasz (C-Index)\n",
    "https://www.mathworks.com/help/stats/clustering.evaluation.calinskiharabaszevaluation-class.html\n",
    "\n",
    "The Calinski-Harabasz criterion is sometimes called the variance ratio criterion (VRC). The Calinski-Harabasz index is defined as “VRC = SS(B)/SS(W) * (N-k)/(k-1)” where SS(B) is the overall between-cluster variance, SS(W) is the overall within-cluster variance, k is the number of clusters, and N is the number of observations. Well-defined clusters have a large between-cluster variance (SSB) and a small within-cluster variance (SSW). The larger the VRCk ratio, the better the data partition. To determine the optimal number of clusters, maximize VRCk with respect to k. The optimal number of clusters is the solution with the highest Calinski-Harabasz index value.The Calinski-Harabasz criterion is best suited for k-means clustering solutions with squared Euclidean distances.\n",
    "##### Pros:\n",
    "The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster; The score is fast to compute\n",
    "##### Cons:\n",
    "The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://artax.karlin.mff.cuni.cz/r-help/library/fpc/html/calinhara.html\n",
    "set.seed(98765)\n",
    "iriss <- iris[sample(150,20),-5]\n",
    "km <- kmeans(iriss,3)\n",
    "calinhara(iriss,km$cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.calinski_harabaz_score(X, labels)  \n",
    ">> 560.39..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Dunn index \n",
    "The Dunn Index is the ratio of the smallest distance between observations not in the same cluster to the largest intra-cluster distance. The Dunn Index has a value between zero and infinity, and should be maximized. The Dunn index (DI) (introduced by J. C. Dunn in 1974) is a metric for evaluating clustering algorithms.[1] This is part of a group of validity indices including the Davies–Bouldin index or Silhouette index, in that it is an internal evaluation scheme, where the result is based on the clustered data itself. As do all other such indices, the aim is to identify sets of clusters that are compact, with a small variance between members of the cluster, and well separated, where the means of different clusters are sufficiently far apart, as compared to the within cluster variance. For a given assignment of clusters, a higher Dunn index indicates better clustering. \n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "One of the drawbacks of using this is the computational cost as the number of clusters and dimensionality of the data increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://artax.karlin.mff.cuni.cz/r-help/library/clValid/html/dunn.html\n",
    "data(mouse)\n",
    "express <- mouse[1:25,c(\"M1\",\"M2\",\"M3\",\"NC1\",\"NC2\",\"NC3\")]\n",
    "rownames(express) <- mouse$ID[1:25]\n",
    "## hierarchical clustering\n",
    "Dist <- dist(express,method=\"euclidean\")\n",
    "clusterObj <- hclust(Dist, method=\"average\")\n",
    "nc <- 2 ## number of clusters      \n",
    "cluster <- cutree(clusterObj,nc)\n",
    "dunn(Dist, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> R-squared index\n",
    "http://www.imm.dtu.dk/~perbb/MAS/ST116/module02/index.html\n",
    "\n",
    "The RS statistics is the usual R square known from ANOVA models – SS between / SS total. Both SS between and SS total are defined in terms of sums of squares, as the total between sum of squares and total sum of squares. The quantity SS between is a measure of the variation between clusters whereas SS total is a measure of the total variation. The values of RS lies between 0 and 1 with values close to 1 indicating high difference between clusters. There are no general rules available for assessing whether or not values of the statistics RMSSTD and RS are small or large, but the relative changes in the values of the statistics as the number of clusters increase can be useful in determining the number of clusters. Calculation of the statistics at each stage in the clustering algorithm, that is for each numbers of clusters, allows plotting the values against the number of clusters. A marked decrease or increase for RMSSTD and RS (\"an elbow\" in the plot), respectively, may indicate that a satisfactory number of clusters have been reached.\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Hubert-Levin (C-index)\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Krzanowski-Lai index\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Hartigan index\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Root-mean-square standard deviation (RMSSTD) index\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Semi-partial R-squared (SPR) index\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Distance between two clusters (CD) index\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Weighted inter-intra index\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Homogeneity index\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Separation index\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "\n",
    "##### Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
