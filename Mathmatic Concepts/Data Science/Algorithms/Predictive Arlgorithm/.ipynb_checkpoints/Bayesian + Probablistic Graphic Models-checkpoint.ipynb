{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian + Probablistic Graphic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bayesian refers to methods in probability and statistics named after Thomas Bayes (c. 1702–61)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "### Model Pros & Cons\n",
    "\n",
    "##### Naïve Bayes\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: It is easy and fast to predict class of test data set. It also perform well in multi class prediction; When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.; It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
    "\n",
    "Cons: If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.; On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.; Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
    "\n",
    "\n",
    "##### Gaussian Naïve Bayes\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Handle numeric features; Others like basic naïve bayes model;\n",
    "\n",
    "Cons: Others like basic naïve bayes model; \n",
    "\n",
    "##### Multinomial Naïve Bayes\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Handle categorical features; Others like basic naïve bayes model;\n",
    "\n",
    "Cons: Others like basic naïve bayes model;\n",
    "\n",
    "##### Bernoulli Naïve Bayes \n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Handle binary features; Others like basic naïve bayes model;\n",
    "\n",
    "Cons: Others like basic naïve bayes model;\n",
    "\n",
    "##### Averaged One-Dependence Estimators (AODE) \n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: More realistic assumption that all non-class features depend on each other;  Like naive Bayes, AODE does not perform model selection and does not use tuneable parameters, so low variance; It supports incremental learning; Predict probability rather than class so that user can define thresholds; Its probabilistic model can directly handle situations where some data are missing; \n",
    "\n",
    "Cons: Infeasible to high-dimensional data; \n",
    "\n",
    "##### Bayesian Network (BN)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: It is easy to exploit expert knowledge in BN models; Very robust in the sense of i) noisy data, ii) missing data and iii) sparse data; Unlike many machine learning models (including Artificial Neural Network), which usually appear as a “black box,” all the parameters in BNs have an understandable semantic interpretation; \n",
    "\n",
    "Cons: Poor on high dimensional data and less data; High computation intensity; Cannot be used to model the correlation relationships between random variables (Directed – causality); \n",
    "\n",
    "##### Markov Network (MN)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "##### Hidden Markov Models (HMM)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "##### Conditional Random Fields (CRFs) \n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/\n",
    "\n",
    "It is used in classification and it assumes that features X (Categorical) strongly independent (proportion of frequency)\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "Naive Bayes has been studied extensively since the 1950s. It was introduced under a different name into the text retrieval community in the early 1960s,[1]:488 and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines.[2] It also finds application in automatic medical diagnosis.[3]\n",
    "Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,[1]:718 which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\n",
    "In the statistics and computer science literature, Naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes.[4] \n",
    "All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "Conditional Probability with maximum likelihood estimator (MLE)\n",
    "\n",
    "#### Process Flow: \n",
    "Convert data into a contingency table with labels and features (words) displaying frequencies; Get margin (Prior) and joint probabilities to calculate posterior probability (Classification prob) \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "•If continuous features do not have normal distribution, we should use transformation or different methods to convert it in normal distribution.\n",
    "\n",
    "•If test data set has zero frequency issue, apply smoothing techniques “Laplace Correction” to predict the class of test data set.\n",
    "\n",
    "•Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over inflating importance.\n",
    "\n",
    "•Naive Bayes classifiers has limited options for parameter tuning like alpha=1 for smoothing, fit_prior=[True|False] to learn class prior probabilities or not and some other options (look at detail here). I would recommend to focus on your  pre-processing of data and the feature selection.\n",
    "\n",
    "•You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, “ensembling, boosting, bagging” won’t help since their purpose is to reduce variance. Naive Bayes has no variance to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "It is used in classification and it assumes that features (numeric X) follow a normal distribution.\n",
    "When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution. For example, suppose the training data contains a continuous attribute, x   {\\displaystyle x}  x. We first segment the data by the class, and then compute the mean and variance of x   {\\displaystyle x}  x in each class. Let μ c     {\\displaystyle \\mu _{c}}  \\mu _{c} be the mean of the values in x   {\\displaystyle x}  x associated with class c, and let σ c  2     {\\displaystyle \\sigma _{c}^{2}}  \\sigma^2_c be the variance of the values in x   {\\displaystyle x}  x associated with class c. Suppose we have collected some observation value v   {\\displaystyle v}  v. Then, the probability distribution of v   {\\displaystyle v}  v given a class c   {\\displaystyle c}  c, p ( x = v ∣ c )   {\\displaystyle p(x=v\\mid c)}  {\\displaystyle p(x=v\\mid c)}, can be computed by plugging v  into the equation for a Normal distribution parameterized by μ c  and σ c  2  is,\n",
    "Another common technique for handling continuous values is to use binning to discretize the feature values, to obtain a new set of Bernoulli-distributed features; some literature in fact suggests that this is necessary to apply naive Bayes, but it is not, and the discretization may throw away discriminative information.[4]\n",
    "Other the same as Naïve bayes … \n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "Conditional Probability with maximum likelihood estimator (MLE)\n",
    "\n",
    "#### Process Flow: \n",
    "Use Gaussian density function to calculate margin (priori for each X) and then calculate posterior via conditional probability \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n",
    "# https://bicorner.com/2015/07/16/naive-bayes-using-r/\n",
    "install.packages(“e1071”)\n",
    "library(e1071)\n",
    "classifier <-naiveBayes(iris[,1:4], iris[,5])\n",
    " table(predict(classifier, iris[,-5]), iris[,5],  dnn=list(‘predicted’,’actual’))\n",
    "# https://cran.r-project.org/web/packages/klaR/klaR.pdf\n",
    "install.packages(“klaR”)\n",
    "library(klaR)\n",
    "classifier <- NaiveBayes(Y~X, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.data.shape[0],(iris.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes\n",
    "\n",
    "It is used in classification and it assumes that features (categorical X) follow a multinomial distribution.\n",
    "Multinomial Naive Bayes is a specialized version of Naive Bayes that is designed more for text documents. Whereas simple naive Bayes would model a document as the presence and absence of particular words, multinomial naive bayes explicitly models the word counts and adjusts the underlying calculations to deal with in.\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "Conditional Probability with maximum likelihood estimator (MLE)\n",
    "\n",
    "#### Process Flow: \n",
    "Use Multinomial mass function to calculate margin (priori for each X) and then calculate posterior via conditional probability \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "# https://cran.r-project.org/web/packages/klaR/klaR.pdf\n",
    "install.packages(“klaR”)\n",
    "library(klaR)\n",
    "classifier <- NaiveBayes(Y~X, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
    "import numpy as np\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict(X[2:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naïve Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes\n",
    "\n",
    "It is used in classification and it assumes that features (binary X) follow a bernoulli distribution.\n",
    "In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks,[9] where binary term occurrence features are used rather than term frequencies. If x i     {\\displaystyle x_{i}}  x_{i} is a boolean expressing the occurrence or absence of the i'th term from the vocabulary\n",
    "\n",
    "#### Input Data: \n",
    "X(booleans)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "Conditional Probability with maximum likelihood estimator (MLE)\n",
    "\n",
    "#### Process Flow: \n",
    "Use bernoulli mass function to calculate margin (priori for each X) and then calculate posterior via conditional probability.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "import numpy as np\n",
    "X = np.random.randint(2, size=(6, 100))\n",
    "Y = np.array([1, 2, 3, 4, 4, 5])\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print(clf.predict(X[2:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged One-Dependence Estimators (AODE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://en.wikipedia.org/wiki/Averaged_one-dependence_estimators\n",
    "\n",
    "Averaged one-dependence estimators (AODE) is a probabilistic classification learning technique. It was developed to address the attribute-independence problem of the popular naive Bayes classifier. It frequently develops substantially more accurate classifiers than naive Bayes at the cost of a modest increase in the amount of computation.[1]\n",
    "\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "\n",
    "#### Process Flow: \n",
    "https://en.wikipedia.org/wiki/Averaged_one-dependence_estimators\n",
    "\n",
    "This formula defines a special form of One Dependence Estimator (ODE), a variant of the naive Bayes classifier that makes the above independence assumption that is weaker (and hence potentially less harmful) than the naive Bayes' independence assumption. In consequence, each ODE should create a less biased estimator than naive Bayes. However, because the base probability estimates are each conditioned by two variables rather than one, they are formed from less data (the training examples that satisfy both variables) and hence are likely to have more variance. AODE reduces this variance by averaging the estimates of all such ODEs.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n",
    "Install.packages(“AnDE”)\n",
    "Library(AnDE)\n",
    "Classifier <- aode(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://people.cs.pitt.edu/~milos/courses/cs2001/cs2001-2.pdf\n",
    "\n",
    "http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html\n",
    "\n",
    "In short, a Bayesian network (BN) is simply a way of showing how things interact and cause specific outcomes.\n",
    "\n",
    "A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\n",
    "A Bayesian belief network is a graphical representation of a probabilistic dependency model. It consists of a set of interconnected nodes, where each node represents a variable in the dependency model and the connecting arcs represent the causal relationships between these variables. Each node or variable may take one of a number of possible states or values. The belief in, or certainty of, each of these states is determined from the belief in each possible state of every node directly connected to it and its relationship with each of these nodes. The belief in each state of a node is updated whenever the belief in each state of any directly connected node changes.\n",
    "Bayesian belief networks are particularly suited to the target recognition problem, where the category, identity and class of a target track are to be determined. Each of these three track attributes may be modelled by a hypothesis node, in which each state represents a di®erent hypothesis. Evidence, such as Identi¯cation Friend or Foe (IFF) reports, Electronic Support (ES) data and track dynamics, is applied to the network through evidence nodes. On receipt of evidence, the belief in the state of the evidence node changes, causing changes in the belief of all nodes to ripple through the entire network, including the hypothesis nodes. In this way, the evidence updates the beliefs for each category, identity and class, and possibly the most likely state of each.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "\n",
    "#### Process Flow: \n",
    "http://www.csse.monash.edu.au/bai/book/BAI_Chapter2.pdf\n",
    "\n",
    "Structure learning (Learn the net structure from data) -> Parameters learning (Learn the distribution parameters from data) ; \n",
    "\n",
    "*Domain representation - \n",
    "Direct_connect (Parent -> child) / Indirect_connect (Ancestor -> descendant); Markov_blanket (Markov blanket of a node, which consists of the node’s parents, its children, and its children’s parents); Root (Node without parents) – original cause; leaf (Node without children) – final effects; intermediate_node (nodes are not root nor leaf); direction(from top to down, from causes to effects); ➔ Once finish the topology structure  specify a conditional distribution (discrete – conditional probability table / continuous – Gaussian dist / mix – logistic/softmax) for each node  Assign initial probabilities for each node by looking into all parent nodes’ unique values  Markov_property (I-maps: independence in BN system / D-maps: dependence in BN system / perfect-maps: mix with both); \n",
    "\n",
    "*Use BN to reason domain – \n",
    "The process of conditioning (Also, probability propagation, inference, belief updating) performed via a “flow of information” through BN. OR compute the posterior probability distribution for a set of query nodes, given values for some evidence/observation nodes – \n",
    "\n",
    "[1] Diagnostic_reasoning: reasoning from symptoms to cause, such as when a doctor observes Dyspnoea and then updates his belief about Cancer and whether the patient is a Smoker. Note that this reasoning occurs in the opposite direction to the network arcs.    \n",
    "\n",
    "[2] Predictive_reasoning: reasoning from new information about causes to new beliefs about effects, following the directions of the network arcs. For example, the patient may tell his physician that he is a smoker; even before any symptoms have been assessed, the physician knows this will increase the chances of the patient having cancer.   \n",
    "\n",
    "[3] Intercausal_reasoning: A further form of reasoning involves reasoning about the mutual causes of a common effect; this has been called intercausal reasoning. explaining away is of some interest. Suppose that there are exactly two possible causes of a particular effect, represented by a v-structure in the BN. So, even though the two causes are initially independent, with knowledge of the effect the presence of one explanatory cause renders an alternative cause less likely. In other words, the alternative cause has been explained away.  \n",
    "\n",
    "[4] Combined_reasoning:  Since any nodes may be query nodes and any may be evidence nodes, sometimes the reasoning does not fit neatly into one of the types described above. Indeed, we can combine the above types of reasoning in any way. \n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "# https://www.r-bloggers.com/bayesian-network-in-r-introduction/\n",
    "\n",
    "install.packages(\"bnlearn\")\n",
    "library(bnlearn)\n",
    "data(coronary)\n",
    "# learn structure\n",
    "bn_df <- data.frame(coronary)\n",
    "res <- hc(bn_df)\n",
    "plot(res)\n",
    "# remove unreasonable links\n",
    "res$arcs <- res$arcs[-which((res$arcs[,'from'] == \"M..Work\" & res$arcs[,'to'] == \"Family\")),]\n",
    "# Training – CPT at each node\n",
    "fittedbn <- bn.fit(res, data = bn_df)\n",
    "# look at inside one node\n",
    "print(fittedbn$Proteins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Network (MN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------- R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Fields (CRFs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
