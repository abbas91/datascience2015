{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called \"time series analysis\", which focuses on comparing values of a single time series or multiple dependent time series at different points in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Overview of Framework:\n",
    "\n",
    "##### 1.Visualize the time-series\n",
    "It is essential to analyze the trends prior to building any kind of time series model. The details we are interested in pertains to any kind of trend, seasonality or random behaviour in the series. We have covered this part in the second part of this series.\n",
    "##### 2.Stationarize the series\n",
    "Once we know the patterns, trends, cycles and seasonality , we can check if the series is stationary or not. Dickey – Fuller is one of the popular test to check the same. We have covered this test in the first part of this article series. This doesn’t ends here! What if the series is found to be non-stationary? There are three commonly used technique to make a time series stationary.\n",
    "##### 3.Identify the optimal parameters\n",
    "The parameters p,d,q can be found using  ACF and PACF plots. An addition to this approach is can be, if both ACF and PACF decreases gradually, it indicates that we need to make the time series stationary and introduce a value to “d”.\n",
    "##### 4.Build the Models\n",
    "With the parameters in hand, we can now try to build ARIMA model. The value found in the previous section might be an approximate estimate and we need to explore more (p,d,q) combinations. The one with the lowest BIC and AIC should be our choice. We can also try some models with a seasonal component. Just in case, we notice any seasonality in ACF/PACF plots.\n",
    "##### 5.Make the prediction\n",
    "Once we have the final ARIMA model, we are now ready to make predictions on the future time points. We can also visualize the trends to cross validate if the model works fine.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Time Series Concepts:\n",
    "\n",
    "### [1] Time Series Statistical Models - Trends >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "#### White Noise (Total Random Trend)\n",
    "\n",
    "\" w(t) - Uncorrelated random variable, with mean 0, variance a^2 \"\n",
    "\" w(t) ~ iid N(0, a^2) : Gausian White Noise \"\n",
    "\n",
    "\n",
    "#### Moving Averages and Filtering (Smooth Trend)\n",
    "\n",
    "\" v(t) - moving average of a range of value to soomth the series \"\n",
    "\" v(t) = i/3 * (w(t-1) + w(t) + w(t+1)) \"\n",
    "\n",
    "\n",
    "#### Autoregressions (Smooth Trend)\n",
    "\n",
    "\" x(t) - a regression prediction of current value as a function of the past n values \"\n",
    "\" x(t) = x(t-1) - .9x(t-2) + w(t) \"\n",
    "\n",
    "\n",
    "#### Random Walk ()\n",
    "\n",
    "X(t) = X(t-1) + Er(t)\n",
    "\n",
    "#### Random Walk with Drift (Global Trend)\n",
    "\n",
    "\" X(t) - A random walk model with a drift, if drift = 0, it is simply a random walk \"\n",
    "\" X(t) = d + X(t-1) + w(t) \" # d = drift \n",
    "\n",
    "\n",
    "\n",
    "#### Signal in Noise (Periodic trend)\n",
    "\n",
    "\" real signal + noise => x(t) = s(t) + v(t) | s:signal, v:noise correlated with t\"\n",
    "\" p(t) - using cosin wave model to micmic real signal\"\n",
    "\" p(t) = 2cos(2pie * (t+15) / 50) + w(t) \" # Add noise\n",
    "\n",
    "\n",
    "\n",
    "### [2] Measure of Dependence >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "\n",
    "#### Mean Function\n",
    "\n",
    "\" u - the usual expected value \"\n",
    "-- \"Moving Average Series\" : E(v(t)) = 0 # White noise average = 0\n",
    "-- \"Random Walk with Drift\" : E(x(t)) = d # drift will be expected\n",
    "-- \"Signal Plus Nosie\" : E(p(t)) = 2cos(2pie*(t+15)/50) # Just the cosin wave\n",
    "\n",
    "\n",
    "#### Autocovariance Function\n",
    "\n",
    "-- \"White Noise\" : cov(w(s), w(t)) = {s=t then SD, s<>t then 0}\n",
    "-- \"Moving Average Series (t-1 + t + t+1)\" : cov(v(s),v(t)) = {s=t then 3/9 * sd, \n",
    "\t                                                          |s-t|=1 then 2/9 * sd, \n",
    "\t                                                          |s-t|=2 then 1/9 * sd, \n",
    "\t                                                          |s-t| > 2 then 0}\n",
    "-- \"Random Walk\" : cov(x(s), x(t)) = min{s,t} sd\n",
    "\n",
    "#### Autocorrelation Function (ACF)\n",
    "\n",
    "\" ACF - effect between x(t) and x(t+h) \" \n",
    "\n",
    "#### Partial autocorrelation function (PACF)\n",
    "\n",
    "\" PACF - effect between x(t) and x(t+h) removing effect from x(t+1), ... , x(t+h-1) \"\n",
    "\n",
    "#### Cross-Covariance Function\n",
    "\n",
    "\"Between two series x(t), y(t) \"\n",
    "\n",
    "#### Cross-Correlation Function\n",
    "\n",
    "\"Between two series x(t), y(t) \"\n",
    "\n",
    "\n",
    "\n",
    "### [3] Stationarity >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "##### Dickey Fuller Test of Stationarity\n",
    "X(t) = Rho * X(t-1) + Er(t)  X(t) - X(t-1) = (Rho - 1) X(t - 1) + Er(t)\n",
    "\n",
    "We have to test if Rho – 1 is significantly different than zero or not. If the null hypothesis gets rejected, we’ll get a stationary time series.\n",
    "\n",
    "Stationary testing and converting a series into a stationary series are the most critical processes in a time series modelling. You need to memorize each and every detail of this concept to move on to the next step of time series modelling.\n",
    "\n",
    "##### Stationary Series\n",
    "There are three basic criterion for a series to be classified as stationary series :\n",
    "\n",
    "The mean of the series should not be a function of time rather should be a constant. The image below has the left hand graph satisfying the condition whereas the graph in red has a time dependent mean.\n",
    "\n",
    "The variance of the series should not a be a function of time. This property is known as homoscedasticity. Following graph depicts what is and what is not a stationary series. (Notice the varying spread of distribution in the right hand graph)\n",
    "\n",
    "The covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the ‘red series’.\n",
    "\n",
    "\n",
    "### [4] Model Selection Metrics & DEA plots >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "#### --- Metrics to choose Models\n",
    "\n",
    "- \" MSE - mean squared error \"\n",
    "- \" F = MSR/MSE \"\n",
    "- \" R^2 \"\n",
    "- \" AIC - p49 small better (tends to be superior in smaller samples where relative number of parameter is large) \"\n",
    "- \" AICc - p49 \"\n",
    "- \" BIC - p50 small better (Tends to be superior in large samples, choose smaller model - higher penality \"\n",
    "\n",
    "#### --- Stationarize Methods\n",
    "\n",
    "\" It is necessary for time-series data to be stationary >>> \"\n",
    "\n",
    "#### 1) Detrending\n",
    "\n",
    "\" Assume -- Trend Stationary (Detrending) \"\n",
    "\n",
    "x(t) = u(t) + y(t) # y(t) is a stationary process\n",
    "\n",
    "\n",
    "#### 2) Differencing\n",
    "\n",
    "\" Assume -- Random Walk With Drift (Differencing) \"\n",
    "\n",
    "u(t) = d + u(t-1) + w(t) # First differencing\n",
    "\n",
    "\" BackShift Operator \" k>0\n",
    "\" ForwardShift Operator \" k>0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3) Transformation\n",
    "\n",
    "\" Equalize variability over the length of single series \"\n",
    "\n",
    "\" Log Transformation \"\n",
    "\n",
    "\" Box-Cox Transformation \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### --- DEA Plotting\n",
    "\n",
    "#### 1) Scatter lag plot\n",
    "\n",
    "\" find nonlinear relationship x(t) - x(t-6) or x(t) - y(t-3) \"\n",
    "\n",
    "#### 2) Regression \n",
    "\n",
    "\" To discover a Signal in Noise \"\n",
    "\n",
    "\n",
    "#### --- Smoother for Time Series\n",
    "\n",
    "#### 1) Moving Average Soomther \n",
    "\n",
    "\" useful in discovering certain traits in a time series, long term trebd, seseanal components \"\n",
    "\" Moving with weights to the positions \"\n",
    "\n",
    "\n",
    "#### 2) Kernel Smoothing \n",
    "\n",
    "\" use a weighted function (normal kernel) \"\n",
    "\" The wider the bandwith - b --> the smoother the result > ksmooth \"\n",
    "\n",
    "\n",
    "#### 3) Lowess - Nearest Neighbor Regression\n",
    "\n",
    "\" The bigger % data used to train, the smoother \"\n",
    "\n",
    "\n",
    "#### 4) Smoothing Splines\n",
    "\n",
    "\" fit a polynomial regression in terms of time \"\n",
    "\n",
    "\n",
    "#### ** Smoothing can also being used to determine nonlinear relationship between series \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step1 - Visualize the Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 - Stationarize the Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 - Identify the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 - Building the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 - Make the Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
