{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. You penalize your loss function by adding a multiple of an L1L1 (LASSO[2]) or an L2L2 (Ridge[3]) norm of your weights vector ww (it is the vector of the learned parameters in your linear regression). regularization artificially discourages complex or extreme explanations of the world even if they fit the what has been observed better. The idea is that such explanations are unlikely to generalize well to the future; they may happen to explain a few data points from the past well, but this may just be because of accidents of the sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Ridge Regressions (L2)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Work best when least square estimate has high variance; Deal with high-dimensional data but shrink coefficients; If many Ps contribute Y, Ridge is good; Deal multi-colinearity; \n",
    "\n",
    "Cons: No feature selection performed; Good for grouping (if some features identical, coefficients are same)\n",
    "\n",
    "#### LASSO Regressions (L1)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Work best when least square estimate has high variance; Deal exetremly well with high-dimensional data but shrink coefficients; If only a few feature actually useful, LASSO is good; Deal multi-colinearity; \n",
    "\n",
    "Cons: Feature selection performed; Good for elimiating travil features; If groups highly correlated features, pick one, others shrink to 0; \n",
    "\n",
    "#### Elastic Net\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Always preferred over L1, L2 specifically.\n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Least-Angle Regression (LARS)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: 1.It is computationally just as fast as forward selection; 2.It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model; 3.If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable; 4.It is easily modified to produce solutions for other estimators, like the lasso.; 5.It is effective in contexts where p >> n (i.e., when the number of dimensions is significantly greater than the number of points.\n",
    "\n",
    "\n",
    "Cons: Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise; 2.Since almost all high dimensional data in the real world will just by chance exhibit some fair degree of collinearity across at least some variables, the problem that LARS has with correlated variables may limit its application to high dimensional data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Ridge Regressions (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "L2 regularization. {lambda X beta^2}\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "lambda = shrink parameter [infinite: null model <--> 0:least square]\n",
    "#### Cost Function: \n",
    "Add panelity term into the RSS minimization \n",
    "#### Process Flow: \n",
    "Standarize features ->\n",
    "Shrink coefficients towards 0 but never at 0 (Shrink by %)\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "library(glmnet)\n",
    "\n",
    "# Create X matrix and Y vector\n",
    "x <- model.matrix(Y ~ . , data)[, -1]\n",
    "Y <- data$Y\n",
    "\n",
    "# Ridge Regression\n",
    "grid <- 10^seq(10, -2, length = 100)\n",
    "table.ridge <- glmnet(x, y, alpha=0, lambda = grid) # alpha = 0 ridge, 1 lasso\n",
    "\n",
    "# See a lambda value in the grid\n",
    "table.ridge$lambda[3] # 1-100\n",
    "coef(table.ridge)[,3] # 1-100\n",
    "sqrt(sum(coef(table.ridge)[-1,3]^2))\n",
    "\n",
    "# Get coefficients for a new lambda value\n",
    "predict(table.ridge, S=[newvale], type=\"coefficients\")[1:20,] # top 20 features\n",
    "\n",
    "# Croos validate to find the best lambda\n",
    "set.seed(10)\n",
    "table.cv <- cv.glmnet(trainX, trainY, alpha=0)\n",
    "best.lam <- table.cv$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- LASSO Regressions (L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "L1 regularization. {lambda X |beta|}\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "lambda = shrink parameter [infinite: null model <--> 0:least square]\n",
    "#### Cost Function: \n",
    "Add panelity term into the RSS minimization \n",
    "#### Process Flow: \n",
    "Standarize features ->\n",
    "Shrink coefficients towards 0 some features will be at 0 (Shrink by #)\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "\n",
    "library(glmnet)\n",
    "table.lasso = glmnet(trainX, trainY, alpha=1, lambda=grid) # alpha = 1 lasso\n",
    "\n",
    "# CV\n",
    "set.seed(10)\n",
    "table.cv <- cv.glmnet(trainX, trainY, alpha=1)\n",
    "best.lam <- table.cv$lambda.min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods. The elastic net method overcomes the limitations of the LASSO (least absolute shrinkage and selection operator) method. Use of this penalty function has several limitations.[1] For example, in the \"large p, small n\" case (high-dimensional data with few examples), the LASSO selects at most n variables before it saturates. Also if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others. To overcome these limitations, the elastic net adds a quadratic part to the penalty (∥ β ∥^2) which when used alone is ridge regression (known also as Tikhonov regularization). \n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
    "\n",
    "The quadratic penalty term makes the loss function strictly convex, and it therefore has a unique minimum. The elastic net method includes the LASSO and ridge regression: in other words, each of them is a special case where λ 1   = λ , λ 2   = 0 or λ 1   = 0 , λ 2   = λ. Meanwhile, the naive version of elastic net method finds an estimator in a two-stage procedure : first for each fixed λ 2     it finds the ridge regression coefficients, and then does a LASSO type shrinkage. This kind of estimation incurs a double amount of shrinkage, which leads to increased bias and poor predictions. To improve the prediction performance, the authors rescale the coefficients of the naive version of elastic net by multiplying the estimated coefficients by ( 1 + λ 2   )  \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------- R\n",
    "\"\"\"\n",
    "Glmnet: Lasso and elastic-net regularized generalized linear models is software which is implemented as an R source \n",
    "package.[8][9] This includes fast algorithms for estimation of generalized linear models with ℓ1 (the lasso), \n",
    "ℓ2 (ridge regression) and mixtures of the two penalties (the elastic net) using cyclical coordinate descent, \n",
    "computed along a regularization path.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------- Python\n",
    "\"\"\"\n",
    "scikit-learn includes linear regression, logistic regression and linear support vector machines with \n",
    "elastic net regularization.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Least-Angle Regression (LARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics, least-angle regression (LARS) is an algorithm for fitting linear regression models to high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. \n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one's correlations with the residual.\n",
    "\n",
    "[1] The basic steps of the Least-angle regression algorithm are:\n",
    "\n",
    "[2] Start with all coefficients β j equal to zero.\n",
    "\n",
    "[3] Find the predictor x j most correlated with y \n",
    "\n",
    "[4] Increase the coefficient β j  in the direction of the sign of its correlation with y. Take residuals r = y − y ^ along the way. Stop when some other predictor x has as much correlation with r as x j has.\n",
    "\n",
    "[5] Increase (β j, β k) in their joint least squares direction, until some other predictor x m     has as much correlation with the residual r.\n",
    "\n",
    "[6] Continue until: all predictors are in the model\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "\"\"\"\n",
    "Least-angle regression is implemented in R via the lars package\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "\"\"\"\n",
    "http://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
