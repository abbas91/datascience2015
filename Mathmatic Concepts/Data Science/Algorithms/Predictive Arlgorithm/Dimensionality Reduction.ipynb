{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration,[1] via obtaining a set of principal variables. It can be divided into feature selection and feature extraction. Dimension Reduction refers to the process of converting a set of data having vast dimensions into data with lesser dimensions ensuring that it conveys similar information concisely. These techniques are typically used while solving machine learning problems to obtain better features for a classification or regression task.\n",
    "\n",
    "https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf\n",
    "\n",
    "DR helps on classifier – help reduce overfitting: The reason is that DR can remove two types of “noise” from the input: (1) independent random noise, which is uncorrelated with the input and the label, and mostly perturbs points away from the data manifold. Simply running PCA, or other unsupervised DR algorithm, with an adequate number of components, can achieve this to some extent. (2) Unwanted degrees of freedom, which are possibly nonlinear, along which the input changes but the label does not. This more radical form of denoising requires the DR to be informed by the labels, of course, and is commonly called supervised DR.\n",
    "\n",
    "\n",
    "- Linear dimension reduction: \n",
    "f1(x1,x2,x3,…), f2(x1,x2,x3,…) … -> f() is linear function\n",
    "\n",
    "https://stat.columbia.edu/~cunningham/pdf/CunninghamJMLR2015.pdf\n",
    "\n",
    "--Linear dimensionality reduction methods have been developed throughout statistics, machine learning, and applied _elds for over a century, and these methods have become indispensable\n",
    "tools for analyzing high dimensional, noisy data. These methods produce a low-dimensional linear mapping of the original high-dimensional data that preserves some feature of interest in the data. Accordingly, linear dimensionality reduction can be used for visualizing or exploring structure in data, denoising or compressing data, extracting meaningful feature spaces, and more.\n",
    "\n",
    "- Non-linear dimension reduction: \n",
    "f1(x1,x2,x3,…), f2(x1,x2,x3,…) … -> f() is non-linear function\n",
    "\n",
    "https://www.cs.utah.edu/~piyush/teaching/25-10-slides.pdf\n",
    "\n",
    "-- High-dimensional data, meaning data that requires more than two or three dimensions to represent, can be difficult to interpret. One approach to simplification is to assume that the data of interest lie on an embedded non-linear manifold within the higher-dimensional space. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space. Non-linear methods can be broadly classified into two groups: those that provide a mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa), and those that just give a visualisation. In the context of machine learning, mapping methods may be viewed as a preliminary feature extraction step, after which pattern recognition algorithms are applied. Typically those that just give a visualisation are based on proximity data – that is, distance measurements.\n",
    "\n",
    "** Data structure linear or non-linear?\n",
    "\n",
    "=========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Principal Component Analysis (PCA) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: First pca has the most information; Decorate features, Reduce dimwnsions; It’s very efficient (via Lanczos algorithm or similar it can be applied to really big matrices.; The basis is hierarchical, ordered by relevance.; It tends to perform quite well for most data sets\n",
    "\n",
    "Cons: If features evenly contribute relationship, pca performs poorly; (Un-supervised) No Y involved so no guarantee that X explain Y well; If the data is strongly non-linear it may not work so well; Results are not always the best for visualization; It is difficult to interpret; Strongly focused in variance, sometimes there’s not a direct relationship between variance and predictive power so it can discard useful information\n",
    "\n",
    "\n",
    "#### Partial Least Sqaure (PLS) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: (supervised) Y involved in the process so good explaination with both X and Y; Decorate features, Reduce dimwnsions; \n",
    "\n",
    "Cons: \n",
    "\n",
    "#### Multiple Corespoundence Analysis (MCA) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Canonical correlations analysis (CCA) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Good for inference cross – correlation between two datasets; Canonical correlation analysis, on the other hand, is a method for comparing draws from two different multivariate distributions.; No assumption of normality; Supervised DR; \n",
    "\n",
    "Cons: Sensitive to small data change; Not identify non-linear relationship; Affect by pattern of missing rather than amount of missing; Only 2 (X, Y) – not real case >2 factors; \n",
    "\n",
    "\n",
    "#### Maximum autocorrelation factors (MAF) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: keep the spatial autocorrelation in data; \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Slow feature analysis (SFA) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: SFA can be applied hierarchically to process high-dimensional input signals and extract complex features; \n",
    "\n",
    "Cons: Unsupervised learning; \n",
    "\n",
    "\n",
    "#### Sufficient dimensionality reduction (SDR) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Reduce dimensions while maintain all information; \n",
    "\n",
    "Cons: Need complete data (Imputation);\n",
    "\n",
    "\n",
    "\n",
    "#### Locality preserving projections (LPP) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Since it capture data local structure; linear so fast but also capture some non-linear properties (Assume manifold is non-linear subspace)(Better than PCA); can be easily employed to unseen data; \n",
    "\n",
    "Cons: Need batch training; \n",
    "\n",
    "\n",
    "\n",
    "#### Under-complete independent component analysis (ICA) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Linear regression, distance metric learning (DML) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Probabilistic PCA (PPCA) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Factor analysis (FA) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Multi-dimensional Scaling (MDS) [Linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Discriminant Analysis (LDA, MDA, QDA, FDA) [Linear | Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Supervised dimension reduction;  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Sammon Mapping\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Nonlinear metric multidimensional scaling; \n",
    "\n",
    "Cons: Nonlinear transformation makes it difficult to use for classification application; \n",
    "\n",
    "\n",
    "\n",
    "#### Self-Organizing Map (SOM) [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Unsupervised learning; Robust to missing/partial data; Good for visualize high-dimensional data; Dimension reduction method; Good to use as process data before clustering (Preserve structure or topology of input data); Easy to understand (close = similar) \n",
    "\n",
    "Cons: Very computationally expensive (More dimensions, more K = more slow); Contradict when organize data while preserve structure – may blur important relationship in data; Need right data which you need a value for each dimension of each member of samples in order to generate a map; every SOM is different and finds different similarites among the sample vectors so a lot of maps need to be constructed in order to get one final good map; \n",
    "\n",
    "\n",
    "#### Projection Pursuit [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: able to bypass the \"curse of dimensionality\" caused by the fact that high-dimensional space is mostly empty; projection pursuit is able to ignore irrelevant (i.e. noisy and information-poor) variables (distinct advantage over methods based on inter-point distances);  \n",
    "\n",
    "Cons: high demand on computer time.;\n",
    "\n",
    "\n",
    "\n",
    "#### Auto-encoders [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: No prior data knowledge needed; Contributed to boost performance in image classification challenges; Intuitive; Can find different levels of features; Probably state of the art for representing data at different levels; Can be trained to denoise data or to generate data\n",
    "\n",
    "Cons: Unsupervised DR; Costly to train, slow; Fuzzy design decisions (net, layers, learning parameters,…); Lack in theoretical justification; •Can overfit big time; Very nice in theory but not to many practical applications; Can be inefficient for massive data\n",
    "\n",
    "\n",
    "\n",
    "#### ISOMap [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Can work well when Data is strongly non-linear.; Can work very well for visualization\n",
    "\n",
    "Cons: Can be inefficient for large data; Certainly not a good idea unless the data is strongly non-linear; Sometimes they just work well for visualization but not for dimensionality reduction\n",
    "\n",
    "\n",
    "\n",
    "#### Locally-linear embedding [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "#### Non-linear PCA [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "#### Maximum Variance Unfolding [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Laplacian Eigenmaps [Non-linear]\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:  \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "PCA is mostly used as a tool in exploratory data analysis and for making predictive models. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute.[4] The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).[5]\n",
    "\n",
    "PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection or \"shadow\" of this object when viewed from its (in some sense; see below) most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.\n",
    "\n",
    "http://setosa.io/ev/eigenvectors-and-eigenvalues/\n",
    "\n",
    "- [EigenValue]: In matrix transformation, the scale (how much) the matrix scale or decrease (change) is the Eigen value.\n",
    "\n",
    "\n",
    "- [EigenVector]: In matrix transformation, the direction (striaght line) that the matrix scale or decrease (change) is the Eigen vector.\n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "Number of CPA \n",
    "#### Cost Function: \n",
    "Linear transformation to change the coordinate system to the 1st and second largest variance (decorlate)\n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "Identify the most variated direction in data and plot a striaght line in data and then do it for the second most variated direction (ortholognal to the first), repeat till N = number of features (Dimensions) in original data. Use those new lines as new coordinates in the N dimensional space (centered 0 in the cross of all lines). The coordinates on the first dimension of this new coordinate system is the first PCA, the second is the second PCA, repeat.\n",
    "\n",
    "- First componets: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = factor11(X1(1) - X1(mean)) + factor21(X2(1) - X2(mean)) + ... + factorP1(Xp(1) - Xp(mean))\n",
    "\n",
    "...\n",
    "\n",
    "Zn = factor1n(..              ) + factor2n(...             ) + ... + factorpn(...            ))  \n",
    "\n",
    "- Second componets: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = ...\n",
    "\n",
    "...\n",
    "\n",
    "Zn = ...\n",
    "\n",
    "*condition --- cpa2:{z1, .., Zn} is prependicular to cpa1:{z1, ... , zn} \n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "# Load data\n",
    "data(iris)\n",
    "head(iris, 3)\n",
    "\n",
    "# log transform \n",
    "log.ir <- log(iris[, 1:4])\n",
    "ir.species <- iris[, 5]\n",
    " \n",
    "# apply PCA - scale. = TRUE is highly \n",
    "# advisable, but default is FALSE. \n",
    "ir.pca <- prcomp(log.ir,\n",
    "                 center = TRUE,\n",
    "                 scale. = TRUE) \n",
    "\n",
    "# print method\n",
    "print(ir.pca)\n",
    "\n",
    "# plot method\n",
    "plot(ir.pca, type = \"l\")\n",
    "\n",
    "# summary method\n",
    "summary(ir.pca)\n",
    "\n",
    "# Detail ploting\n",
    "library(devtools)\n",
    "install_github(\"ggbiplot\", \"vqv\")\n",
    " \n",
    "library(ggbiplot)\n",
    "g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, \n",
    "              groups = ir.species, ellipse = TRUE, \n",
    "              circle = TRUE)\n",
    "g <- g + scale_color_discrete(name = '')\n",
    "g <- g + theme(legend.direction = 'horizontal', \n",
    "               legend.position = 'top')\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Partial Least Sqaure (PLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.\n",
    "\n",
    "PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized).\n",
    "#### Input Data: \n",
    "X (Numeric)\n",
    "#### Initial Parameters: \n",
    "Number of PLS\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "- First PLS: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = factor11(X1(1) - X1(mean)) + factor21(X2(1) - X2(mean)) + ... + factorP1(Xp(1) - Xp(mean))\n",
    "\n",
    "...\n",
    "\n",
    "Zn = factor1n(..              ) + factor2n(...             ) + ... + factorpn(...            )) \n",
    "\n",
    "Simple regression - {Z1, ..., Zn}Y = beta1 * X1(1,...,n) => n * residuals(X1.res)\n",
    "\n",
    "...\n",
    "\n",
    "Simple regression - {z1, ..., Zn}Y = detan * Xp(1,...,n) => n * residuals(Xn.res)\n",
    "\n",
    "- Second PLS: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = factor11(X1.res(1) - X1.res(mean)) + factor21(X2.res(1) - X2.res(mean)) + ... + factorP1(Xp.res(1) - Xp.res(mean))\n",
    "\n",
    "...\n",
    "\n",
    "Zn = factor1n(..              ) + factor2n(...             ) + ... + factorpn(...            ))\n",
    "\n",
    "Simple regression - {Z1, ..., Zn}Y = beta1 * X1.res(1,...,n) => n * residuals(X1.res)\n",
    "\n",
    "...\n",
    "\n",
    "Simple regression - {z1, ..., Zn}Y = detan * Xp.res(1,...,n) => n * residuals(Xn.res)\n",
    "\n",
    "Repeat to M times ...\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "# https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf\n",
    "set.seed(1)\n",
    "table.pls <- plsr(Y ~ ., data = data, subset = train, sclae = TRUE, validation = \"cv\")\n",
    "summary(table.pls)\n",
    "validationplot(table.pls, val.type = \"MSEP\") # lowest MSE\n",
    "\n",
    "# Prediction\n",
    "prediction <- predict(table.pls, test, ncomp = 2) # number of pls = 2\n",
    "mean((prediction - test$Y)^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n",
    "Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n",
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(X, Y)\n",
    "\n",
    "Y_pred = pls2.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Multiple Corespoundence Analysis (MCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics, multiple correspondence analysis (MCA) is a data analysis technique for nominal categorical data, used to detect and represent underlying structures in a data set. It does this by representing data as points in a low-dimensional Euclidean space. The procedure thus appears to be the counterpart of principal component analysis for categorical data.[1][2] MCA can be viewed as an extension of simple correspondence analysis (CA) in that it is applicable to a large set of categorical variables.\n",
    "\n",
    "Multiple correspondence analysis (MCA) is an extension of corre-spondence analysis (CA) which allows one to analyze the pattern of relationships of several categorical dependent variables. As such, it can also be seen as a generalization of principal component anal- ysis when the variables to be analyzed are categorical instead of quantitative. Because MCA has been (re)discovered many times, equivalent methods are known under several different names such as optimal scaling, optimal or appropriate scoring, dual scaling, homogeneity analysis, scalogram analysis, and quantification me- thod.\n",
    "\n",
    "Technically MCA is obtained by using a standard correspon-dence analysis on an indicator matrix (i.e., a matrix whose entries are 0 or 1). The percentages of explained variance need to be cor-rected, and the correspondence analysis interpretation of inter-point distances needs to be adapted.\n",
    "\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "Number of MCA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "# http://www.sthda.com/english/wiki/multiple-correspondence-analysis-essentials-interpretation-and-application-to-investigate-the-associations-between-categories-of-multiple-qualitative-variables-r-software-and-data-mining#multiple-correspondence-analysis-mca\n",
    "\n",
    "install.packages(\"FactoMineR\")\n",
    "# install.packages(\"devtools\")\n",
    "devtools::install_github(\"kassambara/factoextra\")\n",
    "library(\"factoextra\")        \n",
    "\n",
    "# X : a data frame with n rows (individuals) and p columns (categorical variables)\n",
    "# ncp : number of dimensions kept in the final results.\n",
    "# graph : a logical value. If TRUE a graph is displayed.\n",
    "MCA(X, ncp = 5, graph = TRUE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "# https://github.com/esafak/mca\n",
    "# pip install --user mca\n",
    "\n",
    "import mca, pandas, numpy\n",
    "counts = pandas.read_table('data/burgundies.csv', sep=',', skiprows=1, index_col=0, header=0)\n",
    "print(counts.shape)\n",
    "\"(6, 23)\"\n",
    "\n",
    "mca_counts = mca.MCA(counts.drop('oak_type', axis=1))\n",
    "print(mca_counts.fs_r(1)) # 1 = 100%, meaning preserve all variance.\n",
    "\n",
    "\"\"\"\n",
    "        array([[ 0.87127085,  0.11448396, -0.09250792],\n",
    "               [-0.7209849 , -0.22896791, -0.083259  ],\n",
    "               [-0.93238083,  0.11448396, -0.02206285],\n",
    "               [-0.87127085,  0.11448396,  0.09250792],\n",
    "               [ 0.93238083,  0.11448396,  0.02206285],\n",
    "               [ 0.7209849 , -0.22896791,  0.083259  ]])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Canonical correlations analysis (CCA) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "https://en.wikipedia.org/wiki/Canonical_correlation\n",
    "\n",
    "PCA maximize variance while CCA maximize correlation. Canonical Correlation analysis is the analysis of multiple-X multiple-Y correlation.  The Canonical Correlation Coefficient measures the strength of association between two Canonical Variates.\n",
    "A Canonical Variate is the weighted sum of the variables in the analysis.  The canonical variate is denoted CV.  Similarly to the discussions on why to use factor analysis instead of creating unweighted indices as independent variables in regression analysis, canonical correlation analysis is preferable in analyzing the strength of association between two constructs.  This is such because it creates an internal structure, for example, a different importance of single item scores that make up the overall score (as found in satisfaction measurements and aptitude testing).\n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "Now suppose you are observing an event and you take a certain number of observations for the event. If we have 20 dimensions that were recorded for the given event, and we would like to see how well correlated the dimensions are we can divide the data into 2 sets of 10 dimension each.. Now we wish to understand how these two groups are related. We first take a vector in the 10 dimensional space and project all of the observations of group 1 on to it and take another vector and project all of the observations of group 2, now our objective is to find vector 1 and vector 2 such that the correlation between them is maximized. Ones these vectors are obtained we have to find another vector orthogonal to vector 1, similarly an orthogonal vector for vector 2 and the correlation between these vectors has to be maximized. In essence we are finding the new transformed space which are a linear combination of the existing variables for both events such that the correlation is maximized for individual dimensions.\n",
    "\n",
    "In statistics, canonical-correlation analysis (CCA) is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym) of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of the Xi and Yj which have maximum correlation with each other.[1] T. R. Knapp notes that \"virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables.\"[2] The method was first introduced by Harold Hotelling in 1936.[3]\n",
    "\n",
    "Given two column vectors X = ( x 1   , … , x n   ) of random variables with finite second moments, one may define the cross-covariance Σ X Y   = cov( X , Y to be the n × m   matrix whose ( i , j ) entry is the covariance cov ( x i   , y j ). In practice, we would estimate the covariance matrix based on sampled data from X and Y (i.e. from a pair of data matrices).\n",
    "\n",
    "Canonical-correlation analysis seeks vectors a and b such that the random variables a ′  X   and b′ Y maximize the correlation ρ = corr ( a ′  X , b ′  Y ). The random variables U = a′ X and V = b′ Y are the first pair of canonical variables. Then one seeks vectors maximizing the same correlation subject to the constraint that they are to be uncorrelated with the first pair of canonical variables; this gives the second pair of canonical variables. This procedure may be continued up to min {m , n} times.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------- R\n",
    "# https://stats.idre.ucla.edu/r/dae/canonical-correlation-analysis/\n",
    "“R as cancor or in FactoMineR or in CCP”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------- Python\n",
    "# “Scikit-Learn, Python as Cross decomposition”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Maximum autocorrelation factors (MAF) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "http://orbit.dtu.dk/files/2779461/imm209.pdf\n",
    "http://www.ccgalberta.com/ccgresources/report10/2008-406_maf.pdf\n",
    "\n",
    "Data dimension reduction can also be performed with the method of Maximum Autocorrelation Factors (MAF) which seeks to keep the spatial autocorrelation in data.\n",
    "maximum autocorrelation factor (MAF) analysis was originally proposed as an alternative transformation of multivariate spatial imagery to the celebrated PCA transform by Paul Switzer [18]. In the MAF analysis we seek a transformation that maximizes the autocorrelation between neighbouring observations (i.e. pixels). The basic assumption of the MAF analysis is that the interesting signal exhibits high autocorrelation, whereas the noise exhibits low autocorrelation. By building the additional information of the structure of the observations into the model application examples show a more satisfying ordering and compression of the data (cf. [3, 7, 16, 15]). This is particularly the case when some noise components have higher variance than some signal components. In this case the principal components will fail to give an intuitive order of image quality. The MAF analysis requires knowledge of or estimation of the variance-covariance matrix of the data as well as the variance-covariance matrix of the difference between the original data and a spatially shifted version of the data. It may be formulated as a canonical correlation analysis problem [9].\n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "In the MAF analysis we seek a transformation that maximizes the autocorrelation between neighboring observations (i.e. pixels). Resulting new variables are ordered so that the first MAF is the linear combination that exhibits maximum autocorrelation. The Pth MAF is the linear combination that exhibits the highest autocorrelation subject to it being uncorrelated to the previous MAFs.\n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- R\n",
    "\n",
    "# https://www.rdocumentation.org/packages/tofsims/versions/1.0.2/topics/MNF\n",
    "testImage<-MassImage('dummy')\n",
    "testImage<-MNF(testImage)\n",
    "image(analysis(testImage,1), comp = 1)\n",
    "library(tofsimsData)\n",
    "data(tofsimsData)\n",
    "MNF(testImage)\n",
    "image(analysis(testImage,1), comp = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------- Python\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Slow feature analysis (SFA) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "http://www.scholarpedia.org/article/Slow_feature_analysis\n",
    "\n",
    "Slow feature analysis (SFA) is an unsupervised learning algorithm for extracting slowly varying features from a quickly varying input signal. It has been successfully applied, e.g., to the self-organization of complex-cell receptive fields, the recognition of whole objects invariant to spatial transformations, the self-organization of place-cells, extraction of driving forces, and to nonlinear blind source separation.\n",
    "\n",
    "- Hierarchical SFA networks for high-dimensional data (Non-linear)\n",
    "\n",
    "One natural solution to this problem is to apply SFA to subsets of the input, extract the slowest-varying features for each subset, and then use the concatenation of these solutions as the input for another iteration of SFA. At each step, a larger fraction of the input data is integrated into the new solution. In this way, the curse of dimensionality can be avoided, although, in general, the final slow features extracted need not be identical to the global solution obtained with the original, complete input. Thus, the splitting of the data into smaller patches relies on the locality of feature correlations in the input data, which typically holds for natural images.\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "Figure 1: Schematics of the optimization problem solved by Slow Feature Analysis. Given a set of time-varying input signals, x(t), SFA learns instantaneous, non-linear functions g(x) that transform x into slowly-varying output signals, y(t). The optimization procedure guarantees that SFA returns the global optimum for g (i.e., the slowest output signal) in a given function space. As the transformations must be instantaneous, trivial solution like low-pass filtering are not possible.\n",
    "\n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "Invariant features of temporally varying signals are useful for analysis and classification. Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal. It is based on a nonlinear expansion of the input signal and application of principal component analysis to this expanded signal and its time derivative. It is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decorrelated features, which are ordered by their degree of invariance. SFA can be applied hierarchically to process high-dimensional input signals and extract complex features. SFA is applied first to complex cell tuning properties based on simple cell output, including disparity and motion. Then more complicated input-output functions are learned by repeated application of SFA. Finally, a hierarchical network of SFA modules is presented as a simple model of the visual system. The same unstructured network can learn translation, size, rotation, contrast, or, to a lesser degree, illumination invariance for one-dimensional objects, depending on only the training stimulus. Surprisingly, only a few training objects suffice to achieve good generalization to new objects. The generated representation is suitable for object recognition. Performance degrades if the network is trained to learn multiple invariances simultaneously.\n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------- R\n",
    "# https://cran.r-project.org/web/packages/rSFA/rSFA.pdf\n",
    "Library(rSFA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------- Python\n",
    "# http://mdp-toolkit.sourceforge.net/tutorial/tutorial.html#tutorial\n",
    "# “Allow Non-linear SFA”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Sufficient dimensionality reduction (SDR) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "https://en.wikipedia.org/wiki/Sufficient_dimension_reduction\n",
    "In statistics, sufficient dimension reduction (SDR) is a paradigm for analyzing data that combines the ideas of dimension reduction with the concept of sufficiency.\n",
    "Dimension reduction has long been a primary goal of regression analysis. Given a response variable y and a p-dimensional predictor vector x, regression analysis aims to study the distribution of y | x, the conditional distribution of y given x. A dimension reduction is a function R ( x ) that maps x to a subset of R  k, k < p, thereby reducing the dimension of x. For example, R ( x ) may be one or more linear combinations of x.\n",
    "A dimension reduction R ( x ) is said to be sufficient if the distribution of y |  R ( x ) is the same as that of y |  x. In other words, no information about the regression is lost in reducing the dimension of x if the reduction is sufficient.\n",
    "\n",
    ">>> R Package (dr)\n",
    "SIR (Sliced Inverse Regression)\n",
    "SAVE (Sliced Average Variance Estimation)\n",
    "PHD (Principal Hessian Direction)\n",
    "IRE (Inverse Regression Estimation)\n",
    "\n",
    ">>> R Package (ldr)\n",
    "CORE (Covariance Reduction)\n",
    "LAD (Likelihood Acquired Direction)\n",
    "PFC (Principal Fitted Components)\n",
    "\n",
    ">>> Has Non-linear Version\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "MLE\n",
    "#### Process Flow: \n",
    "\n",
    "http://www.bios.unc.edu/research/bias/documents/unc.pdf\\\n",
    "\n",
    "http://emma.memect.com/t/b7b0ecc616f5c4f34466597e441a94e904a7fc5fd70f116e430eea2a92af665b/Dimension%20Reduction.pdf\n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------ R\n",
    "library(ldr)\n",
    "library(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Locality preserving projections (LPP) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "LPP based techniques to be a natural alternative to PCA based techniques in exploratory data analysis, information retrieval, and pattern classification applications.\n",
    "Many problems in information processing involve some form of dimensionality reduction. In this paper, we introduce Locality Preserving Projections (LPP). These are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set. LPP should be seen as an alternative to Principal Component Analysis (PCA) – a classical linear technique that projects the data along the directions of maximal variance. When the high dimensional data lies on a low dimensional manifold embedded in the ambient space, the Locality Preserving Projections are obtained by finding the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the manifold. As a result, LPP shares many of the data representation properties of nonlinear techniques such as Laplacian Eigenmaps or Locally Linear Embedding. Yet LPP is linear and more crucially is defined everywhere in ambient space rather than just on the training data points.\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "https://papers.nips.cc/paper/2359-locality-preserving-projections.pdf\n",
    "\n",
    "It builds a graph incorporating neighborhood information of the data set. Using the notion of the Laplacian of the graph, we then compute a transformation matrix which maps the data points to a subspace. This linear transformation optimally preserves local neighborhood information in a certain sense. The representation map generated by the algorithm may be viewed as a linear discrete approximation to a continuous map that naturally arises from the geometry of the manifold [2]. The new algorithm is interesting from a number of perspectives.\n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------- R\n",
    "\n",
    "# https://rdrr.io/github/bbuchsbaum/neuropls/src/R/lpp.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Python\n",
    "# https://github.com/jakevdp/lpproj\n",
    "# $ pip install lpproj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Under-complete independent component analysis (ICA) [Linear]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Linear regression, distance metric learning (DML) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Probabilistic PCA (PPCA) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Factor analysis (FA) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------- Multi-dimensional Scaling (MDS) [Linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Discriminant Analysis (LDA, MDA, QDA, FDA) [Linear | Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ Self-Organizing Map (SOM) [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "http://www.cs.bham.ac.uk/~jxb/NN/l16.pdf The principal goal of an SOM is to transform an incoming signal pattern of arbitrary dimension into a one or two dimensional discrete map, and to perform this transformation adaptively in a topologically ordered fashion. A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space. This makes SOMs useful for visualizing low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network.[1][2] The Kohonen net is a computationally convenient abstraction building on work on biologically neural models from the 1970s[3] and morphogenesis models dating back to Alan Turing in the 1950s.[4] Like most artificial neural networks, SOMs operate in two modes: training and mapping. \"Training\" builds the map using input examples (a competitive process, also called vector quantization), while \"mapping\" automatically classifies a new input vector. A self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. The usual arrangement of nodes is a two-dimensional regular spacing in a hexagonal or rectangular grid. The self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space. The procedure for placing a vector from data space onto the map is to find the node with the closest (smallest distance metric) weight vector to the data space vector.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "\n",
    "#### Initial Parameters: \n",
    "Initial weights; Number of output nodes; K(default = 1, competitive learning); Learning rate(Update steps); Distance Metrics;\n",
    "\n",
    "#### Cost Function: \n",
    "D(X,J) – Only single X Competitive, winner-takes-all\n",
    "\n",
    "#### Process Flow: \n",
    "Process: Continuous high dimensional input space map to a corresponding discrete low dimensional output space (2 dimensional map) \n",
    "\n",
    "1.Randomize the map's nodes' weight vectors \n",
    "\n",
    "2.Grab an input vector D ( t ) ",
    "\n",
    "\n",
    "3.Traverse each node in the map 1.Use the Euclidean distance formula to find the similarity between the input vector and the map's node's weight vector 2.Track the node that produces the smallest distance (this node is the best matching unit, BMU)\n",
    "\n",
    "4.Update the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector 1.W v ( s + 1 ) = W v ( s ) + θ ( u , v , s ) ⋅ α ( s ) ⋅ ( D ( t ) − W v ( s ) ) ",
    "\n",
    "\n",
    "5.Increase s and repeat from step 2 while s < λ\n",
    "\n",
    "A variant algorithm: \n",
    "\n",
    "1.Randomize the map's nodes' weight vectors \n",
    "\n",
    "2.Traverse each input vector in the input data set 1.Traverse each node in the map 1.Use the Euclidean distance formula to find the similarity between the input vector and the map's node's weight vector 2.Track the node that produces the smallest distance (this node is the best matching unit, BMU)\n",
    "\n",
    "2.Update the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector\n",
    "\n",
    "1.W v ( s + 1 ) = W v ( s ) + θ ( u , v , s ) ⋅ α ( s ) ⋅ ( D ( t ) − W v ( s ) )\n",
    "\n",
    "3.Increase s and repeat from step 2 while s < λ\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------- R\n",
    "# https://www.r-bloggers.com/self-organising-maps-for-customer-segmentation-using-r/\n",
    "# Load the kohonen package \n",
    "require(kohonen)\n",
    "\n",
    "# Create a training data set (rows are samples, columns are variables\n",
    "# Here I am selecting a subset of my variables available in \"data\"\n",
    "data_train <- data[, c(2,4,5,8)]\n",
    "\n",
    "# Change the data frame with training data to a matrix\n",
    "# Also center and scale all variables to give them equal importance during\n",
    "# the SOM training process. \n",
    "data_train_matrix <- as.matrix(scale(data_train))\n",
    "\n",
    "# Create the SOM Grid - you generally have to specify the size of the \n",
    "# training grid prior to training the SOM. Hexagonal and Circular \n",
    "# topologies are possible\n",
    "som_grid <- somgrid(xdim = 20, ydim=20, topo=\"hexagonal\")\n",
    "\n",
    "# Finally, train the SOM, options for the number of iterations,\n",
    "# the learning rates, and the neighbourhood are available\n",
    "som_model <- som(data_train_matrix, \n",
    "                 grid=som_grid, \n",
    "                 rlen=100, \n",
    "                 alpha=c(0.05,0.01), \n",
    "                 keep.data = TRUE,\n",
    "                 n.hood=“circular” )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python\n",
    "\n",
    "# http://peterwittek.com/somoclu-in-python.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import somoclu\n",
    "%matplotlib inline  \n",
    "c1 = np.random.rand(50, 3)/5\n",
    "c2 = (0.6, 0.1, 0.05) + np.random.rand(50, 3)/5\n",
    "c3 = (0.4, 0.1, 0.7) + np.random.rand(50, 3)/5\n",
    "data = np.float32(np.concatenate((c1, c2, c3)))\n",
    "colors = [\"red\"] * 50\n",
    "colors.extend([\"green\"] * 50)\n",
    "colors.extend([\"blue\"] * 50)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=colors)\n",
    "labels = range(150)\n",
    "\n",
    "n_rows, n_columns = 100, 160\n",
    "som = somoclu.Somoclu(n_columns, n_rows, data=data)\n",
    "%time som.train()\n",
    "som.view_component_planes()\n",
    "# More.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Projection Pursuit [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "Projection pursuit (PP) is a type of statistical technique which involves finding the most \"interesting\" possible projections in multidimensional data. Often, projections which deviate more from a normal distribution are considered to be more interesting. As each projection is found, the data are reduced by removing the component along that projection, and the process is repeated to find new projections; this is the \"pursuit\" aspect that motivated the technique known as matching pursuit.\n",
    "The idea of projection pursuit is to locate the projection or projections from high-dimensional space to low-dimensional space that reveal the most details about the structure of the data set. Once an interesting set of projections has been found, existing structures (clusters, surfaces, etc.) can be extracted and analyzed separately.\n",
    "Projection pursuit has been widely used for blind source separation, so it is very important in independent component analysis. Projection pursuit seeks one projection at a time such that the extracted signal is as non-Gaussian as possible\n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/mvahtmlnode115.html\n",
    "The aim of projection pursuit is to reveal possible nonlinear and therefore interesting structures hidden in the high-dimensional data. To what extent these structures are \"\"interesting'' is measured by an index. \n",
    "The traditional approach to examining these high-dimensional data sets is to reduce their dimensionality, usually by linear and/or nonlinear mapping or projection strategies (see Crawford and Fall (10)). Humans are very good at visual pattern recognition, and projecting the data set down to one-, two-, or even three-dimensional space allows this ability to be utilized. However, projection is a data-smoothing operation, since existing structure can only be obscured by a projection and never enhanced by it (see Friedman (11)). \n",
    "\n",
    "The idea of projection pursuit is to locate the projection or projections from high- to low-dimensional space that reveal the most details about the structure of the data set. Once an interesting set of projections has been found, existing structures (clusters, surfaces, etc.) can be extracted and analyzed separately. There are two general approaches taken to projection pursuit: manual and automatic. \n",
    "\n",
    "The most basic form of manual projection pursuit is the scatterplot, which is, in its most simple form, a two-dimensional display to indicate data characteristics over two selected dimensions at a time. It is quite simple to produce all  (2n) pair-wise scatterplots for N-dimensional space and perform analysis on these. Unfortunately, this method only allows structure across the two plotted dimensions to be discovered. When the number of dimensions to analyze grows very large, other projection methods must be considered. See Crawford and Fall (10) for more details. Variations on the use of scatterplots include the PRIM-9 system (Tukey et. al. (12)), PRIM-H (Donoho et. al. (13)), and Orion (McDonald (14)). \n",
    "The main limitation of manual projection pursuit is the amount of time it takes to exhaustively explore a given space. If one were to use Asimov's Grand Tour concept (15), which calls for presenting projections of the data set in a sequence with a difference of views so slight as to make the sequence similar to watching a movie, and make a complete search of a high-dimensional space, it would take approximately three hours to completely explore a four-dimensional space (see Huber (9)). Clearly, touring spaces of even higher dimensionality would be out of the question, unless perhaps one has some sense of what, based on a given projection, the next change in the view should be to produce the desired result. \n",
    "Friedman and Tukey devised a method (16) to automate the task of projection pursuit. Basically, they characterize a given projection by a numerical index that indicates the amount of structure that is present. This index can then be used as the basis for a heuristic search to locate the \"\"interesting'' projections. Different types of heuristic searches are suggested in Friedman and Tukey (16) and in Tukey and Tukey (17). \n",
    "Once structure has been found, it is then removed from the data. The data are then examined for further structure, which, if found, is also removed. This process continues until there is no remaining structure detectable within the data. A variety of ways to remove structure have been suggested; the interested reader should see Huber (9) and Friedman (11) for some significant examples. \n",
    "\n",
    "Projection pursuit methods are a great step forward in the problem of high-dimensional data analysis, although according to Crawford and Fall (10) they do have many limitations. One of the most common problems is the difficulty in determining just what the solutions from automatic projection pursuit methods (typically projection index values) actually mean. Also, most projection pursuit software doesn't possess the ability to make inferences, and so can get fooled by false structure. Finally, it is difficult, if not impossible, in general to algorithmically specify what constitutes structure in data.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- R\n",
    "# http://www.stat.wvu.edu/~jharner/courses/dsci504/docs/tourr.pdf\n",
    "Library(tour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Sammon Mapping [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Sammon mapping or Sammon projection is an algorithm that maps a high-dimensional space to a space of lower dimensionality (see multidimensional scaling) by trying to preserve the structure of inter-point distances in high-dimensional space in the lower-dimension projection. It is particularly suited for use in exploratory data analysis. The method was proposed by John W. Sammon in 1969.[1] It is considered a non-linear approach as the mapping cannot be represented as a linear combination of the original variables as possible in techniques such as principal component analysis, which also makes it more difficult to use for classification applications.\n",
    "The minimization can be performed either by gradient descent, as proposed initially, or by other means, usually involving iterative methods. The number of iterations need to be experimentally determined and convergent solutions are not always guaranteed. Many implementations prefer to use the first Principal Components as a starting configuration.[3]\n",
    "The Sammon mapping has been one of the most successful nonlinear metric multidimensional scaling methods since its advent in 1969, but effort has been focused on algorithm improvement rather than on the form of the stress function. The performance of the Sammon mapping has been improved by extending its stress function using left Bregman divergence [4] and right Bregman divergence.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "\n",
    "#### Cost Function: \n",
    "https://en.wikipedia.org/wiki/Sammon_mapping\n",
    "\n",
    "#### Process Flow: \n",
    "https://en.wikipedia.org/wiki/Sammon_mapping\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------- R\n",
    "\n",
    "# http://sites.stat.psu.edu/~dhunter/R/html/MASS/html/sammon.html\n",
    "\n",
    "data(swiss)\n",
    "swiss.x <- as.matrix(swiss[, -1])\n",
    "swiss.sam <- sammon(dist(swiss.x))\n",
    "plot(swiss.sam$points, type = \"n\")\n",
    "text(swiss.sam$points, labels = as.character(1:nrow(swiss.x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------- Python\n",
    "\n",
    "# https://github.com/adailsonfilho/sammon/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ----- Auto-encoders [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\n",
    "An autoencoder, autoassociator or Diabolo network[1]:19 is an artificial neural network used for unsupervised learning of efficient codings.[2][3] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data. \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "Architecturally, the simplest form of an autoencoder is a feedforward, non-recurrent neural network very similar to the multilayer perceptron (MLP) – having an input layer, an output layer and one or more hidden layers connecting them –, but with the output layer having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs (instead of predicting the target value Y   {\\displaystyle Y}  Y given inputs X   {\\displaystyle X}  X). Therefore, autoencoders are unsupervised learning models.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------- R\n",
    "# https://cran.r-project.org/web/packages/autoencoder/autoencoder.pdf\n",
    "Library(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Python\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- ISOMap [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "Isomap is a nonlinear dimensionality reduction method. It is one of several widely used low-dimensional embedding methods.[1] Isomap is used for computing a quasi-isometric, low-dimensional embedding of a set of high-dimensional data points. The algorithm provides a simple method for estimating the intrinsic geometry of a data manifold based on a rough estimate of each data point’s neighbors on the manifold. Isomap is highly efficient and generally applicable to a broad range of data sources and dimensionalities. \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "Isomap is one representative of isometric mapping methods, and extends metric multidimensional scaling (MDS) by incorporating the geodesic distances imposed by a weighted graph. To be specific, the classical scaling of metric MDS performs low-dimensional embedding based on the pairwise distance between data points, which is generally measured using straight-line Euclidean distance. Isomap is distinguished by its use of the geodesic distance induced by a neighborhood graph embedded in the classical scaling. This is done to incorporate manifold structure in the resulting embedding. Isomap defines the geodesic distance to be the sum of edge weights along the shortest path between two nodes (computed using Dijkstra's algorithm, for example). The top n eigenvectors of the geodesic distance matrix, represent the coordinates in the new n-dimensional Euclidean space.\n",
    "\n",
    "A very high-level description of Isomap algorithm is given below.\n",
    "- Determine the neighbors of each point. \n",
    "All points in some fixed radius.\n",
    "K nearest neighbors.\n",
    "- Construct a neighborhood graph. \n",
    "Each point is connected to other if it is a K nearest neighbor.\n",
    "Edge length equal to Euclidean distance.\n",
    "- Compute shortest path between two nodes. \n",
    "Dijkstra's algorithm\n",
    "Floyd–Warshall algorithm\n",
    "- Compute lower-dimensional embedding. \n",
    "Multidimensional scaling\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------- R\n",
    "# http://cc.oulu.fi/~jarioksa/softhelp/vegan/html/isomap.html\n",
    "## The following examples also overlay minimum spanning tree to\n",
    "## the graphics in red.\n",
    "op <- par(mar=c(4,4,1,1)+0.2, mfrow=c(2,2))\n",
    "data(BCI)\n",
    "dis <- vegdist(BCI)\n",
    "tr <- spantree(dis)\n",
    "pl <- ordiplot(cmdscale(dis), main=\"cmdscale\")\n",
    "lines(tr, pl, col=\"red\")\n",
    "ord <- isomap(dis, k=3)\n",
    "ord\n",
    "pl <- plot(ord, main=\"isomap k=3\")\n",
    "lines(tr, pl, col=\"red\")\n",
    "pl <- plot(isomap(dis, k=5), main=\"isomap k=5\")\n",
    "lines(tr, pl, col=\"red\")\n",
    "pl <- plot(isomap(dis, epsilon=0.45), main=\"isomap epsilon=0.45\")\n",
    "lines(tr, pl, col=\"red\")\n",
    "par(op)\n",
    "## The following command requires user interaction\n",
    "## Not run: \n",
    "rgl.isomap(ord, size=4, color=\"hotpink\")\n",
    "## End(Not run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html\n",
    "sklearn.manifold.Isomap(n_neighbors=5, n_components=2, eigen_solver='auto', tol=0, max_iter=None, path_method='auto', neighbors_algorithm='auto', n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------- Locally-linear embedding [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------- Non-linear PCA [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ Maximum Variance Unfolding [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Laplacian Eigenmaps [Non-linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
