{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration,[1] via obtaining a set of principal variables. It can be divided into feature selection and feature extraction. Dimension Reduction refers to the process of converting a set of data having vast dimensions into data with lesser dimensions ensuring that it conveys similar information concisely. These techniques are typically used while solving machine learning problems to obtain better features for a classification or regression task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Principal Component Analysis (PCA)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: First pca has the most information; Decorate features, Reduce dimwnsions; \n",
    "\n",
    "Cons: If features evenly contribute relationship, pca performs poorly; (Un-supervised) No Y involved so no guarantee that X explain Y well; \n",
    "\n",
    "#### Partial Least Sqaure (PLS)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: (supervised) Y involved in the process so good explaination with both X and Y; Decorate features, Reduce dimwnsions; \n",
    "\n",
    "Cons: \n",
    "\n",
    "#### Multiple Corespoundence Analysis (MCA)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "#### Sammon Mapping\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Nonlinear metric multidimensional scaling; \n",
    "\n",
    "Cons: Nonlinear transformation makes it difficult to use for classification application; \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "PCA is mostly used as a tool in exploratory data analysis and for making predictive models. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute.[4] The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).[5]\n",
    "\n",
    "PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection or \"shadow\" of this object when viewed from its (in some sense; see below) most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.\n",
    "\n",
    "http://setosa.io/ev/eigenvectors-and-eigenvalues/\n",
    "\n",
    "- [EigenValue]: In matrix transformation, the scale (how much) the matrix scale or decrease (change) is the Eigen value.\n",
    "\n",
    "\n",
    "- [EigenVector]: In matrix transformation, the direction (striaght line) that the matrix scale or decrease (change) is the Eigen vector.\n",
    "\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "#### Initial Parameters: \n",
    "Number of CPA \n",
    "#### Cost Function: \n",
    "Linear transformation to change the coordinate system to the 1st and second largest variance (decorlate)\n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "Identify the most variated direction in data and plot a striaght line in data and then do it for the second most variated direction (ortholognal to the first), repeat till N = number of features (Dimensions) in original data. Use those new lines as new coordinates in the N dimensional space (centered 0 in the cross of all lines). The coordinates on the first dimension of this new coordinate system is the first PCA, the second is the second PCA, repeat.\n",
    "\n",
    "- First componets: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = factor11(X1(1) - X1(mean)) + factor21(X2(1) - X2(mean)) + ... + factorP1(Xp(1) - Xp(mean))\n",
    "\n",
    "...\n",
    "\n",
    "Zn = factor1n(..              ) + factor2n(...             ) + ... + factorpn(...            ))  \n",
    "\n",
    "- Second componets: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = ...\n",
    "\n",
    "...\n",
    "\n",
    "Zn = ...\n",
    "\n",
    "*condition --- cpa2:{z1, .., Zn} is prependicular to cpa1:{z1, ... , zn} \n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "# Load data\n",
    "data(iris)\n",
    "head(iris, 3)\n",
    "\n",
    "# log transform \n",
    "log.ir <- log(iris[, 1:4])\n",
    "ir.species <- iris[, 5]\n",
    " \n",
    "# apply PCA - scale. = TRUE is highly \n",
    "# advisable, but default is FALSE. \n",
    "ir.pca <- prcomp(log.ir,\n",
    "                 center = TRUE,\n",
    "                 scale. = TRUE) \n",
    "\n",
    "# print method\n",
    "print(ir.pca)\n",
    "\n",
    "# plot method\n",
    "plot(ir.pca, type = \"l\")\n",
    "\n",
    "# summary method\n",
    "summary(ir.pca)\n",
    "\n",
    "# Detail ploting\n",
    "library(devtools)\n",
    "install_github(\"ggbiplot\", \"vqv\")\n",
    " \n",
    "library(ggbiplot)\n",
    "g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, \n",
    "              groups = ir.species, ellipse = TRUE, \n",
    "              circle = TRUE)\n",
    "g <- g + scale_color_discrete(name = '')\n",
    "g <- g + theme(legend.direction = 'horizontal', \n",
    "               legend.position = 'top')\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Partial Least Sqaure (PLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.\n",
    "\n",
    "PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized).\n",
    "#### Input Data: \n",
    "X (Numeric)\n",
    "#### Initial Parameters: \n",
    "Number of PLS\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "- First PLS: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = factor11(X1(1) - X1(mean)) + factor21(X2(1) - X2(mean)) + ... + factorP1(Xp(1) - Xp(mean))\n",
    "\n",
    "...\n",
    "\n",
    "Zn = factor1n(..              ) + factor2n(...             ) + ... + factorpn(...            )) \n",
    "\n",
    "Simple regression - {Z1, ..., Zn}Y = beta1 * X1(1,...,n) => n * residuals(X1.res)\n",
    "\n",
    "...\n",
    "\n",
    "Simple regression - {z1, ..., Zn}Y = detan * Xp(1,...,n) => n * residuals(Xn.res)\n",
    "\n",
    "- Second PLS: {factor1^2 + factor2^2 + ... + factorp^2 = 1}\n",
    "\n",
    "Z1 = factor11(X1.res(1) - X1.res(mean)) + factor21(X2.res(1) - X2.res(mean)) + ... + factorP1(Xp.res(1) - Xp.res(mean))\n",
    "\n",
    "...\n",
    "\n",
    "Zn = factor1n(..              ) + factor2n(...             ) + ... + factorpn(...            ))\n",
    "\n",
    "Simple regression - {Z1, ..., Zn}Y = beta1 * X1.res(1,...,n) => n * residuals(X1.res)\n",
    "\n",
    "...\n",
    "\n",
    "Simple regression - {z1, ..., Zn}Y = detan * Xp.res(1,...,n) => n * residuals(Xn.res)\n",
    "\n",
    "Repeat to M times ...\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "# https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf\n",
    "set.seed(1)\n",
    "table.pls <- plsr(Y ~ ., data = data, subset = train, sclae = TRUE, validation = \"cv\")\n",
    "summary(table.pls)\n",
    "validationplot(table.pls, val.type = \"MSEP\") # lowest MSE\n",
    "\n",
    "# Prediction\n",
    "prediction <- predict(table.pls, test, ncomp = 2) # number of pls = 2\n",
    "mean((prediction - test$Y)^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n",
    "Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n",
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(X, Y)\n",
    "\n",
    "Y_pred = pls2.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Multiple Corespoundence Analysis (MCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics, multiple correspondence analysis (MCA) is a data analysis technique for nominal categorical data, used to detect and represent underlying structures in a data set. It does this by representing data as points in a low-dimensional Euclidean space. The procedure thus appears to be the counterpart of principal component analysis for categorical data.[1][2] MCA can be viewed as an extension of simple correspondence analysis (CA) in that it is applicable to a large set of categorical variables.\n",
    "\n",
    "Multiple correspondence analysis (MCA) is an extension of corre-spondence analysis (CA) which allows one to analyze the pattern of relationships of several categorical dependent variables. As such, it can also be seen as a generalization of principal component anal- ysis when the variables to be analyzed are categorical instead of quantitative. Because MCA has been (re)discovered many times, equivalent methods are known under several different names such as optimal scaling, optimal or appropriate scoring, dual scaling, homogeneity analysis, scalogram analysis, and quantification me- thod.\n",
    "\n",
    "Technically MCA is obtained by using a standard correspon-dence analysis on an indicator matrix (i.e., a matrix whose entries are 0 or 1). The percentages of explained variance need to be cor-rected, and the correspondence analysis interpretation of inter-point distances needs to be adapted.\n",
    "\n",
    "#### Input Data: \n",
    "X(Categorical)\n",
    "#### Initial Parameters: \n",
    "Number of MCA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- R\n",
    "# http://www.sthda.com/english/wiki/multiple-correspondence-analysis-essentials-interpretation-and-application-to-investigate-the-associations-between-categories-of-multiple-qualitative-variables-r-software-and-data-mining#multiple-correspondence-analysis-mca\n",
    "\n",
    "install.packages(\"FactoMineR\")\n",
    "# install.packages(\"devtools\")\n",
    "devtools::install_github(\"kassambara/factoextra\")\n",
    "library(\"factoextra\")        \n",
    "\n",
    "# X : a data frame with n rows (individuals) and p columns (categorical variables)\n",
    "# ncp : number of dimensions kept in the final results.\n",
    "# graph : a logical value. If TRUE a graph is displayed.\n",
    "MCA(X, ncp = 5, graph = TRUE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------- Python\n",
    "# https://github.com/esafak/mca\n",
    "# pip install --user mca\n",
    "\n",
    "import mca, pandas, numpy\n",
    "counts = pandas.read_table('data/burgundies.csv', sep=',', skiprows=1, index_col=0, header=0)\n",
    "print(counts.shape)\n",
    "\"(6, 23)\"\n",
    "\n",
    "mca_counts = mca.MCA(counts.drop('oak_type', axis=1))\n",
    "print(mca_counts.fs_r(1)) # 1 = 100%, meaning preserve all variance.\n",
    "\n",
    "\"\"\"\n",
    "        array([[ 0.87127085,  0.11448396, -0.09250792],\n",
    "               [-0.7209849 , -0.22896791, -0.083259  ],\n",
    "               [-0.93238083,  0.11448396, -0.02206285],\n",
    "               [-0.87127085,  0.11448396,  0.09250792],\n",
    "               [ 0.93238083,  0.11448396,  0.02206285],\n",
    "               [ 0.7209849 , -0.22896791,  0.083259  ]])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Sammon Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Sammon mapping or Sammon projection is an algorithm that maps a high-dimensional space to a space of lower dimensionality (see multidimensional scaling) by trying to preserve the structure of inter-point distances in high-dimensional space in the lower-dimension projection. It is particularly suited for use in exploratory data analysis. The method was proposed by John W. Sammon in 1969.[1] It is considered a non-linear approach as the mapping cannot be represented as a linear combination of the original variables as possible in techniques such as principal component analysis, which also makes it more difficult to use for classification applications.\n",
    "The minimization can be performed either by gradient descent, as proposed initially, or by other means, usually involving iterative methods. The number of iterations need to be experimentally determined and convergent solutions are not always guaranteed. Many implementations prefer to use the first Principal Components as a starting configuration.[3]\n",
    "The Sammon mapping has been one of the most successful nonlinear metric multidimensional scaling methods since its advent in 1969, but effort has been focused on algorithm improvement rather than on the form of the stress function. The performance of the Sammon mapping has been improved by extending its stress function using left Bregman divergence [4] and right Bregman divergence.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric)\n",
    "\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "\n",
    "#### Cost Function: \n",
    "https://en.wikipedia.org/wiki/Sammon_mapping\n",
    "\n",
    "#### Process Flow: \n",
    "https://en.wikipedia.org/wiki/Sammon_mapping\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------- R\n",
    "\n",
    "# http://sites.stat.psu.edu/~dhunter/R/html/MASS/html/sammon.html\n",
    "\n",
    "data(swiss)\n",
    "swiss.x <- as.matrix(swiss[, -1])\n",
    "swiss.sam <- sammon(dist(swiss.x))\n",
    "plot(swiss.sam$points, type = \"n\")\n",
    "text(swiss.sam$points, labels = as.character(1:nrow(swiss.x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------- Python\n",
    "\n",
    "# https://github.com/adailsonfilho/sammon/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
