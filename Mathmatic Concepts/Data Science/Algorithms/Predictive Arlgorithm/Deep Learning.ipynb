{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "Deep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high level abstractions in data. It solves this central problem in representation learning by introducing representations that are expressed in terms of others, simpler representations. Deep learning enables the computer to build complex concepts out of simpler concepts.\n",
    "\n",
    "#### Computational Graphs\n",
    "\n",
    "It maps inputs to the outputs where each node performs an operation. Depth is the length of the longest path from input to output but depends on the definition of what consititutes a possible computaional step. Can be a calculation level or on the model level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "                                                         ###################\n",
    "                                                         #     Output      #\n",
    "                                                         #                 #\n",
    "                                                         ###################\n",
    "                                                                  |\n",
    "                                                                  |\n",
    "#                 ###############      ###############   ###################\n",
    "                  #   Output    #      #   Output    #   # Mapping from    #\n",
    "                  #             #      #             #   # Features        #\n",
    "                  ###############      ###############   ###################\n",
    "                         |                    |                   |\n",
    "                         |                    |                   |\n",
    "                         |                    |                   |\n",
    "###############   ###############      ###############   #####################\n",
    "#   Output    #   # Mapping from#      # Mapping from#   # Additional layers # ------ Special for\n",
    "#             #   # Features    #      # Features    #   # of more abstract f#         deep learning         \n",
    "###############   ###############      ###############   #####################\n",
    "       |                 |                    |                   |\n",
    "       |                 |                    |                   |\n",
    "###############   ###############      ###############   ###################\n",
    "# Hand-Design #   # Hand-Design #      # Features    #   # Simple Features #\n",
    "# Program     #   # Features    #      #             #   #                 #\n",
    "###############   ###############      ###############   ###################\n",
    "       |                 |                    |                   |\n",
    "       |                 |                    |                   |\n",
    "###############   ###############      ###############   ###################\n",
    "#  Input      #   #  Input      #      #    Input    #   #      Input      #\n",
    "###############   ###############      ###############   ###################\n",
    "                                                           \"Deep Learning\"\n",
    "\"Rule-based\"     \"Classic Machine\"     -------------------------------------\n",
    "  \"System\"          \"Learning\"              \"Representation Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## >>>>> Mathmetic Background (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] ----------------------------------------- Linear Algebra\n",
    "\n",
    "#### Scalars\n",
    "\n",
    "A scalar is just a number: 4, 15, 100\n",
    "\n",
    "#### Vectors\n",
    "\n",
    "A vector is an 1D array of numbers: x[i] = [12,24,61]\n",
    "\n",
    "#### Matrices\n",
    "\n",
    "A matrics is a 2D array of numbers: X[i,j] = [[12, 24],[31,17]]\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "A matrics is a >2D array of numbers: X[i,j,k] = ....3D....\n",
    "\n",
    "#### Transpose \n",
    "\n",
    "The transpose of a matrix is the mirror image of the matrix across a diagonal line, called main the main diagonal.\n",
    "(A^T)i,j = Aj,i\n",
    "\n",
    "\n",
    "#### Multiplying Matrics and Vectors\n",
    "\n",
    "- Element-wise product\n",
    "\n",
    "According elements in each matrixs product each other - Same shape; A*B = B*A\n",
    "\n",
    "- Matrix product\n",
    "\n",
    "M X N product N X D = M X D; A(B+C) = AB + AC; A(BC) = (AB)C; A*B <> B*A; (AB)^T = B^T * A^T transpose of a matrix product \n",
    "\n",
    "A ~ m x n, b ~ m, solve x ~ n\n",
    "\n",
    "#### Identity Matrix / Inverse Matrix\n",
    "\n",
    "- Matrix Inversion\n",
    "\n",
    "A^-1 (Inversed Matrix of A)\n",
    "A (Matrix of A)\n",
    "In (Identity Matrix - preserve n-dimensional vectors)\n",
    "A^-1 * A = In\n",
    "\n",
    "- Normal Equation using Matrix Inversion\n",
    "\n",
    "A * x = b\n",
    "A^-1 * A * x = A^-1 * b ---> Both * A^-1\n",
    "In * x = A^-1 * b ---> Transform use Inversion\n",
    "x = A^-1 * b ---> Solve for x\n",
    "(m need to be bigger than n)\n",
    "\n",
    "- Identity Matrix\n",
    "\n",
    "[1 0 0]\n",
    "[0 1 0]\n",
    "[0 0 1]\n",
    "\n",
    "\n",
    "#### Linear Dependence and Span\n",
    "\n",
    "**(In order to make sure A^-1 exist)\n",
    "\n",
    "Equation - ( A * x = b ), exactly one solution for every value of 'b' | no solution for every 'b' | infinite solutions for every 'b' \n",
    "\n",
    "Normally, some set of vectors {v(1),v(2),v(3),...,v(n)} is given by multipying each vector v(i) by corresponding scalar coefficient and adding the result ---> The 'span' of a set of vectors is the set of all points obitained by linear combination of original vectors.\n",
    "\n",
    "Determine whether A * x = b has a solution thus amounts to testing whether 'b' is in the 'span' of the columns of A. The particular span is known as the 'column space' or 'range' of A. 'n' >= 'm'.\n",
    "\n",
    "- Linearly dependent\n",
    "\n",
    "Consider 2X2 matrix has 2 identical columns which has the same column space as 2X1 matrix has the same column. The 2 columns in 2X2 matrix are linearly dependent\n",
    "\n",
    "- Linearly independent\n",
    "\n",
    "A set of vectors is linearly independent if no vector in the set is a linear combination of the other vectors.\n",
    "\n",
    "\n",
    "\n",
    "#### Norms\n",
    "\n",
    "We usually measure the size of a vector using a function called a 'norm'.\n",
    "\n",
    "L-p Norm: SUM(|X(i)|^p)^1/p\n",
    "\n",
    "L-2 Norm: p=2, Euclidean Norm\n",
    "\n",
    "However, L-2 Norm increase very slowly near origin 0 (But in ML very important to identify diff between 0 and small value)\n",
    "\n",
    "Then, use \n",
    "\n",
    "L-1 Norm: p=1\n",
    "\n",
    "L-inifinite Norm: (max norn) max|X(i)|\n",
    "\n",
    "Frobenius Norm: (Measure the size of the matrix) (alternative to L-2 Norm for vector)\n",
    "\n",
    "\"dot product of two matrixs can be rewrite in Norm\"\n",
    "\n",
    "-- X^T * Y = ||X||2 * ||Y||2 cos(theta)\n",
    "\n",
    "\n",
    "\n",
    "#### Special Matrics and Vectors\n",
    "\n",
    "- Diagonal Matrix\n",
    "\n",
    "Consist mostly of zeros and have non-zero entries only along the main disgonal.\n",
    "\n",
    "- Symmetric Matrix\n",
    "\n",
    "A matrix that is equal to its own transpose\n",
    "\n",
    "- Unit Vector\n",
    "\n",
    "A  unit vector is a vector with unit Norm: ||X||2 = 1\n",
    "\n",
    "- Orthogonal Matrix\n",
    "\n",
    "vector X and vector Y is orthogonal, if X^T * Y = 0\n",
    "\n",
    "A Orthogonal Matrix is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal\n",
    "\n",
    "\n",
    "#### Eigen decomposition\n",
    "\n",
    "Example1: integers can be decomposed into prime factors -> 12 = 2 X 2 X 3\n",
    "\n",
    "We can also decompose matrices in ways that shows us information about their functional properties.\n",
    "\n",
    "Matrix --> decomposation --> a set of [eigenvectors] and [eigenvalues]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Singular Value Decomposition\n",
    "\n",
    "#### The Moore-Penrose Pseudoinverse\n",
    "\n",
    "\n",
    "#### The Trace Operator\n",
    "\n",
    "\n",
    "#### The Determinant\n",
    "\n",
    "\n",
    "\n",
    "## [2] ----------------------------------------- Probability and Information Theory\n",
    "\n",
    "#### Why Probability?\n",
    "\n",
    "\n",
    "#### Random Variables\n",
    "\n",
    "\n",
    "#### Probability Distribution\n",
    "\n",
    "- Mass Function\n",
    "\n",
    "\n",
    "- Density Function\n",
    "\n",
    "#### Marginal Probability\n",
    "\n",
    "\n",
    "#### Conditional Probability\n",
    "\n",
    "\n",
    "#### The Chain Rule of Conditional Probabilities\n",
    "\n",
    "\n",
    "#### Independence and Conditional Independence\n",
    "\n",
    "\n",
    "#### Expectation, Variance and Covariance\n",
    "\n",
    "\n",
    "#### Common Probability Distributions\n",
    "\n",
    "- Bernoulli Distribution\n",
    "\n",
    "- Multinoulli Distribution\n",
    "\n",
    "- Gaussian Distribution\n",
    "\n",
    "- Exponential Distribution\n",
    "\n",
    "- Laplace Distribution\n",
    "\n",
    "- Dirac Distribution\n",
    "\n",
    "- Empirical Distribution\n",
    "\n",
    "- Mixtures of Distribution\n",
    "\n",
    "\n",
    "#### Useful Properties of Common Functions\n",
    "\n",
    "\n",
    "\n",
    "#### Bayes' Rules\n",
    "\n",
    "\n",
    "\n",
    "#### Technical Details of Continuous Variables\n",
    "\n",
    "\n",
    "#### Information Theory\n",
    "\n",
    "\n",
    "#### Kullback-Leibler (KL) divergence\n",
    "\n",
    "\n",
    "#### Structured Probablistic Models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [3] ----------------------------------------- Numerical Computation\n",
    "\n",
    "#### Overflow and Underflow\n",
    "\n",
    "\n",
    "#### Poor Conditioning\n",
    "\n",
    "\n",
    "#### Gradient-Based Optimization\n",
    "\n",
    "- Basic Concepts\n",
    "\n",
    "\n",
    "- Beyond the Gradient: Jacobian and Hessian Matrices\n",
    "\n",
    "\n",
    "#### Constrained Optimization\n",
    "\n",
    "\n",
    "- Example: Linear Least Squares\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [4] ----------------------------------------- Machine Learning Basics\n",
    "\n",
    "\n",
    "### Learning Algorithms\n",
    "\n",
    "#### (1) The Task, T\n",
    "\n",
    "- Classification\n",
    "\n",
    "\n",
    "- Classification with missing inputs\n",
    "\n",
    "\n",
    "- Regression\n",
    "\n",
    "\n",
    "- Transcription\n",
    "\n",
    "\n",
    "- Machine translation\n",
    "\n",
    "\n",
    "- Structured output\n",
    "\n",
    "\n",
    "- Anomaly detection\n",
    "\n",
    "\n",
    "- Synthesis and sampling\n",
    "\n",
    "\n",
    "- Imputation of missing values\n",
    "\n",
    "\n",
    "- Denoising\n",
    "\n",
    "\n",
    "- Density estimation or probability mass function estimation\n",
    "\n",
    "\n",
    "#### (2) The performance Measure, P\n",
    "\n",
    "\n",
    "#### (3) The Experience, E\n",
    "\n",
    "- Unsupervised Learning Algorithms\n",
    "\n",
    "\n",
    "\n",
    "- Supervised Learning Algorithms\n",
    "\n",
    "\n",
    "### Capacity, Overfitting and Underfitting\n",
    "\n",
    "\n",
    "- Capacity\n",
    "\n",
    "\n",
    "- Overfitting\n",
    "\n",
    "\n",
    "- Underfitting\n",
    "\n",
    "\n",
    "- The No Free Lunch Theorem\n",
    "\n",
    "\n",
    "- Regularization\n",
    "\n",
    "\n",
    "\n",
    "### Hyperparameters and Validation Sets\n",
    "\n",
    "- Validation Sets\n",
    "\n",
    "\n",
    "- Cross Validation Sets\n",
    "\n",
    "\n",
    "### Estimators, Bias and Variance\n",
    "\n",
    "- Point Estimation\n",
    "\n",
    "\n",
    "- Bias\n",
    "\n",
    "\n",
    "- Variance and Standard Error\n",
    "\n",
    "\n",
    "- Trading off Bias and Variance to Minimize Mean Squared Error\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "- Basic\n",
    "\n",
    "\n",
    "- Conditional Log-Likelihood and Mean Squared Error\n",
    "\n",
    "\n",
    "- Properties of Maximum Likelihood\n",
    "\n",
    "\n",
    "### Bayesian Statistics\n",
    "\n",
    "- Basic\n",
    "\n",
    "\n",
    "- Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "\n",
    "### Supervised Learning Algorithms\n",
    "\n",
    "- Basic Concept\n",
    "\n",
    "\n",
    "### Unsupervised Learning Algorithms\n",
    "\n",
    "- Basic Concept\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\n",
    "\n",
    "### Building a Machine Learning Algorithm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "## Challenges Motivating Deep Learning **\n",
    "\n",
    "#### The Curse of Dimensionality\n",
    "\n",
    "\n",
    "#### Local Constancy and Smoothness Regularization\n",
    "\n",
    "\n",
    "#### Manifold Learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Deep Networks: Modern Practices (3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Deep Learning Research (4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
