{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Classification Tree (CART)\n",
    "Pros: Simple to understand and to interpret. Trees can be visualised; Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.; The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.; Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.; Able to handle multi-output problems.; Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.; Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.; Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.; Robust!; Non-linear;\n",
    "\n",
    "Cons: Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.; Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.; The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.; There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.; Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.; Easy to overfit; Globaly search (Expensive)\n",
    "\n",
    "\n",
    "#### Regression Tree (CART)\n",
    "Pros: [Same above]\n",
    "\n",
    "Cons: [Same above]\n",
    "\n",
    "#### C4.5 - C5.0 Tree\n",
    "- C4.5 Tree\n",
    "\n",
    "Pros: Handling both continuous and discrete attributes - In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it; Handling training data with missing attribute values - C4.5 allows attribute values to be marked as ? for missing. Missing attribute values are simply not used in gain and entropy calculations; Handling attributes with differing costs; Pruning trees after creation - C4.5 goes back through the tree once it's been created and attempts to remove branches that do not help by replacing them with leaf nodes.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- C5.0 Tree\n",
    "\n",
    "Pros: [Improvement over c4.5] - Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude); Memory usage - C5.0 is more memory efficient than C4.5; Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees.; Support for boosting - Boosting improves the trees and gives them more accuracy.; Weighting - C5.0 allows you to weight different cases and misclassification types.; Winnowing - a C5.0 option automatically winnows the attributes to remove those that may be unhelpful.\n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Random Forest\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Bagging Trees\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Boosting Trees\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Classification Tree (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "- Predict categorical\n",
    "\n",
    "Classification trees are used to predict membership of cases or objects in the classes of a categorical dependent variable from their measurements on one or more predictor variables. Classification tree analysis is one of the main techniques used in Data Mining.\n",
    "The goal of classification trees is to predict or explain responses on a categorical dependent variable, and as such, the available techniques have much in common with the techniques used in the more traditional methods of Discriminant Analysis, Cluster Analysis, Nonparametric Statistics, and Nonlinear Estimation. The flexibility of classification trees make them a very attractive analysis option, but this is not to say that their use is recommended to the exclusion of more traditional methods. Indeed, when the typically more stringent theoretical and distributional assumptions of more traditional methods are met, the traditional methods may be preferable. But as an exploratory technique, or as a technique of last resort when traditional methods fail, classification trees are, in the opinion of many researchers, unsurpassed.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "# https://www.r-bloggers.com/classification-trees/\n",
    "library(tree)\n",
    "\n",
    "xtabs( ~ class, data = ecoli.df)\n",
    "\"\"\"\n",
    "class\n",
    " cp  im imL imS imU  om omL  pp \n",
    "143  77   2   2  35  20   5  52\n",
    "\"\"\"\n",
    "\n",
    "ecoli.tree1 = tree(class ~ mcv + gvh + lip + chg + aac + alm1 + alm2,\n",
    "  data = ecoli.df)\n",
    "summary(ecoli.tree1)\n",
    "\n",
    "\"\"\"\n",
    "Classification tree:\n",
    "tree(formula = class ~ mcv + gvh + lip + chg + aac + alm1 + alm2, \n",
    "    data = ecoli.df)\n",
    "Variables actually used in tree construction:\n",
    "[1] \"alm1\" \"mcv\"  \"gvh\"  \"aac\"  \"alm2\"\n",
    "Number of terminal nodes:  10 \n",
    "Residual mean deviance:  0.7547 = 246 / 326 \n",
    "Misclassification error rate: 0.122 = 41 / 336\n",
    "\"\"\"\n",
    "# Ploting the tree\n",
    "plot(ecoli.tree1)\n",
    "text(ecoli.tree1, all = T)\n",
    "\n",
    "# To prune the tree we use cross-validation to identify the point to prune.\n",
    "cv.tree(ecoli.tree1)\n",
    "\"\"\"\n",
    "$size\n",
    " [1] 10  9  8  7  6  5  4  3  2  1\n",
    " \n",
    "$dev\n",
    " [1]  463.6820  457.4463  447.9824  441.8617  455.8318  478.9234  533.5856  586.2820  713.2992 1040.3878\n",
    " \n",
    "$k\n",
    " [1]      -Inf  12.16500  15.60004  19.21572  34.29868  41.10627  50.57044  64.05494 180.78800 355.67747\n",
    " \n",
    "$method\n",
    "[1] \"deviance\"\n",
    " \n",
    "attr(,\"class\")\n",
    "[1] \"prune\"         \"tree.sequence\"\n",
    "\"\"\"\n",
    "# This suggests a tree size of 6 and we can re-fit the tree:\n",
    "ecoli.tree2 = prune.misclass(ecoli.tree1, best = 6)\n",
    "summary(ecoli.tree2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "# After being fitted, the model can then be used to predict the class of samples:\n",
    "clf.predict([[2., 2.]])\n",
    "\n",
    "# Alternatively, the probability of each class can be predicted, which is the fraction of training samples \n",
    "# of the same class in a leaf:\n",
    "clf.predict_proba([[2., 2.]])\n",
    "\n",
    "# Case 2 - use irs dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "\n",
    "# fitting the model ------------------ Sample code\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) # impurity meansure // depth of tree\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Once trained, we can export the tree in Graphviz format using the export_graphviz exporter. \n",
    "# Below is an example export of a tree trained on the entire iris dataset:\n",
    "with open(\"iris.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)\n",
    "\n",
    "# Then we can use Graphviz’s dot tool to create a PDF file (or any other supported file type): \n",
    "# dot -Tpdf iris.dot -o iris.pdf.\n",
    "import os\n",
    "os.unlink('iris.dot')\n",
    "    \n",
    "# Plot\n",
    "from IPython.display import Image  \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Regression Tree (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "- Predict Continuous\n",
    "\n",
    "A regression tree is built through a process known as binary recursive partitioning, which is an iterative process that splits the data into partitions or branches, and then continues splitting each partition into smaller groups as the method moves up each branch.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "# http://www.di.fc.ul.pt/~jpn/r/tree/tree.html\n",
    "library(tree)\n",
    "\n",
    "real.estate <- read.table(\"cadata.dat\", header=TRUE)\n",
    "tree.model <- tree(log(MedianHouseValue) ~ Longitude + Latitude, data=real.estate)\n",
    "plot(tree.model)\n",
    "text(tree.model, cex=.75)\n",
    "\n",
    "# We can compare the predictions with the dataset \n",
    "# (darker is more expensive) which seem to capture the global price trend:\n",
    "price.deciles <- quantile(real.estate$MedianHouseValue, 0:10/10)\n",
    "cut.prices <- cut(real.estate$MedianHouseValue, price.deciles, include.lowest=TRUE)\n",
    "plot(real.estate$Longitude, real.estate$Latitude, col=grey(10:2/11)[cut.prices], pch=20, xlab=\"Longitude\",ylab=\"Latitude\")\n",
    "partition.tree(tree.model, ordvars=c(\"Longitude\",\"Latitude\"), add=TRUE)\n",
    "# Summary\n",
    "summary(tree.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "# http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html\n",
    "# http://scikit-learn.org/stable/modules/tree.html\n",
    "\" Mostly the same as classification tree but use 'DecisionTreeRegressor()' instead \"\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])\n",
    "array([ 0.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- C4.5 - C5.0 Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "- Predict Categorical (Classifier)\n",
    "\n",
    "https://en.wikipedia.org/wiki/C4.5_algorithm\n",
    "\n",
    "- C4.5 Tree\n",
    "\n",
    "C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan.[1] C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy. The training data is a set S={s_{1},s_{2},...} of already classified samples. Each sample s_{i} consists of a p-dimensional vector (x_{{1,i}},x_{{2,i}},...,x_{{p,i}}), where the x_{j} represent attribute values or features of the sample, as well as the class in which s_{i} falls. At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurs on the smaller sublists.\n",
    "\n",
    "- C5.0 Tree\n",
    "\n",
    "Quinlan went on to create C5.0 and See5 (C5.0 for Unix/Linux, See5 for Windows) which he markets commercially. C5.0 offers a number of improvements on C4.5. Some of these are:[5][6]\n",
    "\n",
    "*Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude)\n",
    "\n",
    "*Memory usage - C5.0 is more memory efficient than C4.5\n",
    "\n",
    "*Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees.\n",
    "\n",
    "*Support for boosting - Boosting improves the trees and gives them more accuracy.\n",
    "\n",
    "*Weighting - C5.0 allows you to weight different cases and misclassification types.\n",
    "\n",
    "*Winnowing - a C5.0 option automatically winnows the attributes to remove those that may be unhelpful.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "- C4.5 Tree\n",
    "\n",
    "\n",
    "- C5.0 Tree\n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "######## C4.5 Tree #######\n",
    "\n",
    "\" C5.0 is better in all - \"\n",
    "\n",
    "######## C5.0 Tree #######\n",
    "# http://www.patricklamle.com/Tutorials/Decision%20tree%20R/Decision%20trees%20in%20R%20using%20C50.html\n",
    "\n",
    "credit <- read.csv(\"credit.csv\")\n",
    "str(credit)\n",
    "\"\"\"\n",
    "'data.frame':\t1000 obs. of  17 variables:\n",
    " $ checking_balance    : Factor w/ 4 levels \"< 0 DM\",\"> 200 DM\",..: 1 3 4 1 1 4 4 3 4 3 ...\n",
    " $ months_loan_duration: int  6 48 12 42 24 36 24 36 12 30 ...\n",
    " $ credit_history      : Factor w/ 5 levels \"critical\",\"good\",..: 1 2 1 2 4 2 2 2 2 1 ...\n",
    " $ purpose             : Factor w/ 6 levels \"business\",\"car\",..: 5 5 4 5 2 4 5 2 5 2 ...\n",
    " $ amount              : int  1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ...\n",
    " $ savings_balance     : Factor w/ 5 levels \"< 100 DM\",\"> 1000 DM\",..: 5 1 1 1 1 5 4 1 2 1 ...\n",
    " $ employment_duration : Factor w/ 5 levels \"< 1 year\",\"> 7 years\",..: 2 3 4 4 3 3 2 3 4 5 ...\n",
    " $ percent_of_income   : int  4 2 2 2 3 2 3 2 2 4 ...\n",
    " $ years_at_residence  : int  4 2 3 4 4 4 4 2 4 2 ...\n",
    " $ age                 : int  67 22 49 45 53 35 53 35 61 28 ...\n",
    " $ other_credit        : Factor w/ 3 levels \"bank\",\"none\",..: 2 2 2 2 2 2 2 2 2 2 ...\n",
    " $ housing             : Factor w/ 3 levels \"other\",\"own\",..: 2 2 2 1 1 1 2 3 2 2 ...\n",
    " $ existing_loans_count: int  2 1 1 1 2 1 1 1 1 2 ...\n",
    " $ job                 : Factor w/ 4 levels \"management\",\"skilled\",..: 2 2 4 2 2 4 2 1 4 1 ...\n",
    " $ dependents          : int  1 1 2 2 2 2 1 1 1 1 ...\n",
    " $ phone               : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 1 1 2 1 2 1 1 ...\n",
    " $ default             : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 1 2 1 1 1 1 2 ...\n",
    "\"\"\"\n",
    "\n",
    "# Train Model\n",
    "library(C50)\n",
    "credit_model <- C5.0(credit_train[-17], credit_train$default)\n",
    "\n",
    "summary(credit_model)\n",
    "\n",
    "# Prediction\n",
    "credit_pred <- predict(credit_model, credit_test)\n",
    "\n",
    "# Confusion table\n",
    "library(gmodels)\n",
    "CrossTable(credit_test$default, credit_pred,\n",
    "           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n",
    "           dnn = c('actual default', 'predicted default'))\n",
    "\n",
    "\" 5. Improving the model with (adaptative) boosting ¶\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "######## C4.5 Tree #######\n",
    "\n",
    "\" C5.0 is better in all - \"\n",
    "\n",
    "######## C5.0 Tree #######\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Random forests or random decision forests[1][2] are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. Decision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie et al., because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate. In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3]:587–588 This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n",
    "# -- Random Forests ---------------- Sample code 1\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy', # impurity meansure\n",
    "                                n_estimators=10, # learners\n",
    "                                random_state=1,\n",
    "                                n_jobs=2) # cores\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ---------------------------------- Sample code 2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "df['species'] = pd.Factor(iris.target, iris.target_names)\n",
    "df.head()\n",
    "\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "\n",
    "features = df.columns[:4]\n",
    "clf = RandomForestClassifier(n_jobs=2)\n",
    "y, _ = pd.factorize(train['species'])\n",
    "clf.fit(train[features], y)\n",
    "\n",
    "preds = iris.target_names[clf.predict(test[features])]\n",
    "pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Bagging Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Boosting Tress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
