{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm.\n",
    "\n",
    "The majority of recursive partitioning algorithms are special cases of a simple two-stage algorithm: First partition the observations by univariate splits in a recursive way and second fit a constant model in each cell of the resulting partition.\n",
    "\n",
    "- CART: Grow full tree and prune; Build use train and test set (prune); Each step allows binary splits; Prediction - is better suited for creating a model that has high prediction accuracy of new cases ; \n",
    "\n",
    "- C4.5: Grow full tree and prune; Build use single dataset; Each step allows multiple splits; Prediction - is better suited for creating a model that has high prediction accuracy of new cases; \n",
    "\n",
    "- CHAID: Use a statistical stopping rule the tree growth; Build use single dataset; Each step allows multiple splits; Inference - when the goal is to describe or understand the relationship between a response variable and a set of explanatory variables; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Classification Tree (CART)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Simple to understand and to interpret. Trees can be visualised; Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.; The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.; Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.; Able to handle multi-output problems.; Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.; Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.; Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.; Non-linear; easy to handle categorical feature without transform; If non-linear relationship is good;  Handle missing value; Grow full tree and prune; Build use train and test set (prune); Each step allows binary splits; Prediction - is better suited for creating a model that has high prediction accuracy of new cases ; \n",
    "\n",
    "\n",
    "Cons: Poor prediction performance – high variance ; Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.; Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.; The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.; There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.; Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.; Easy to overfit; Globaly search (Expensive); Poor prediction performance – high variance ; \n",
    "\n",
    "\n",
    "#### Regression Tree (CART)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: [Same above]\n",
    "\n",
    "Cons: [Same above]\n",
    "\n",
    "#### C4.5 - C5.0 Tree\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "- C4.5 Tree\n",
    "\n",
    "Pros: Handling both continuous and discrete attributes - In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it; Handling training data with missing attribute values - C4.5 allows attribute values to be marked as ? for missing. Missing attribute values are simply not used in gain and entropy calculations; Handling attributes with differing costs; Pruning trees after creation - C4.5 goes back through the tree once it's been created and attempts to remove branches that do not help by replacing them with leaf nodes. Grow full tree and prune; Build use single dataset; Each step allows multiple splits; Prediction - is better suited for creating a model that has high prediction accuracy of new cases; \n",
    "\n",
    "Cons:\n",
    "\n",
    "- C5.0 Tree\n",
    "\n",
    "Pros: [Improvement over c4.5] - Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude); Memory usage - C5.0 is more memory efficient than C4.5; Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees.; Support for boosting - Boosting improves the trees and gives them more accuracy.; Weighting - C5.0 allows you to weight different cases and misclassification types.; Winnowing - a C5.0 option automatically winnows the attributes to remove those that may be unhelpful. Solve high variance(over fitting) with bottom-up technique (Pruning); Accept continuous and discrete features; Handle missing data; Grow full tree and prune; Build use single dataset; Each step allows multiple splits; Prediction - is better suited for creating a model that has high prediction accuracy of new cases; \n",
    "\n",
    "Cons: Over fitting happens when algorithm model picks up data with uncommon characteristics , especially when data is noisy.; \n",
    "\n",
    "\n",
    "#### Chi-squared Automatic Interaction Detection (CHAID)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Good for inference and analysis; \n",
    "\n",
    "Cons: Not best for prediction;\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Stump\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Low variance Used in ensemble model;\n",
    "\n",
    "Cons: Too poor to predict (Weak learner);\n",
    "\n",
    "\n",
    "#### Cubist Model Tree (Extension to M5)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Efficient on high-dimensionality; Tree is smaller than regression tree so more accurate; Can extrapolate the prediction; \n",
    "\n",
    "Cons: \n",
    "\n",
    "\n",
    "#### GUIDE Tree\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Use a statistical stopping rule the tree growth; Build use single dataset; Each step allows multiple splits; Inference - when the goal is to describe or understand the relationship between a response variable and a set of explanatory variables; it is not biased in split-variable selection, unlike CART which is biased towards selecting split-variables which allow more splits, and those which have more missing values.;\n",
    "\n",
    "Cons: \n",
    "\n",
    "\n",
    "#### MOB Tree\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "\n",
    "\n",
    "#### QUEST Tree\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: easily handle categorical predictor variables with many categories; it is not biased in split-variable selection, unlike CART which is biased towards selecting split-variables which allow more splits, and those which have more missing values;\n",
    "\n",
    "Cons: \n",
    "\n",
    "\n",
    "#### Conditional Inference Tree\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: avoids the following variable selection bias - tend to select variables that have many possible splits or many missing values; \n",
    "\n",
    "Cons: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Logistic Model Tree (LMT)\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Further reduce the “high variance” problem with simple decision tree; Good for high dimensional / high variance data; \n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Bagging Trees\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Reduce the “high variance” problem with simple decision tree (Not as good as random forest);\n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Boosting Trees\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: Reduce the “high variance” problem with simple decision tree by boosting; \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Classification Tree (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "- Predict categorical\n",
    "\n",
    "https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf\n",
    "\n",
    "The CART algorithm decides on a split based on the amount of homogeneity within class that is achieved by the split. And later on, the split is reconsidered based on considerations of over-fitting.\n",
    "\n",
    "In sum, the CART implementation is very similar to C4.5; the one notable difference is that CART constructs the tree based on a numerical splitting criterion recursively applied to the data, whereas C4.5 includes the intermediate step of constructing *rule set*s.\n",
    "\n",
    "Classification trees are used to predict membership of cases or objects in the classes of a categorical dependent variable from their measurements on one or more predictor variables. Classification tree analysis is one of the main techniques used in Data Mining.\n",
    "The goal of classification trees is to predict or explain responses on a categorical dependent variable, and as such, the available techniques have much in common with the techniques used in the more traditional methods of Discriminant Analysis, Cluster Analysis, Nonparametric Statistics, and Nonlinear Estimation. The flexibility of classification trees make them a very attractive analysis option, but this is not to say that their use is recommended to the exclusion of more traditional methods. Indeed, when the typically more stringent theoretical and distributional assumptions of more traditional methods are met, the traditional methods may be preferable. But as an exploratory technique, or as a technique of last resort when traditional methods fail, classification trees are, in the opinion of many researchers, unsurpassed.\n",
    "\n",
    "The CART algorithm decides on a split based on the amount of homogeneity within class that is achieved by the split. And later on, the split is reconsidered based on considerations of over-fitting.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "Minimal observations in each region ~ 5\n",
    "\n",
    "Pruning parameter (trade-off over fitting - complexity) ~ 0 - infinite\n",
    "#### Cost Function: \n",
    "- Classification error rate\n",
    "- Gini Index\n",
    "- Cross-Entropy\n",
    "\n",
    "*Define the purity in terms of class after each split (if one class dominate the region, value is very small)\n",
    "\n",
    "#### Process Flow: \n",
    "Split the features in an order that achieve the smallest [purity metric] (One feature at a time); Ypred ~ the major class in split which Y(i) belongs to; Stopping criterion – each region (region formed from splitting) < minimal observations. Ex. =5; \n",
    "\n",
    "Pruning Tree – tuning parameter to trade-off between over fitting and complexity\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "\n",
    "############# tree package ##############\n",
    "# https://www.r-bloggers.com/classification-trees/\n",
    "library(tree)\n",
    "\n",
    "xtabs( ~ class, data = ecoli.df)\n",
    "\"\"\"\n",
    "class\n",
    " cp  im imL imS imU  om omL  pp \n",
    "143  77   2   2  35  20   5  52\n",
    "\"\"\"\n",
    "\n",
    "ecoli.tree1 = tree(class ~ mcv + gvh + lip + chg + aac + alm1 + alm2,\n",
    "  data = ecoli.df)\n",
    "summary(ecoli.tree1)\n",
    "\n",
    "\"\"\"\n",
    "Classification tree:\n",
    "tree(formula = class ~ mcv + gvh + lip + chg + aac + alm1 + alm2, \n",
    "    data = ecoli.df)\n",
    "Variables actually used in tree construction:\n",
    "[1] \"alm1\" \"mcv\"  \"gvh\"  \"aac\"  \"alm2\"\n",
    "Number of terminal nodes:  10 \n",
    "Residual mean deviance:  0.7547 = 246 / 326 \n",
    "Misclassification error rate: 0.122 = 41 / 336\n",
    "\"\"\"\n",
    "# Ploting the tree\n",
    "plot(ecoli.tree1)\n",
    "text(ecoli.tree1, all = T)\n",
    "\n",
    "# To prune the tree we use cross-validation to identify the point to prune.\n",
    "cv.tree(ecoli.tree1)\n",
    "\"\"\"\n",
    "$size\n",
    " [1] 10  9  8  7  6  5  4  3  2  1\n",
    " \n",
    "$dev\n",
    " [1]  463.6820  457.4463  447.9824  441.8617  455.8318  478.9234  533.5856  586.2820  713.2992 1040.3878\n",
    " \n",
    "$k\n",
    " [1]      -Inf  12.16500  15.60004  19.21572  34.29868  41.10627  50.57044  64.05494 180.78800 355.67747\n",
    " \n",
    "$method\n",
    "[1] \"deviance\"\n",
    " \n",
    "attr(,\"class\")\n",
    "[1] \"prune\"         \"tree.sequence\"\n",
    "\"\"\"\n",
    "# This suggests a tree size of 6 and we can re-fit the tree:\n",
    "ecoli.tree2 = prune.misclass(ecoli.tree1, best = 6)\n",
    "summary(ecoli.tree2)\n",
    "\n",
    "# whether pruning tree\n",
    "Set.seed(3)\n",
    "cv.table = cv.tree(tree, FUN = prune.misclass)\n",
    "cv.table\n",
    "# Show graph\n",
    "Par(mfrow = c(1,2))\n",
    "Plot(cv.table$size, cv.table$dev, type = “b”)\n",
    "Plot(……………$k, ………………………………………..)\n",
    "# prune the tree\n",
    "Prune.table = prune.misclass(tree, bast=[size])\n",
    "Plot(prune.table)\n",
    "Text(prune.table, pretty=0)\n",
    "\n",
    "Predict.tree <- predict(tree, test.data, type = “class”)\n",
    "Table(predict.tree, test.data$Y)\n",
    "\n",
    "\n",
    "\n",
    "########## rpart packages ##############\n",
    "# https://cran.r-project.org/web/packages/rpart/rpart.pdf\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "mytree <- rpart(Y~., data=data, method=\"class\", # classifier\n",
    "               control=rpart.control(minsplit=1000, # minmal size to attempt a split\n",
    "                                    minbucket=300, # min size in each leaf\n",
    "                                    cp=0.008, # complexity \n",
    "                                    maxdepth=13, # max splits\n",
    "                                    xval=5)) # number of CV performed\n",
    "summary(mytree)\n",
    "rpart.plot(mytree)\n",
    "printcp(mytree)\n",
    "mytree$variable.importance\n",
    "\n",
    "a = rownames(mytree$frame) == '<leaf>'\n",
    "path.rpart(mytree, nodes=as.numeric(a[1]), print.it=FALSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "# After being fitted, the model can then be used to predict the class of samples:\n",
    "clf.predict([[2., 2.]])\n",
    "\n",
    "# Alternatively, the probability of each class can be predicted, which is the fraction of training samples \n",
    "# of the same class in a leaf:\n",
    "clf.predict_proba([[2., 2.]])\n",
    "\n",
    "# Case 2 - use irs dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "\n",
    "# fitting the model ------------------ Sample code\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) # impurity meansure // depth of tree\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Once trained, we can export the tree in Graphviz format using the export_graphviz exporter. \n",
    "# Below is an example export of a tree trained on the entire iris dataset:\n",
    "with open(\"iris.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)\n",
    "\n",
    "# Then we can use Graphviz’s dot tool to create a PDF file (or any other supported file type): \n",
    "# dot -Tpdf iris.dot -o iris.pdf.\n",
    "import os\n",
    "os.unlink('iris.dot')\n",
    "    \n",
    "# Plot\n",
    "from IPython.display import Image  \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Regression Tree (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "- Predict Continuous\n",
    "\n",
    "https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf\n",
    "\n",
    "The CART algorithm decides on a split based on the amount of homogeneity within class that is achieved by the split. And later on, the split is reconsidered based on considerations of over-fitting.\n",
    "\n",
    "In sum, the CART implementation is very similar to C4.5; the one notable difference is that CART constructs the tree based on a numerical splitting criterion recursively applied to the data, whereas C4.5 includes the intermediate step of constructing *rule set*s.\n",
    "\n",
    "A regression tree is built through a process known as binary recursive partitioning, which is an iterative process that splits the data into partitions or branches, and then continues splitting each partition into smaller groups as the method moves up each branch.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "Minimal observations in each region ~ 5\n",
    "\n",
    "Pruning parameter (trade-off over fitting - complexity) ~ 0 - infinite\n",
    "\n",
    "#### Cost Function: \n",
    "Minimize the RSS given each split\n",
    "\n",
    "#### Process Flow: \n",
    "Split the features in an order that achieve the smallest RSS (One feature at a time); RSS = sum(Y(i) – Ypred)^2 where Ypred ~ the Mean in split which Y(i) belongs to; Stopping criterion – each region (region formed from splitting) < minimal observations. Ex. =5; \n",
    "\n",
    "Pruning Tree – tuning parameter to trade-off between over fitting and complexity\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "############## tree package ################\n",
    "# http://www.di.fc.ul.pt/~jpn/r/tree/tree.html\n",
    "library(tree)\n",
    "\n",
    "real.estate <- read.table(\"cadata.dat\", header=TRUE)\n",
    "tree.model <- tree(log(MedianHouseValue) ~ Longitude + Latitude, data=real.estate)\n",
    "plot(tree.model)\n",
    "text(tree.model, cex=.75)\n",
    "\n",
    "# We can compare the predictions with the dataset \n",
    "# (darker is more expensive) which seem to capture the global price trend:\n",
    "price.deciles <- quantile(real.estate$MedianHouseValue, 0:10/10)\n",
    "cut.prices <- cut(real.estate$MedianHouseValue, price.deciles, include.lowest=TRUE)\n",
    "plot(real.estate$Longitude, real.estate$Latitude, col=grey(10:2/11)[cut.prices], pch=20, xlab=\"Longitude\",ylab=\"Latitude\")\n",
    "partition.tree(tree.model, ordvars=c(\"Longitude\",\"Latitude\"), add=TRUE)\n",
    "# Summary\n",
    "summary(tree.model)\n",
    "\n",
    "# whether pruning tree\n",
    "Set.seed(3)\n",
    "cv.table = cv.tree(tree)\n",
    "cv.table\n",
    "# Show graph\n",
    "Par(mfrow = c(1,2))\n",
    "Plot(cv.table$size, cv.table$dev, type = “b”)\n",
    "# prune the tree\n",
    "Prune.table = prune.tree(tree, bast=[size])\n",
    "Plot(prune.table)\n",
    "Text(prune.table, pretty=0)\n",
    "\n",
    "Predict.tree <- predict(tree, test.data[,-Y])\n",
    "Plot(predict.tree, Y)\n",
    "Abline(0,1)\n",
    "Mean((predict.tree – Y)^2) # MSE\n",
    "\n",
    "\n",
    "\n",
    "########## rpart packages ##############\n",
    "# https://cran.r-project.org/web/packages/rpart/rpart.pdf\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "mytree <- rpart(Y~., data=data, method=\"anova\", # regressor\n",
    "               control=rpart.control(minsplit=1000, # minmal size to attempt a split\n",
    "                                    minbucket=300, # min size in each leaf\n",
    "                                    cp=0.008, # complexity \n",
    "                                    maxdepth=13, # max splits\n",
    "                                    xval=5)) # number of CV performed\n",
    "summary(mytree)\n",
    "rpart.plot(mytree)\n",
    "printcp(mytree)\n",
    "mytree$variable.importance\n",
    "\n",
    "a = rownames(mytree$frame) == '<leaf>'\n",
    "path.rpart(mytree, nodes=as.numeric(a[1]), print.it=FALSE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "# http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html\n",
    "# http://scikit-learn.org/stable/modules/tree.html\n",
    "\" Mostly the same as classification tree but use 'DecisionTreeRegressor()' instead \"\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])\n",
    "array([ 0.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- C4.5 - C5.0 Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "- Predict Categorical (Classifier)\n",
    "\n",
    "https://en.wikipedia.org/wiki/C4.5_algorithm\n",
    "\n",
    "- C4.5 Tree\n",
    "\n",
    "C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan.[1] C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy. The training data is a set S={s_{1},s_{2},...} of already classified samples. Each sample s_{i} consists of a p-dimensional vector (x_{{1,i}},x_{{2,i}},...,x_{{p,i}}), where the x_{j} represent attribute values or features of the sample, as well as the class in which s_{i} falls. At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurs on the smaller sublists. \n",
    "\n",
    "Quinlan's next iteration. The new features (versus ID3) are: (i) accepts both continuous and discrete features; (ii) handles incomplete data points; (iii) solves over-fitting problem by (very clever) bottom-up technique usually known as \"pruning\"; and (iv) different weights can be applied the features that comprise the training data. Of these, the first three are very important--and i would suggest that any DT implementation you choose have all three. The fourth (differential weighting) is much less important\n",
    "\n",
    "- C5.0 Tree\n",
    "\n",
    "Quinlan went on to create C5.0 and See5 (C5.0 for Unix/Linux, See5 for Windows) which he markets commercially. C5.0 offers a number of improvements on C4.5. Some of these are:[5][6]\n",
    "\n",
    "Perhaps the most significant claimed improvement of C5.0 versus C4.5 is support for boosted trees. Ensemble support for DTs--boosted trees and Random Forests--has been included in the DT implementation in Orange; here, ensemble support was added to a C4.5 algorithm.\n",
    "\n",
    "*Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude)\n",
    "\n",
    "*Memory usage - C5.0 is more memory efficient than C4.5\n",
    "\n",
    "*Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees.\n",
    "\n",
    "*Support for boosting - Boosting improves the trees and gives them more accuracy.\n",
    "\n",
    "*Weighting - C5.0 allows you to weight different cases and misclassification types.\n",
    "\n",
    "*Winnowing - a C5.0 option automatically winnows the attributes to remove those that may be unhelpful.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "Shannon Entropy - to pick the features with the largest information gains\n",
    "\n",
    "#### Process Flow: \n",
    "- C4.5 Tree\n",
    "It builds a decision tree for the given data in a top-down fashion, starting from a set of objects and a specification of properties Resources and Information. each node of the tree, one property is tested based on maximizing information gain and minimizing entropy, and the results are used to split the object set. This process is recursively done until the set in a given sub-tree is homogeneous (i.e. it contains objects belonging to the same category). The ID3 algorithm uses a greedy search. It selects a test using the information gain criterion, and then never explores the possibility of alternate choices.\n",
    "\n",
    "\n",
    "- C5.0 Tree\n",
    "［same as above]\n",
    "\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "######## C4.5 Tree #######\n",
    "\n",
    "\" C5.0 is better in all - \"\n",
    "\n",
    "######## C5.0 Tree #######\n",
    "# http://www.patricklamle.com/Tutorials/Decision%20tree%20R/Decision%20trees%20in%20R%20using%20C50.html\n",
    "# https://cran.r-project.org/web/packages/C50/C50.pdf\n",
    "\n",
    "credit <- read.csv(\"credit.csv\")\n",
    "str(credit)\n",
    "\"\"\"\n",
    "'data.frame':\t1000 obs. of  17 variables:\n",
    " $ checking_balance    : Factor w/ 4 levels \"< 0 DM\",\"> 200 DM\",..: 1 3 4 1 1 4 4 3 4 3 ...\n",
    " $ months_loan_duration: int  6 48 12 42 24 36 24 36 12 30 ...\n",
    " $ credit_history      : Factor w/ 5 levels \"critical\",\"good\",..: 1 2 1 2 4 2 2 2 2 1 ...\n",
    " $ purpose             : Factor w/ 6 levels \"business\",\"car\",..: 5 5 4 5 2 4 5 2 5 2 ...\n",
    " $ amount              : int  1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ...\n",
    " $ savings_balance     : Factor w/ 5 levels \"< 100 DM\",\"> 1000 DM\",..: 5 1 1 1 1 5 4 1 2 1 ...\n",
    " $ employment_duration : Factor w/ 5 levels \"< 1 year\",\"> 7 years\",..: 2 3 4 4 3 3 2 3 4 5 ...\n",
    " $ percent_of_income   : int  4 2 2 2 3 2 3 2 2 4 ...\n",
    " $ years_at_residence  : int  4 2 3 4 4 4 4 2 4 2 ...\n",
    " $ age                 : int  67 22 49 45 53 35 53 35 61 28 ...\n",
    " $ other_credit        : Factor w/ 3 levels \"bank\",\"none\",..: 2 2 2 2 2 2 2 2 2 2 ...\n",
    " $ housing             : Factor w/ 3 levels \"other\",\"own\",..: 2 2 2 1 1 1 2 3 2 2 ...\n",
    " $ existing_loans_count: int  2 1 1 1 2 1 1 1 1 2 ...\n",
    " $ job                 : Factor w/ 4 levels \"management\",\"skilled\",..: 2 2 4 2 2 4 2 1 4 1 ...\n",
    " $ dependents          : int  1 1 2 2 2 2 1 1 1 1 ...\n",
    " $ phone               : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 1 1 2 1 2 1 1 ...\n",
    " $ default             : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 1 2 1 1 1 1 2 ...\n",
    "\"\"\"\n",
    "\n",
    "# Train Model\n",
    "library(C50)\n",
    "credit_model <- C5.0(credit_train[-17], credit_train$default)\n",
    "\n",
    "summary(credit_model)\n",
    "\n",
    "# Prediction\n",
    "credit_pred <- predict(credit_model, credit_test)\n",
    "\n",
    "# Confusion table\n",
    "library(gmodels)\n",
    "CrossTable(credit_test$default, credit_pred,\n",
    "           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n",
    "           dnn = c('actual default', 'predicted default'))\n",
    "\n",
    "\" 5. Improving the model with (adaptative) boosting ¶\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "######## C4.5 Tree #######\n",
    "\n",
    "\" C5.0 is better in all - \"\n",
    "\n",
    "######## C5.0 Tree #######\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Chi-squared Automatic Interaction Detection (CHAID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Chi-square Automatic Interaction Detector (CHAID) was a technique created by Gordon V. Kass in 1980.  CHAID is a tool used to discover the relationship between variables.  CHAID analysis builds a predictive medel, or tree, to help determine how variables best merge to explain the outcome in the given dependent variable. In CHAID analysis, nominal, ordinal, and continuous data can be used, where continuous predictors are split into categories with approximately equal number of observations.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "At each split, the algorithm looks for the predictor variable that if split, most \"explains\" the category response variable. In order to decide whether to create a particular split based on this variable, the CHAID algorithm tests a hypothesis regarding dependence between the splitted variable and the categorical response(using the chi-squared test for independence). Using a pre-specified significance level, if the test shows that the splitted variable and the response are independent, the algorithm stops the tree growth. Otherwise the split is created, and the next best split is searched\n",
    "\n",
    "CHAID creates all possible cross tabulations for each categorical predictor until the best outcome is achieved and no further splitting can be performed.  In the CHAID technique, we can visually see the relationships between the split variables and the associated related factor within the tree.  The development of the decision, or classification tree, starts with identifying the target variable or dependent variable; which would be considered the root.  CHAID analysis splits the target into two or more categories that are called the initial, or parent nodes, and then the nodes are split using statistical algorithms into child nodes. Unlike in regression analysis, the CHAID technique does not require the data to be normally distributed.\n",
    "\n",
    "Merging: In CHAID analysis, if the dependent variable is continuous, the F test is used and if the dependent variable is categorical, the chi-square test is used.  Each pair of predictor categories are assessed to determine what is least significantly different with respect to the dependent variable.  Due to these steps of merging, a Bonferroni adjusted p-value is calculated for the merged cross tabulation.\n",
    "\n",
    "In CHAID analysis, the following are the components of the decision tree:\n",
    "\n",
    "1.Root node: Root node contains the dependent, or target, variable.  For example, CHAID is appropriate if a bank wants to predict the credit card risk based upon information like age, income, number of credit cards, etc.  In this example, credit card risk is the target variable and the remaining factors are the predictor variables.\n",
    "\n",
    "2.Parent's node: The algorithm splits the target variable into two or more categories.  These categories are called parent node or initial node.  For the bank example, high, medium and low categories are the parent's nodes.\n",
    "\n",
    "3.Child node: Independent variable categories which come below the parent's categories in the CHAID analysis tree are called the child node.\n",
    "\n",
    "4.Terminal node: The last categories of the CHAID analysis tree are called the terminal node.  In the CHAID analysis tree, the category that is a major influence on the dependent variable comes first and the less important category comes last.  Thus, it is called the terminal node.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------- R\n",
    "# http://dni-institute.in/blogs/tag/chaid-using-r/\n",
    "# Decision Tree: CHAID\n",
    " \n",
    "install.packages(\"CHAID\", repos=\"http://R-Forge.R-project.org\")\n",
    "library(CHAID)\n",
    "library(help=CHAID)\n",
    " \n",
    "names(termCrosssell)\n",
    " \n",
    "table(termCrosssell$housing)\n",
    " \n",
    "dt.chaid  <- chaid(y~ marital+education , \n",
    "                   control = chaid_control(minprob = 0.001,\n",
    "                                           minsplit = 500,minbucket = 200),\n",
    "                   data=termCrosssell)\n",
    " \n",
    "plot(dt.chaid, \n",
    "     uniform = T, \n",
    "     compress = T, \n",
    "     margin = 0.2, \n",
    "     branch = 0.3)\n",
    "# Label on Decision Tree\n",
    "text(dt.chaid, \n",
    "     use.n = T, \n",
    "     digits = 3, \n",
    "     cex = 0.6)\n",
    "summary(dt.chaid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Python\n",
    "# https://pypi.python.org/pypi/CHAID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Decision Stump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Decision stumps perform surprisingly well on some commonly used benchmark datasets from the UCI repository (Holte, 1993), which illustrates that learners with a high Bias and low Variance may perform well because they are less prone to Overfitting. Decision stumps are also often used as weak learners in Ensemble Methods such as boosting\n",
    "\n",
    "A decision stump is a machine learning model consisting of a one-level decision tree. That is, it is a decision tree with one internal node (the root) which is immediately connected to the terminal nodes (its leaves). A decision stump makes a prediction based on the value of just a single input feature. Sometimes they are also called 1-rules.\n",
    "Depending on the type of the input feature, several variations are possible. For nominal features, one may build a stump which contains a leaf for each possible feature value or a stump with the two leaves, one of which corresponds to some chosen category, and the other leaf to all the other categories. For binary features these two schemes are identical. A missing value may be treated as a yet another category.\n",
    "\n",
    "For continuous features, usually, some threshold feature value is selected, and the stump contains two leaves — for values below and above the threshold. However, rarely, multiple thresholds may be chosen and the stump therefore contains three or more leaves.\n",
    "\n",
    "Decision stumps are often used as components (called \"weak learners\" or \"base learners\") in machine learning ensemble techniques such as bagging and boosting. For example, a state-of-the-art Viola–Jones face detection algorithm employs AdaBoost with decision stumps as weak learners.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "A decision stump is a Decision Tree, which uses only a single attribute for splitting. For discrete attributes, this typically means that the tree consists only of a single interior node (i.e., the root has only leaves as successor nodes). If the attribute is numerical, the tree may be more complex.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R\n",
    "# Example used in ensemble model\n",
    "pkgs <- c('pROC', 'RWeka')\n",
    "lapply(pkgs, require, character.only = T)\n",
    "df1 <- read.csv(\"credit_count.txt\")\n",
    "df2 <- df1[df1$CARDHLDR == 1, ]\n",
    "set.seed(2016)\n",
    "n <- nrow(df2)\n",
    "sample <- sample(seq(n), size = n / 2, replace = FALSE)\n",
    "train <- df2[sample, ]\n",
    "test <- df2[-sample, ]\n",
    "x <- paste(\"AGE + ACADMOS + ADEPCNT + MAJORDRG + MINORDRG + OWNRENT + INCOME + SELFEMPL + INCPER + EXP_INC\")\n",
    "fml <- as.formula(paste(\"as.factor(DEFAULT) ~ \", x))\n",
    " \n",
    "### IDENTIFY THE MOST PREDICTIVE ATTRIBUTE ###\n",
    "imp <- InfoGainAttributeEval(fml, data = train)\n",
    "imp_x <- test[, names(imp[imp == max(imp)])]\n",
    "roc(as.factor(test$DEFAULT), imp_x)\n",
    "# Area under the curve: 0.6243\n",
    " \n",
    "### CONSTRUCT A WEAK CLASSIFIER OF DECISION STUMP ###\n",
    "stump <- DecisionStump(fml, data = train)\n",
    "print(stump)\n",
    "roc(as.factor(test$DEFAULT), predict(stump, newdata = test, type = \"probability\")[, 2])\n",
    "# Area under the curve: 0.5953\n",
    "### BUILD A BAGGING CLASSIFIER WITH 1,000 STUMPS IN PARALLEL ###\n",
    "bagging <- Bagging(fml, data = train, control = Weka_control(\"num-slots\" = 0, I = 1000, W = \"DecisionStump\", S = 2016, P = 50))\n",
    "roc(as.factor(test$DEFAULT), predict(bagging, newdata = test, type = \"probability\")[, 2])\n",
    "# Area under the curve: 0.6346\n",
    " \n",
    "### BUILD A BOOSTING CLASSIFIER WITH STUMPS ###\n",
    "boosting <- AdaBoostM1(fml, data = train, control = Weka_control(I = 100, W = \"DecisionStump\", S = 2016))\n",
    "roc(as.factor(test$DEFAULT), predict(boosting, newdata = test, type = \"probability\")[, 2])\n",
    "# Area under the curve: 0.6585\n",
    "  \n",
    "### DEVELOP A LOGIT MODEL FOR THE BENCHMARK ###\n",
    "logit <- Logistic(fml, data = train)\n",
    "roc(as.factor(test$DEFAULT), predict(logit, newdata = test, type = \"probability\")[, 2])\n",
    "# Area under the curve: 0.6473\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python\n",
    "# Any tree make only one split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------- Cubist Model Tree (Extension to M5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Cubist is a rule{based model that is an extension of Quinlan's M5 model tree. A tree is grown where the terminal leaves contain linear regression models. These models are based on the predictors used in previous splits. Also, there are intermediate linear models at each step of the tree. A prediction is made using the linear regression model at the terminal node of the tree, but is \\smoothed\" by taking into account the prediction from the linear model in the previous node of the tree (which also occurs recursively up the tree). The tree is reduced to a set of rules, which initially are paths from the top of the tree to the bottom. Rules are eliminated via pruning and/or combined for simplication.\n",
    "\n",
    "This is explained better in Quinlan (1992). Wang and Witten (1997) attempted to recreate this model using a \\rational reconstruction\" of Quinlan (1992) that is the basis for the M5P model in Weka (and the R package RWeka).\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "http://sci2s.ugr.es/keel/pdf/algorithm/congreso/1992-Quinlan-AI.pdf\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------- R\n",
    "# https://cran.r-project.org/web/packages/Cubist/vignettes/cubist.pdf\n",
    "library(Cubist)\n",
    "library(mlbench)\n",
    "data(BostonHousing)\n",
    "BostonHousing$chas <- as.numeric(BostonHousing$chas) - 1\n",
    "set.seed(1)\n",
    "inTrain <- sample(1:nrow(BostonHousing), floor(.8*nrow(BostonHousing)))\n",
    "trainingPredictors <- BostonHousing[ inTrain, -14]\n",
    "testPredictors <- BostonHousing[-inTrain, -14]\n",
    "trainingOutcome <- BostonHousing$medv[ inTrain]\n",
    "testOutcome <- BostonHousing$medv[-inTrain]\n",
    "modelTree <- cubist(x = trainingPredictors, y = trainingOutcome)\n",
    "modelTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------- GUIDE Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "http://www.stat.wisc.edu/~loh/treeprogs/guide/guideman.pdf\n",
    "\n",
    "Classification trees and Piecewise-linear least-squares, quantile, Poisson, proportional hazards or multi-response (e.g., longtudinal) regression trees.\n",
    "GUIDE stands for Generalized, Unbiased, Interaction Detection and Estimation. It is the only classification and \n",
    "\n",
    "regression tree algorithm with all these features:\n",
    "\n",
    "1. Unbiased variable selection.\n",
    "\n",
    "2. Kernel and nearest-neighbor node models for classification trees.\n",
    "\n",
    "3. Weighted least squares, least median of squares, quantile, Poisson, and relative risk (proportional hazards) regression models.\n",
    "\n",
    "4. Univariate, multivariate, and longitudinal response variables.\n",
    "\n",
    "5. Pairwise interaction detection at each node.\n",
    "\n",
    "6. Linear splits on two variables at a time for classification trees.\n",
    "\n",
    "7. Categorical variables for splitting only, or for both splitting and fitting (via 0-1 dummy variables), in regression tree models.\n",
    "\n",
    "8. Ranking and scoring of predictor variables.\n",
    "\n",
    "9. Tree ensembles (bagging and forests).\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "http://www.stat.wisc.edu/~loh/treeprogs/guide/LFMCY16.pdf\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------ R\n",
    "# http://user.math.uzh.ch/hothorn/talks/constparty_Singapore_2014.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------- MOB Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://cran.r-project.org/web/packages/party/vignettes/MOB.pdf\n",
    "\n",
    "An recently suggested algorithm for model-based recursive partitioning (Zeileis,\n",
    "Hothorn, and Hornik 2008). The basic steps are: (1) fit a parametric model to a data set,\n",
    "(2) test for parameter instability over a set of partitioning variables, (3) if there is some\n",
    "overall parameter instability, split the model with respect to the variable associated with\n",
    "the highest instability, (4) repeat the procedure in each of the child nodes.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "The basic idea is to grow a tee in which every node is associated with a model of typeM. To\n",
    "assess whether splitting of the node is necessary a fluctuation test for parameter instability is\n",
    "performed. If there is significant instability with respect to any of the partitioning variables\n",
    "Zj , the node is splitted into B locally optimal segments (currently only B = 2 is implemented)\n",
    "and then the procedure is repeated in each of the B children. If no more significant instabilities\n",
    "can be found, the recursion stops. More precisely, the steps of the algorithm are\n",
    "\n",
    "1. Fit the model once to all observations in the current node.\n",
    "\n",
    "2. Assess whether the parameter estimates are stable with respect to every partitioning variable Z1, . . . ,Zℓ. If there is some overall instability, select the variable Zj associated with the highest parameter instability, otherwise stop.\n",
    "\n",
    "3. Compute the split point(s) that locally optimize the objective function \t.\n",
    "\n",
    "4. Split the node into child nodes and repeat the procedure\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------- R\n",
    "Library(party)\n",
    "data(\"BostonHousing\", package = \"mlbench\")\n",
    "BostonHousing$lstat <- log(BostonHousing$lstat)\n",
    "BostonHousing$rm <- BostonHousing$rm^2\n",
    "BostonHousing$chas <- factor(BostonHousing$chas, levels = 0:1, labels = c(\"no\", \"yes\"))\n",
    "BostonHousing$rad <- factor(BostonHousing$rad, ordered = TRUE)\n",
    "ctrl <- mob_control(alpha = 0.05, bonferroni = TRUE, minsplit = 40,\n",
    "+ objfun = deviance, verbose = TRUE)\n",
    "fmBH <- mob(medv ~ lstat + rm | zn + indus + chas + nox + age + dis + rad + tax + crim +\n",
    "+ data = BostonHousing, control = ctrl, model = linearModel)\n",
    "fmBH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------- QUEST Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "(Quick, Unbiased, and Efficient Statistical Tree) QUEST is a binary-split decision tree algorithm for classification and data mining. The objective of QUEST is similar to that of the CART(TM) algorithm described in the book, Classification and Regression Trees. \n",
    "\n",
    "QUEST has the following properties: \n",
    "\n",
    "It uses an unbiased variable selection technique by default. \n",
    "\n",
    "It provides linear splits using Fisher's LDA method. \n",
    "\n",
    "It uses imputation instead of surrogate splits to deal with missing values. \n",
    "\n",
    "It can easily handle categorical predictor variables with many categories. \n",
    "\n",
    "If there are no missing values in the data, it can optionally use the CART algorithm to produce a tree with univariate splits. \n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "It uses ANOVA F and contingency table Chi Square tests to select variables for splitting. Variables with multiple classes are merged into two super-classes to get binary splits which are determined using QDA (Quadratic Discriminant analysis). The tree can be pruned using the CART algorithm. It can be used for both classification and regression tasks.\n",
    "\n",
    "Quest first transforms categorical (symbolic) variables into continuous variables by assigning discriminant coordinates to categories of the predictor. Then it applies quadratic discriminant analysis (QDA) to determine the split point. Notice that QDA usually produces two cut-off points—choose the one that is closer to the sample mean of the first superclass.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------- R\n",
    "# https://cran.r-project.org/web/packages/partykit/vignettes/partykit.pdf\n",
    "# http://user.math.uzh.ch/hothorn/talks/constparty_Singapore_2014.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------- Conditional Inference Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://cran.r-project.org/web/packages/partykit/vignettes/ctree.pdf\n",
    "\n",
    "CTree is a non-parametric class of regression trees embedding tree-structured regression models into a well defined theory of conditional inference procedures.\n",
    "\n",
    "It is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. A statistical approach [to recursive partitioning] which takes into account the distributional properties of the measures. \n",
    "Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "It uses a covariate selection scheme that is based on statistical theory (i.e. selection by permutation-based significance tests) and thereby avoids a potential bias in rpart, otherwise they seem similar; e.g. conditional inference trees can be used as base learners for Random Forests.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------ R\n",
    "# https://cran.r-project.org/web/packages/partykit/vignettes/ctree.pdf\n",
    "Library(partykit)\n",
    "Ctree_control(teststat = “max”)\n",
    "ls <- data.frame(y = gl(3, 50, labels = c(\"A\", \"B\", \"C\")),\n",
    "+ x1 = rnorm(150) + rep(c(1, 0, 0), c(50, 50, 50)),\n",
    "+ x2 = runif(150))\n",
    "ct <- ctree(y ~ x1 + x2, data = ls); plot(ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------- Logistic Model Tree (LMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In computer science, a logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning.[1][2]\n",
    "\n",
    "Logistic model trees are based on the earlier idea of a model tree: a decision tree that has linear regression models at its leaves to provide a piecewise linear regression model (where ordinary decision trees with constants at their leaves would produce a piecewise constant model).[1] In the logistic variant, the LogitBoost algorithm is used to produce an LR model at every node in the tree; the node is then split using the C4.5 criterion. Each LogitBoost invocation is warm-started[vague] from its results in the parent node. Finally, the tree is pruned.[3]\n",
    "The basic LMT induction algorithm uses cross-validation to find a number of LogitBoost iterations that does not overfit the training data. A faster version has been proposed that uses the Akaike information criterion to control LogitBoost stopping.[\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "http://www.cs.uni-potsdam.de/~landwehr/diploma_thesis.pdf\n",
    "\n",
    "New scheme for selecting the attributes to be included in the logistic regression models, and introduce a way of building the logistic models at the leaves by refining logistic models that have been trained at higher levels in the tree, i.e. on larger subsets of the training data. training data.\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------- R\n",
    "# https://cran.r-project.org/web/packages/RWeka/RWeka.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ----------------------- Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "Random forests or random decision forests[1][2] are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. Decision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie et al., because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate. In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3]:587–588 This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "#### Input Data: \n",
    "X(Numeric) / X(Categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "Number of features sampled each time when train the tree: M\n",
    "\n",
    "Number of trees need to train to make prediction: B\n",
    "\n",
    "#### Cost Function: \n",
    "- Regression ~ (RSS) \n",
    "\n",
    "- Classification ~ (Classification error, Gini index, Cross-entropy)\n",
    "\n",
    "#### Process Flow: \n",
    "We need many tree to average them to reduce variance and increase prediction accuracy -> bootstrap raw data with the same size of observation but only randomly select m subset of features (m < P) to train one tree (No pruning – full tree) -> Repeat multiple times to form the “forest” -> If regression, average all prediction, If classification, choose the majority class\n",
    "\n",
    "-Variable importance ~ each feature: {total RSS decreased / number of trees} | {total Gini decreased / number of trees}\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "Library(“randomForest”)\n",
    "# mtry = number of features to select, ntree = number of trees to build\n",
    "Tree.random <- randomForest(Y ~. , data = train, mtry = 6, ntree = 500, importance = TRUE)\n",
    "Predict.random <- predict(Tree.random, newdata = test)\n",
    "# Importance\n",
    "Importance(Tree.random) # %Increase the large the import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "# Classifier\n",
    "# -- Random Forests ---------------- Sample code 1\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy', # impurity meansure\n",
    "                                n_estimators=10, # learners\n",
    "                                random_state=1,\n",
    "                                n_jobs=2) # cores\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ---------------------------------- Sample code 2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "df['species'] = pd.Factor(iris.target, iris.target_names)\n",
    "df.head()\n",
    "\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "\n",
    "features = df.columns[:4]\n",
    "clf = RandomForestClassifier(n_jobs=2)\n",
    "y, _ = pd.factorize(train['species'])\n",
    "clf.fit(train[features], y)\n",
    "\n",
    "preds = iris.target_names[clf.predict(test[features])]\n",
    "pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])\n",
    "\n",
    "# Regressor\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# http://scikit-learn.org/stable/modules/ensemble.html#forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Bagging Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "It is almost the same as random forest, except it uses all P features to train one tree each time except use only subset m of the features.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "Number of trees need to train to make prediction: B\n",
    "\n",
    "#### Cost Function: \n",
    "- Regression ~ (RSS) \n",
    "\n",
    "- Classification ~ (Classification error, Gini index, Cross-entropy)\n",
    "\n",
    "#### Process Flow: \n",
    "We need many tree to average them to reduce variance and increase prediction accuracy -> bootstrap raw data with the same size of observation and same size of features P to train one tree (No pruning – full tree) -> Repeat multiple times to form the “forest” -> If regression, average all prediction, If classification, choose the majority class\n",
    "\n",
    "-Variable importance ~ each feature: {total RSS decreased / number of trees} | {total Gini decreased / number of trees}\n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "library(“randomForest”)\n",
    "# mtry = number of features to select, ntree = number of trees to build, choose mtry = P becomes bagging\n",
    "Tree.random <- randomForest(Y ~. , data = train, mtry = 13, ntree = 500, importance = TRUE)\n",
    "Predict.random <- predict(Tree.random, newdata = test)\n",
    "# Importance\n",
    "Importance(Tree.random) # %Increase the large the import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "\n",
    "# Classifier\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)\n",
    "# Regressor\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import [regressor]\n",
    "bagging = BaggingRegressor ([regressor](),\n",
    "                            max_samples=0.5, max_features=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Generalized Boosted Regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "\n",
    "It builds based on previous tree and focus on those weakness to boost the performance.\n",
    "\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "Number of trees (If too large over fit the data * different from bagging)\n",
    "\n",
    "Shrinkage parameter (A small positive number) – controls learning rate of boosting ~ 0.01 or 0.001. Ex. very small shrinkage requires large number of trees\n",
    "\n",
    "Number of split in each tree (Control the complexity – data fit of the boosting) ~ 1 (weak learner better)\n",
    "\n",
    "#### Cost Function: \n",
    "RSS\n",
    "#### Process Flow: \n",
    "Build a weak learner on data -> obtains the error when predict -> reweight the model to build a new model focus on those error and plus the previous model -> repeat this process for many new model to address the error -> boosted the performance \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "# https://cran.r-project.org/web/packages/gbm/gbm.pdf\n",
    "library(gbm)\n",
    "# shrinkage default = 0.001, n.trees = number of trees, interaction.depth = maximimal tree split, distribution = Y=binary classification, Bernoulli, Y=numeric regression, gaussian\n",
    "Table.boost <- gbm(Y ~ . , data = train, shrinkage = 0.001, distribution = \"gaussian\", n.trees = 100, interaction.depth=1)\n",
    "Summary(Table.boost)\n",
    "Predict.boost = predict(Table.boost, newdata = test, n.trees=5000)\n",
    "Mean((predict.boost – Y)^2) # MSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "    max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --------------------- Generalized Boosted Classification tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "\n",
    "It builds based on previous tree and focus on those weakness to boost the performance.\n",
    "\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n",
    "\n",
    "#### Input Data: \n",
    "X(Numeric) / X(categorical)\n",
    "\n",
    "#### Initial Parameters: \n",
    "Number of trees (If too large over fit the data * different from bagging)\n",
    "\n",
    "Shrinkage parameter (A small positive number) – controls learning rate of boosting ~ 0.01 or 0.001. Ex. very small shrinkage requires large number of trees\n",
    "\n",
    "Number of split in each tree (Control the complexity – data fit of the boosting) ~ 1 (weak learner better)\n",
    "\n",
    "#### Cost Function: \n",
    "(Classification error, Gini index, Cross-entropy)\n",
    "\n",
    "#### Process Flow: \n",
    "Build a weak learner on data -> obtains the error when predict -> reweight the model to build a new model focus on those error and plus the previous model -> repeat this process for many new model to address the error -> boosted the performance \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- R\n",
    "# https://cran.r-project.org/web/packages/gbm/gbm.pdf\n",
    "library(gbm)\n",
    "# shrinkage default = 0.001, n.trees = number of trees, interaction.depth = maximimal tree split, distribution = Y=binary classification, Bernoulli, Y=numeric regression, gaussian\n",
    "Table.boost <- gbm(Y ~ . , data = train, shrinkage = #, distribution = “Bernoulli”, n.trees = #, interaction.depth=#)\n",
    "Summary(Table.boost)\n",
    "Predict.boost = predict(Table.boost, newdata = test, n.trees=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- Python\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
