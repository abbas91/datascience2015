{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/09/questions-ensemble-modeling/\n",
    "https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/\n",
    "http://www.scholarpedia.org/article/Ensemble_learning\n",
    "\n",
    "### Overview:\n",
    "Supervised learning algorithms are most commonly described as performing the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner. The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.\n",
    "Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model, so ensembles may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. Fast algorithms such as decision trees are commonly used in ensemble methods (for example Random Forest), although slower algorithms can benefit from ensemble techniques as well.\n",
    "By analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection.\n",
    "\n",
    "### Ensemble Theory:\n",
    "An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data.\n",
    "Empirically, ensembles tend to yield better results when there is a significant diversity among the models.[4][5] Many ensemble methods, therefore, seek to promote diversity among the models they combine.[6][7] Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).[8] Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.\n",
    "\n",
    "### Ensemble Size:\n",
    "While the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble which having more or less than this number of classifiers would deteriorate the accuracy. It is called \"the law of diminishing returns in ensemble construction.\" Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.\n",
    "\n",
    "#### Create Base Model (Supervised) - Diversity:\n",
    "Sample observations\n",
    "Sample features\n",
    "Different leaner model\n",
    "Introduce randomness to learning procedure\n",
    "\n",
    "#### Create Base model (Unsupervised) - Diversity: \n",
    "Bootstrap Samples\n",
    "Sample features\n",
    "Different clustering algorithm\n",
    "Random number of clusters\n",
    "Random initialization\n",
    "Introduce randomness to learning procedure\n",
    "\n",
    "### Common Type of Ensembles:\n",
    "\n",
    "- Bagging:\n",
    "\n",
    "Bootstrap aggregating, often abbreviated as bagging, involves having each model in the ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy.\n",
    "\n",
    "- Boosting:\n",
    "\n",
    "Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data. By far, the most common implementation of Boosting is Adaboost, although some newer algorithms are reported to achieve better results.\n",
    "\n",
    "- Stacking:\n",
    "\n",
    "Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer logistic regression model is often used as the combiner. Stacking typically yields performance better than any single one of the trained models.[22] It has been successfully used on both supervised learning tasks (regression,[23] classification and distance learning [24]) and unsupervised learning (density estimation).\n",
    "\n",
    "- Bucket of models:\n",
    "\n",
    "The bucket of models has a few different flavors. Methods that fall under this category use the predictions from your models as elements in a feature vector, and then train another model on top of them to make predictions. Two often used models are a perceptron and linear regression.\n",
    "A \"bucket of models\" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.\n",
    "\n",
    "The most common approach used for model-selection is cross-validation selection (sometimes called a \"bake-off contest\"). It is described with the following pseudo-code:\n",
    "For each model m in the bucket:\n",
    "  Do c times: (where 'c' is some constant)\n",
    "    Randomly divide the training dataset into two datasets: A, and B.\n",
    "    Train m with A\n",
    "    Test m with B\n",
    "Select the model that obtains the highest average score\n",
    "\n",
    "- BOC (Bayes Optimal Classifier):\n",
    "\n",
    "Not Focus for now\n",
    "\n",
    "- BPA (Bayesian Parameter Averaging):\n",
    "\n",
    "Not Focus for now\n",
    "\n",
    "Bayesian parameter averaging (BPA) is an ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law.[13] Unlike the Bayes optimal classifier, Bayesian model averaging (BMA) can be practically implemented. Hypotheses are typically sampled using a Monte Carlo sampling technique such as MCMC.\n",
    "\n",
    "- BMC (Bayesian Model Combination)\n",
    "\n",
    "Not Focus for now\n",
    "\n",
    "Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging.\n",
    "\n",
    "#### Advantages: \n",
    "Better predication and stable, robust result comparing to single model; A complex problem can be decomposed into multiple sub-problems that are easier to understand and solve (divide-and-conquer approach); Better result if learners are diversified (Uncorrelated error reduction); \n",
    "\n",
    "#### Disadvantages: \n",
    "Potentially overfit;\n",
    "\n",
    "======================================================================================================================\n",
    "\n",
    "--------------------- R\n",
    "\n",
    "http://machinelearningmastery.com/machine-learning-ensembles-with-r/\n",
    "Library(caretEnsemble)\n",
    "\n",
    "------------------ Python\n",
    "\n",
    "http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Random Forest\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Xgboost (eXtreme Gradient Boosted Trees) \n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: High performance, very fast, handle large data; works for generic loss functions; More customizable parameters; Resist to over-fitting; Can pick up more deeper feature interaction since it is dealing with gradients (Compare to adaboost which only weighting); \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "#### Ada Boosting\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: High Performance, fast, mainly for classification with exponential loss; Good for high dimensional data(automatically select features); Final prediction is linear combination so easy to debug; Resist to over-fitting;\n",
    "\n",
    "Cons: Less customizable than XGboost; Slightly slower than XGboost; Less performed on deeper feature interaction than XGboost; \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------- Xgboost (eXtreme Gradient Boosted Trees) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "XGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is a library designed and optimized for boosting trees algorithms. It uses Gradient Descent for updating parameters;\n",
    "\n",
    "XGBoost’s main goal is to push the extreme of the computation limits of machines to provide a scalable, portable and accurate for large scale tree boosting. The term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman[1]. The GBM, boosted trees, has been around for really a while, and there are a lot of materials on the topic.\n",
    "\n",
    "Boosting is an ensemble technique in which new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made.\n",
    "\n",
    "The ensemble technique uses the tree ensemble model which is a set of classification and regression trees (CART). The ensemble approach is used because a single CART, usually, does not have a strong predictive power. By using a set of CART (i.e. a tree ensemble model) a sum of the predictions of multiple trees is considered.\n",
    "\n",
    "Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction.\n",
    "\n",
    "The objective of the XGBoost model is given as:\n",
    "\n",
    "Obj = L + Ω\n",
    "\n",
    "Where, ",
    "L is the loss function which controls the predictive power, and ",
    "Ω is regularization component which controls simplicity and overfitting\n",
    "The loss function (L) which needs to be optimized can be Root Mean Squared Error for regression, Logloss for binary classification, or mlogloss for multi-class classification.\n",
    "\n",
    "The regularization component (Ω) is dependent on the number of leaves and the prediction score assigned to the leaves in the tree ensemble model.\n",
    "It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models. The Gradient boosting algorithm supports both regression and classification predictive modeling problems.\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "Leaner / number of boosts / leaner parameters \n",
    "#### Cost Function: \n",
    "MSE, Absolute Error \n",
    "\n",
    "#### Process Flow: \n",
    "http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "*Step1\n",
    "*Build model on residuals and make predication, combine predictions = final prediction\n",
    "*Not necessarily tree based model, you can “plug in” various classes of weak learners\n",
    "Fit a model to the data, F1(x) = y\n",
    "Fit a model to the residuals, h1(x) = y - F1(x)\n",
    "Create a new model, F2(x) = F1(x) + h1(x) …\n",
    "\n",
    "*Step2\n",
    "How many iteration of boost tree needed? –Since still generate single prediction, we can use cross-validation to determine M by evaluating Cost function (MSE).\n",
    "\n",
    "*Additional\n",
    "By each gradient descent step multiply a factor 0-1 called “learning rate”\n",
    "It also has ability to sample the data rows and columns (Like Random Forest)\n",
    "#### Evaluation Methods: \n",
    "Regression / classification performance metrics (With CV)\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------- R\n",
    "http://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html\n",
    "install.packages(\"xgboost\")\n",
    "library(xgboost)\n",
    "data(agaricus.train, package='xgboost')\n",
    "data(agaricus.test, package='xgboost')\n",
    "train <- agaricus.train\n",
    "test <- agaricus.test\n",
    "bstSparse <- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = \"binary:logistic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------ Python\n",
    "http://xgboost.readthedocs.io/en/latest/python/python_intro.html\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix('train.svm.txt')\n",
    "dtest = xgb.DMatrix('test.svm.buffer')\n",
    "# Setting parameters\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "evallist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "# train model\n",
    "num_round = 10\n",
    "bst = xgb.train( plst, dtrain, num_round, evallist )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Ada Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "AdaBoost, short for \"Adaptive Boosting\", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the Gödel Prize in 2003 for their work. It can be used in conjunction with many other types of learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing (e.g., their error rate is smaller than 0.5 for binary classification), the final model can be proven to converge to a strong learner.\n",
    "Every learning algorithm will tend to suit some problem types better than others, and will typically have many different parameters and configurations to be adjusted before achieving optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier.[1][2] When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples.\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "Leaner / number of boosts / leaner parameters\n",
    "\n",
    "#### Cost Function: \n",
    "Weak leaner cost function (AUC)\n",
    "\n",
    "#### Process Flow: \n",
    "http://mccormickml.com/2013/12/13/adaboost-tutorial/\n",
    "\n",
    "AdaBoost is a popular boosting technique which helps you combine multiple “weak classifiers” into a single “strong classifier”. A weak classifier is simply a classifier that performs poorly, but performs better than random guessing. A simple example might be classifying a person as male or female based on their height. You could say anyone over 5’ 9” is a male and anyone under that is a female. You’ll misclassify a lot of people that way, but your accuracy will still be greater than 50%.\n",
    "\n",
    "AdaBoost can be applied to any classification algorithm, so it’s really a technique that builds on top of other classifiers as opposed to being a classifier itself.\n",
    "\n",
    "You could just train a bunch of weak classifiers on your own and combine the results, so what does AdaBoost do for you? There’s really two things it figures out for you:\n",
    "\n",
    "It helps you choose the training set for each new classifier that you train based on the results of the previous classifier.\n",
    "\n",
    "It determines how much weight should be given to each classifier’s proposed answer when combining the results.\n",
    "\n",
    "- Training Set Selection\n",
    "\n",
    "Each weak classifier should be trained on a random subset of the total training set. The subsets can overlap–it’s not the same as, for example, dividing the training set into ten portions. AdaBoost assigns a “weight” to each training example, which determines the probability that each example should appear in the training set. Examples with higher weights are more likely to be included in the training set, and vice versa. After training a classifier, AdaBoost increases the weight on the misclassified examples so that these examples will make up a larger part of the next classifiers training set, and hopefully the next classifier trained will perform better on them.\n",
    "\n",
    "- Classifier Output Weights\n",
    "\n",
    "After each classifier is trained, the classifier’s weight is calculated based on its accuracy. More accurate classifiers are given more weight. A classifier with 50% accuracy is given a weight of zero, and a classifier with less than 50% accuracy (kind of a funny concept) is given negative weight.\n",
    "\n",
    "The final classifier consists of ‘T’ weak classifiers. h_t(x) is the output of weak classifier ‘t’ (in this paper, the outputs are limited to -1 or +1). Alpha_t is the weight applied to classifier ‘t’ as determined by AdaBoost. So the final output is just a linear combination of all of the weak classifiers, and then we make our final decision simply by looking at the sign of this sum.\n",
    "\n",
    "The classifiers are trained one at a time. After each classifier is trained, we update the probabilities of each of the training examples appearing in the training set for the next classifier.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------- R\n",
    "https://cran.r-project.org/web/packages/fastAdaboost/fastAdaboost.pdf\n",
    "install.packages(“fastAdaboost”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------ Python\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=200, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
