{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "xxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Models Pros & Cons\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "#### Random Forest\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "\n",
    "#### Xgboost (eXtreme Gradient Boosted Trees) \n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: High performance, very fast, handle large data; works for generic loss functions; More customizable parameters; Resist to over-fitting; Can pick up more deeper feature interaction since it is dealing with gradients (Compare to adaboost which only weighting); \n",
    "\n",
    "Cons:\n",
    "\n",
    "\n",
    "#### Ada Boosting\n",
    "- Big O Notation (Cost Function):\n",
    "\n",
    "Pros: High Performance, fast, mainly for classification with exponential loss; Good for high dimensional data(automatically select features); Final prediction is linear combination so easy to debug; Resist to over-fitting;\n",
    "\n",
    "Cons: Less customizable than XGboost; Slightly slower than XGboost; Less performed on deeper feature interaction than XGboost; \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------- Xgboost (eXtreme Gradient Boosted Trees) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "XGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is a library designed and optimized for boosting trees algorithms. It uses Gradient Descent for updating parameters;\n",
    "\n",
    "XGBoost’s main goal is to push the extreme of the computation limits of machines to provide a scalable, portable and accurate for large scale tree boosting. The term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman[1]. The GBM, boosted trees, has been around for really a while, and there are a lot of materials on the topic.\n",
    "\n",
    "Boosting is an ensemble technique in which new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made.\n",
    "\n",
    "The ensemble technique uses the tree ensemble model which is a set of classification and regression trees (CART). The ensemble approach is used because a single CART, usually, does not have a strong predictive power. By using a set of CART (i.e. a tree ensemble model) a sum of the predictions of multiple trees is considered.\n",
    "\n",
    "Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction.\n",
    "\n",
    "The objective of the XGBoost model is given as:\n",
    "\n",
    "Obj = L + Ω\n",
    "\n",
    "Where, ",
    "L is the loss function which controls the predictive power, and ",
    "Ω is regularization component which controls simplicity and overfitting\n",
    "The loss function (L) which needs to be optimized can be Root Mean Squared Error for regression, Logloss for binary classification, or mlogloss for multi-class classification.\n",
    "\n",
    "The regularization component (Ω) is dependent on the number of leaves and the prediction score assigned to the leaves in the tree ensemble model.\n",
    "It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models. The Gradient boosting algorithm supports both regression and classification predictive modeling problems.\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "Leaner / number of boosts / leaner parameters \n",
    "#### Cost Function: \n",
    "MSE, Absolute Error \n",
    "\n",
    "#### Process Flow: \n",
    "http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "*Step1\n",
    "*Build model on residuals and make predication, combine predictions = final prediction\n",
    "*Not necessarily tree based model, you can “plug in” various classes of weak learners\n",
    "Fit a model to the data, F1(x) = y\n",
    "Fit a model to the residuals, h1(x) = y - F1(x)\n",
    "Create a new model, F2(x) = F1(x) + h1(x) …\n",
    "\n",
    "*Step2\n",
    "How many iteration of boost tree needed? –Since still generate single prediction, we can use cross-validation to determine M by evaluating Cost function (MSE).\n",
    "\n",
    "*Additional\n",
    "By each gradient descent step multiply a factor 0-1 called “learning rate”\n",
    "It also has ability to sample the data rows and columns (Like Random Forest)\n",
    "#### Evaluation Methods: \n",
    "Regression / classification performance metrics (With CV)\n",
    "#### Tips: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------- R\n",
    "http://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html\n",
    "install.packages(\"xgboost\")\n",
    "library(xgboost)\n",
    "data(agaricus.train, package='xgboost')\n",
    "data(agaricus.test, package='xgboost')\n",
    "train <- agaricus.train\n",
    "test <- agaricus.test\n",
    "bstSparse <- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = \"binary:logistic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------ Python\n",
    "http://xgboost.readthedocs.io/en/latest/python/python_intro.html\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix('train.svm.txt')\n",
    "dtest = xgb.DMatrix('test.svm.buffer')\n",
    "# Setting parameters\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "evallist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "# train model\n",
    "num_round = 10\n",
    "bst = xgb.train( plst, dtrain, num_round, evallist )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Ada Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "\n",
    "AdaBoost, short for \"Adaptive Boosting\", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the Gödel Prize in 2003 for their work. It can be used in conjunction with many other types of learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing (e.g., their error rate is smaller than 0.5 for binary classification), the final model can be proven to converge to a strong learner.\n",
    "Every learning algorithm will tend to suit some problem types better than others, and will typically have many different parameters and configurations to be adjusted before achieving optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier.[1][2] When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples.\n",
    "\n",
    "#### Input Data: \n",
    "\n",
    "#### Initial Parameters: \n",
    "Leaner / number of boosts / leaner parameters\n",
    "\n",
    "#### Cost Function: \n",
    "Weak leaner cost function (AUC)\n",
    "\n",
    "#### Process Flow: \n",
    "http://mccormickml.com/2013/12/13/adaboost-tutorial/\n",
    "\n",
    "AdaBoost is a popular boosting technique which helps you combine multiple “weak classifiers” into a single “strong classifier”. A weak classifier is simply a classifier that performs poorly, but performs better than random guessing. A simple example might be classifying a person as male or female based on their height. You could say anyone over 5’ 9” is a male and anyone under that is a female. You’ll misclassify a lot of people that way, but your accuracy will still be greater than 50%.\n",
    "\n",
    "AdaBoost can be applied to any classification algorithm, so it’s really a technique that builds on top of other classifiers as opposed to being a classifier itself.\n",
    "\n",
    "You could just train a bunch of weak classifiers on your own and combine the results, so what does AdaBoost do for you? There’s really two things it figures out for you:\n",
    "\n",
    "It helps you choose the training set for each new classifier that you train based on the results of the previous classifier.\n",
    "\n",
    "It determines how much weight should be given to each classifier’s proposed answer when combining the results.\n",
    "\n",
    "- Training Set Selection\n",
    "\n",
    "Each weak classifier should be trained on a random subset of the total training set. The subsets can overlap–it’s not the same as, for example, dividing the training set into ten portions. AdaBoost assigns a “weight” to each training example, which determines the probability that each example should appear in the training set. Examples with higher weights are more likely to be included in the training set, and vice versa. After training a classifier, AdaBoost increases the weight on the misclassified examples so that these examples will make up a larger part of the next classifiers training set, and hopefully the next classifier trained will perform better on them.\n",
    "\n",
    "- Classifier Output Weights\n",
    "\n",
    "After each classifier is trained, the classifier’s weight is calculated based on its accuracy. More accurate classifiers are given more weight. A classifier with 50% accuracy is given a weight of zero, and a classifier with less than 50% accuracy (kind of a funny concept) is given negative weight.\n",
    "\n",
    "The final classifier consists of ‘T’ weak classifiers. h_t(x) is the output of weak classifier ‘t’ (in this paper, the outputs are limited to -1 or +1). Alpha_t is the weight applied to classifier ‘t’ as determined by AdaBoost. So the final output is just a linear combination of all of the weak classifiers, and then we make our final decision simply by looking at the sign of this sum.\n",
    "\n",
    "The classifiers are trained one at a time. After each classifier is trained, we update the probabilities of each of the training examples appearing in the training set for the next classifier.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------- R\n",
    "https://cran.r-project.org/web/packages/fastAdaboost/fastAdaboost.pdf\n",
    "install.packages(“fastAdaboost”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------ Python\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=200, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
