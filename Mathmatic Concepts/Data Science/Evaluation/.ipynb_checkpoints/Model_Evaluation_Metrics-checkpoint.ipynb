{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple stages in developing a machine learning model for use in a software application. It follows that there are multiple places where one needs to evaluate the model. Roughly speaking, the first phase involves prototyping, where we try out different models to find the best one (model selection). Once we are satisfied with a pro‐totype model, we deploy it into production, where it will go through further testing on live data.1 Figure 1-1 illustrates this workflow.\n",
    "\n",
    "Why is it so complicated? Two reasons. First of all, note that online and offline evaluations may measure very different metrics. Offline evaluation might use one of the metrics like accuracy or precision-recall, which we discuss in Chapter 2. Furthermore, training and validation might even use different metrics, but that’s an even finer point (see the note in Chapter 2). Online evaluation, on the other hand, might measure business metrics such as customer lifetime value, which may not be available on historical data but are closer to what your business really cares about (more about picking the right metric for online evaluation in Chapter 5).\n",
    "\n",
    "Secondly, note that there are two sources of data: historical and live. Many statistical models assume that the distribution of data stays the same over time. (The technical term is that the distribution is stationary.) But in practice, the distribution of data changes over time, sometimes drastically. This is called distribution drift. As an example, think about building a recommender for news articles. The trending topics change every day, sometimes every hour; what was popular yesterday may no longer be relevant today. One can imagine the distribution of user preference for news articles changing rapidly over time. Hence it’s important to be able to detect distribution drift and adapt the model accordingly.\n",
    "\n",
    "One way to detect distribution drift is to continue to track the model’s performance on the validation metric on live data. If the performance is comparable to the validation results when the model was built, then the model still fits the data. When performance starts to degrade, then it’s probable that the distribution of live data has drifted sufficiently from historical data, and it’s time to retrain the model. Monitoring for distribution drift is often done “offline” from the production environment. Hence we are grouping it into offline evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Online Evaluation Mechanisms:\n",
    "\n",
    "Once a satisfactory model is found during the prototyping phase, it can be deployed to production, where it will interact with real users and live data. The online phase has its own testing procedure. The most commonly used form of online testing is A/B testing, which is based on statistical hypothesis testing. The basic concepts may be well known, but there are many pitfalls and challenges in doing it correctly. Chapter 5 goes into a checklist of questions to ask when running an A/B test, so as to avoid some of the pernicious pitfalls. A less well-known form of online model selection is an algorithm called multiarmed bandits. We’ll take a look at what it is and why it might be a better alternative to A/B tests in some situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Offline Evaluation Mechanisms:\n",
    "\n",
    "As alluded to earlier, the main task during the prototyping phase is to select the right model to fit the data. The model must be evaluated on a dataset that’s statistically independent from the one it was trained on. Why? Because its performance on the training set is an overly optimistic estimate of its true performance on new data. The process of training the model has already adapted to the training data. A more fair evaluation would measure the model’s performance on data that it hasn’t yet seen. In statistical terms, this gives an estimate of the generalization error, which measures how well the model generalizes to new data. So where does one obtain new data? Most of the time, we have just the one dataset we started out with. The statistician’s solution to this problem is to chop it up or resample it and pretend that we have new data.\n",
    "\n",
    "One way to generate new data is to hold out part of the training set and use it only for evaluation. This is known as hold-out validation. The more general method is known as k-fold cross-validation. There are other, lesser known variants, such as bootstrapping or jackknife resampling. These are all different ways of chopping up or resampling one dataset to simulate new data. Chapter 3 covers offline evaluation and model selection.\n",
    "\n",
    "#### [1] Hold-out Set Validation\n",
    "\n",
    "\n",
    "#### [2] K-fold Cross Validation\n",
    "\n",
    "\n",
    "#### [3] Leave-one-out Cross Validation\n",
    "\n",
    "\n",
    "#### [4] Boostraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Model Evaluation Metrics:\n",
    "\n",
    "focuses on evaluation metrics. Different machine learning tasks have different performance metrics. If I build a classifier to detect spam emails versus normal emails, then I can use classification performance metrics such as average accuracy, log-loss, and area under the curve (AUC). If I’m trying to predict a numeric score, such as Apple’s daily stock price, then I might consider the root-mean-square error (RMSE). If I am ranking items by relevance to a query submitted to a search engine, then there are ranking losses such as precision-recall (also popular as a classification metric) or normalized discounted cumulative gain (NDCG). These are examples of performance metrics for various tasks.\n",
    "\n",
    "### -->> Regression Problems:\n",
    "\n",
    "#### [1] RMSE\n",
    "\n",
    "\n",
    "#### [2] Quantiles of Errors\n",
    "\n",
    "\n",
    "#### [3] “Almost Correct” Predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### -->> Classification Problems:\n",
    "\n",
    "\n",
    "#### [0] Accuracy\n",
    "\n",
    "\n",
    "#### [0-1] Per-Class Accuracy\n",
    "\n",
    "\n",
    "#### [1] Confusion Matrix\n",
    "\n",
    "\n",
    "#### [2] Gain / Lift Chart\n",
    "\n",
    "\n",
    "#### [3] Kolomogorov Smirnov Chart\n",
    "\n",
    "\n",
    "#### [4] Area Under the ROC curve (AUC – ROC)\n",
    "\n",
    "\n",
    "#### [5] Kappa Statistics\n",
    "\n",
    "\n",
    "#### [6] F1 Score (also F-score or F-measure)\n",
    "\n",
    "\n",
    "#### [7] Log-Loss\n",
    "\n",
    "\n",
    "\n",
    "### -->> Ranking Problems:\n",
    "\n",
    "\n",
    "#### [1] Precision-Recall\n",
    "\n",
    "\n",
    "#### [2] Precision-Recall Curve and the F1 Score\n",
    "\n",
    "\n",
    "#### [3] NDCG\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Hyperparameter Tuning:\n",
    "\n",
    "You may have heard of terms like hyperparameter search, autotuning (which is just a shorter way of saying hyperparameter search), or grid search (a possible method for hyperparameter search). Where do those terms fit in? To understand hyperparameter search, we have to talk about the difference between a model parameter and a hyperparameter. In brief, model parameters are the knobs that the training algorithm knows how to tweak; they are learned from data. Hyperparameters, on the other hand, are not learned by the training method, but they also need to be tuned. \n",
    "\n",
    "To make this more concrete, say we are building a linear classifier to differentiate between spam and nonspam emails. This means that we are looking for a line in feature space that separates spam from nonspam. The training process determines where that line lies, but it won’t tell us how many features (or words) to use to represent the emails. The line is the model parameter, and the number of features is the hyperparameter. Hyperparameters can get complicated quickly. Much of the prototyping phase involves iterating between trying out different models, hyperparameters, and features. Searching for the optimal hyperparameter can be a laborious task. This is where search algorithms such as grid search, random search, or smart search come in. These are all search methods that look through hyperparameter space and find good configurations. Hyperparameter tuning is covered in detail in Chapter 4.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> The Pitfalls of A/B Testing:\n",
    "\n",
    "xxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Evaluation Mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Evaluation Mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pitfalls of A/B Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >>>> Regression Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### [1] Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:\n",
    "\n",
    "- The power of ‘square root’  empowers this metric to show large number deviations.\n",
    "- The ‘squared’ nature of this metric helps to deliver more robust results which prevents cancelling the positive and negative error values. In other words, this metric aptly displays the plausible magnitude of error term.\n",
    "- It avoids the use of absolute error values which is highly undesirable in mathematical calculations.\n",
    "- When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable.\n",
    "- RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric.\n",
    "- As compared to mean absolute error, RMSE gives higher weightage and punishes large errors.\n",
    "\n",
    "RMSE metric is given by:\n",
    "\n",
    "##### SQRT(  (prediction - original)^2 / N  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------- Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### >>>> Classification Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### [1] Confusion Matrix\n",
    "\n",
    "A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. In general we are concerned with one of the above defined metric. For instance, in a pharmaceutical company, they will be more concerned with minimal wrong positive diagnosis. Hence, they will be more concerned about high Specificity. On the other hand an attrition model will be more concerned with Senstivity.Confusion matrix are generally used only with class output models.\n",
    "\n",
    "- [Accuracy] : the proportion of the total number of predictions that were correct.\n",
    "\n",
    "- [Positive Predictive Value or Precision] : the proportion of positive cases that were correctly identified.\n",
    "- [Negative Predictive Value] : the proportion of negative cases that were correctly identified.\n",
    "- [Sensitivity or Recall] : the proportion of actual positive cases which are correctly identified.\n",
    "- [Specificity] : the proportion of actual negative cases which are correctly identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] Gain / Lift Chart\n",
    "\n",
    "http://www.listendata.com/2014/08/excel-template-gain-and-lift-charts.html\n",
    "\n",
    "http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html\n",
    "\n",
    "Gain or lift is a measure of the effectiveness of a classification model calculated as the ratio between the results obtained with and without the model. Gain and lift charts are visual aids for evaluating performance of classification models. However, in contrast to the confusion matrix that evaluates models on the whole population gain or lift chart evaluates model performance in a portion of the population. Expect most target captured in the first a few bins.\n",
    "\n",
    "- Use model to calculate probability for each observation\n",
    "- Sort the observations by probability desc\n",
    "- Bin all observations into 10 bins - 10% each\n",
    "- Base line (Random model) - 10% obeservations = 10% target\n",
    "- Model line - 10% obeservations = (calculate ?% target) in that bin\n",
    "\n",
    "Gain chart: Plot \"Base line\" and \"Model line\" with % bin\n",
    "Lift chart: Plot \"Base ratio = 1\" and \"Model ratio = model line / Base line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3] Kolomogorov Smirnov Chart\n",
    "\n",
    "http://www.saedsayad.com/model_evaluation_c.htm\n",
    "\n",
    "K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.\n",
    "\n",
    "On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.\n",
    "\n",
    "- Calculate probability for each observation\n",
    "- Sort the observations by probability and bin them by probability - 100% - 90%, 89% - 80%, etc.\n",
    "- Calculate Cumulative % proportion of target and non-target cases in each bin\n",
    "\n",
    "K-S Chart: Plot the cumulative proportion of target / non-target lines with probability bins\n",
    "\n",
    "K-S statistics: the highest difference between the proportions of target and non-target lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4] Area Under the ROC curve (AUC – ROC)\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/\n",
    "\n",
    "*ROC (Receiver operating characteristic) curve\n",
    "\n",
    "This is again one of the popular metrics used in the industry.  The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. This statement will get clearer in the following sections. The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate.\n",
    "\n",
    "Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. \n",
    "\n",
    "AUC Standard:\n",
    "- .90-1 = excellent (A)\n",
    "- .80-.90 = good (B)\n",
    "- .70-.80 = fair (C)\n",
    "- .60-.70 = poor (D)\n",
    "- .50-.60 = fail (F)\n",
    "\n",
    "Points to remember:\n",
    "\n",
    "1. For a model which gives class as output, will be represented as a single point in ROC plot.\n",
    "\n",
    "2. Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared.\n",
    "\n",
    "3. In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other.\n",
    "\n",
    "\n",
    "Why should you use ROC and not metrics like lift curve?\n",
    "\n",
    "- Lift is dependent on total response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.\n",
    "\n",
    "- ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5] Kappa Statistics\n",
    "\n",
    "- Usually used for un-balance data\n",
    "\n",
    "Calculation:\n",
    "\n",
    "--T------F\n",
    "  \n",
    "T a(20)  b(5)\n",
    "\n",
    "F c(10)  d(15)\n",
    "\n",
    "- The probability of random agreement(T)\n",
    "\n",
    "-----P(T) = (a+b) / (a+b+c+d) * (a+c) / (a+b+c+d) = 0.3\n",
    "\n",
    "- The probability of random agreement(F)\n",
    "\n",
    "-----P(F) = (c+d) / (a+b+c+d) * (b+d) / (a+b+c+d) = 0.2\n",
    "\n",
    "- The Over-all probability of random agreement\n",
    "\n",
    "-----P(ALL) = P(T) + P(F) = 0.5\n",
    "\n",
    "- The observed proportinate agreement\n",
    "\n",
    "-----P(o) = (a+d) / (a+b+c+d) = 0.7\n",
    "\n",
    "- Kappa Statistics\n",
    "\n",
    "-----K = ( P(o) - P(ALL) ) / ( 1 - P(ALL) )\n",
    "\n",
    "\n",
    "##### Measurement\n",
    "\n",
    "- Perfect agreement κ=1.\n",
    "- κ=0, does not mean perfect disagreement; it only means agreement by chance as that would indicate that the diagonal cell probabilities are simply product of the corresponding marginals.\n",
    "- If agreement is greater than agreement by chance, then κ≥0.\n",
    "- If agreement is less than agreement obtained by chance, then κ≤0.\n",
    "- The minimum possible value of κ=−1.\n",
    "- A value of kappa higher than 0.75 will indicate excellent agreement while lower than 0.4 will indicate poor agreement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Pros\n",
    "\n",
    "- Kappa statistics are easily calculated and software is readily available (e.g., SAS PROC FREQ).\n",
    "- Kappa statistics are appropriate for testing whether agreement exceeds chance levels for binary and nominal ratings.\n",
    "\n",
    "##### Cons\n",
    "\n",
    "- Kappa is not really a chance-corrected measure of agreement (see above).\n",
    "- Kappa is an omnibus index of agreement. It does not make distinctions among various types and sources of disagreement.\n",
    "- Kappa is influenced by trait prevalence (distribution) and base-rates. As a result, kappas are seldom comparable across studies, procedures, or populations (Thompson & Walter, 1988; Feinstein & Cicchetti, 1990).\n",
    "- Kappa may be low even though there are high levels of agreement and even though individual ratings are accurate. Whether a given kappa value implies a good or a bad rating system or diagnostic method depends on what model one assumes about the decisionmaking of raters (Uebersax, 1988).\n",
    "- With ordered category data, one must select weights arbitrarily to calculate weighted kappa (Maclure & Willet, 1987).\n",
    "- Kappa requires that two rater/procedures use the same rating categories. There are situations where one is interested in measuring the consistency of ratings for raters that use different categories (e.g., one uses a scale of 1 to 3, another uses a scale of 1 to 5).\n",
    "- Tables that purport to categorize ranges of kappa as \"good,\" \"fair,\" \"poor\" etc. are inappropriate; do not use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6] F1 Score (also F-score or F-measure)\n",
    "\n",
    "In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results, and r is the number of correct positive results divided by the number of positive results that should have been returned. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "\n",
    "Calculation:\n",
    "\n",
    "--T------F\n",
    "  \n",
    "T a(20)  b(5)\n",
    "\n",
    "F c(10)  d(15)\n",
    "\n",
    "precision = a / (a+b)\n",
    "\n",
    "Recall = a / (a+c)\n",
    "\n",
    "F-Score = 2 * {precision * Recall} * {precision + Recall} --- 0<->1\n",
    "\n",
    "Measurement:\n",
    "\n",
    "- More than 0.7 (Good)\n",
    "- Less than 0.3 (Bad)\n",
    "\n",
    "Keep in mind:\n",
    "\n",
    "- The F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance.[3] Earlier works focused primarily on the F1 score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall[4] and so {\\displaystyle F_{\\beta }} F_{\\beta } is seen in wide application.\n",
    "\n",
    "- The F-score is also used in machine learning.[5] Note, however, that the F-measures do not take the true negatives into account, and that measures such as the Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.[2]\n",
    "\n",
    "- The F-score has been widely used in the natural language processing literature, such as the evaluation of named entity recognition and word segmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >>>> Ranking Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
