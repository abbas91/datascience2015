{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Validation Resampling Process:\n",
    "\n",
    "#### [1] Hold-out Set Validation\n",
    "\n",
    "\n",
    "#### [2] K-fold Cross Validation\n",
    "\n",
    "\n",
    "#### [3] Leave-one-out Cross Validation\n",
    "\n",
    "\n",
    "#### [4] Boostraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>> Model Performance Metrics:\n",
    "\n",
    "\n",
    "### -->> Regression Problems:\n",
    "\n",
    "#### [1] RMSE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### -->> Classification Problems:\n",
    "\n",
    "#### [1] Confusion Matrix\n",
    "\n",
    "\n",
    "#### [2] Gain / Lift Chart\n",
    "\n",
    "\n",
    "#### [3] Kolomogorov Smirnov Chart\n",
    "\n",
    "\n",
    "#### [4] Area Under the ROC curve (AUC – ROC)\n",
    "\n",
    "\n",
    "#### [5] Kappa Statistics\n",
    "\n",
    "\n",
    "#### [6] F1 Score (also F-score or F-measure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validation Resampling Process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### [1] Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:\n",
    "\n",
    "- The power of ‘square root’  empowers this metric to show large number deviations.\n",
    "- The ‘squared’ nature of this metric helps to deliver more robust results which prevents cancelling the positive and negative error values. In other words, this metric aptly displays the plausible magnitude of error term.\n",
    "- It avoids the use of absolute error values which is highly undesirable in mathematical calculations.\n",
    "- When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable.\n",
    "- RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric.\n",
    "- As compared to mean absolute error, RMSE gives higher weightage and punishes large errors.\n",
    "\n",
    "RMSE metric is given by:\n",
    "\n",
    "##### SQRT(  (prediction - original)^2 / N  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------- Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Performance Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### [1] Confusion Matrix\n",
    "\n",
    "A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. In general we are concerned with one of the above defined metric. For instance, in a pharmaceutical company, they will be more concerned with minimal wrong positive diagnosis. Hence, they will be more concerned about high Specificity. On the other hand an attrition model will be more concerned with Senstivity.Confusion matrix are generally used only with class output models.\n",
    "\n",
    "- [Accuracy] : the proportion of the total number of predictions that were correct.\n",
    "\n",
    "- [Positive Predictive Value or Precision] : the proportion of positive cases that were correctly identified.\n",
    "- [Negative Predictive Value] : the proportion of negative cases that were correctly identified.\n",
    "- [Sensitivity or Recall] : the proportion of actual positive cases which are correctly identified.\n",
    "- [Specificity] : the proportion of actual negative cases which are correctly identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] Gain / Lift Chart\n",
    "\n",
    "http://www.listendata.com/2014/08/excel-template-gain-and-lift-charts.html\n",
    "\n",
    "http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html\n",
    "\n",
    "Gain or lift is a measure of the effectiveness of a classification model calculated as the ratio between the results obtained with and without the model. Gain and lift charts are visual aids for evaluating performance of classification models. However, in contrast to the confusion matrix that evaluates models on the whole population gain or lift chart evaluates model performance in a portion of the population. Expect most target captured in the first a few bins.\n",
    "\n",
    "- Use model to calculate probability for each observation\n",
    "- Sort the observations by probability desc\n",
    "- Bin all observations into 10 bins - 10% each\n",
    "- Base line (Random model) - 10% obeservations = 10% target\n",
    "- Model line - 10% obeservations = (calculate ?% target) in that bin\n",
    "\n",
    "Gain chart: Plot \"Base line\" and \"Model line\" with % bin\n",
    "Lift chart: Plot \"Base ratio = 1\" and \"Model ratio = model line / Base line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3] Kolomogorov Smirnov Chart\n",
    "\n",
    "http://www.saedsayad.com/model_evaluation_c.htm\n",
    "\n",
    "K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.\n",
    "\n",
    "On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.\n",
    "\n",
    "- Calculate probability for each observation\n",
    "- Sort the observations by probability and bin them by probability - 100% - 90%, 89% - 80%, etc.\n",
    "- Calculate Cumulative % proportion of target and non-target cases in each bin\n",
    "\n",
    "K-S Chart: Plot the cumulative proportion of target / non-target lines with probability bins\n",
    "\n",
    "K-S statistics: the highest difference between the proportions of target and non-target lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4] Area Under the ROC curve (AUC – ROC)\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/\n",
    "\n",
    "*ROC (Receiver operating characteristic) curve\n",
    "\n",
    "This is again one of the popular metrics used in the industry.  The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. This statement will get clearer in the following sections. The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate.\n",
    "\n",
    "Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. \n",
    "\n",
    "AUC Standard:\n",
    "- .90-1 = excellent (A)\n",
    "- .80-.90 = good (B)\n",
    "- .70-.80 = fair (C)\n",
    "- .60-.70 = poor (D)\n",
    "- .50-.60 = fail (F)\n",
    "\n",
    "Points to remember:\n",
    "\n",
    "1. For a model which gives class as output, will be represented as a single point in ROC plot.\n",
    "\n",
    "2. Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared.\n",
    "\n",
    "3. In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other.\n",
    "\n",
    "\n",
    "Why should you use ROC and not metrics like lift curve?\n",
    "\n",
    "- Lift is dependent on total response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.\n",
    "\n",
    "- ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5] Kappa Statistics\n",
    "\n",
    "- Usually used for un-balance data\n",
    "\n",
    "Calculation:\n",
    "\n",
    "--T------F\n",
    "  \n",
    "T a(20)  b(5)\n",
    "\n",
    "F c(10)  d(15)\n",
    "\n",
    "- The probability of random agreement(T)\n",
    "\n",
    "-----P(T) = (a+b) / (a+b+c+d) * (a+c) / (a+b+c+d) = 0.3\n",
    "\n",
    "- The probability of random agreement(F)\n",
    "\n",
    "-----P(F) = (c+d) / (a+b+c+d) * (b+d) / (a+b+c+d) = 0.2\n",
    "\n",
    "- The Over-all probability of random agreement\n",
    "\n",
    "-----P(ALL) = P(T) + P(F) = 0.5\n",
    "\n",
    "- The observed proportinate agreement\n",
    "\n",
    "-----P(o) = (a+d) / (a+b+c+d) = 0.7\n",
    "\n",
    "- Kappa Statistics\n",
    "\n",
    "-----K = ( P(o) - P(ALL) ) / ( 1 - P(ALL) )\n",
    "\n",
    "\n",
    "##### Measurement\n",
    "\n",
    "- Perfect agreement κ=1.\n",
    "- κ=0, does not mean perfect disagreement; it only means agreement by chance as that would indicate that the diagonal cell probabilities are simply product of the corresponding marginals.\n",
    "- If agreement is greater than agreement by chance, then κ≥0.\n",
    "- If agreement is less than agreement obtained by chance, then κ≤0.\n",
    "- The minimum possible value of κ=−1.\n",
    "- A value of kappa higher than 0.75 will indicate excellent agreement while lower than 0.4 will indicate poor agreement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Pros\n",
    "\n",
    "- Kappa statistics are easily calculated and software is readily available (e.g., SAS PROC FREQ).\n",
    "- Kappa statistics are appropriate for testing whether agreement exceeds chance levels for binary and nominal ratings.\n",
    "\n",
    "##### Cons\n",
    "\n",
    "- Kappa is not really a chance-corrected measure of agreement (see above).\n",
    "- Kappa is an omnibus index of agreement. It does not make distinctions among various types and sources of disagreement.\n",
    "- Kappa is influenced by trait prevalence (distribution) and base-rates. As a result, kappas are seldom comparable across studies, procedures, or populations (Thompson & Walter, 1988; Feinstein & Cicchetti, 1990).\n",
    "- Kappa may be low even though there are high levels of agreement and even though individual ratings are accurate. Whether a given kappa value implies a good or a bad rating system or diagnostic method depends on what model one assumes about the decisionmaking of raters (Uebersax, 1988).\n",
    "- With ordered category data, one must select weights arbitrarily to calculate weighted kappa (Maclure & Willet, 1987).\n",
    "- Kappa requires that two rater/procedures use the same rating categories. There are situations where one is interested in measuring the consistency of ratings for raters that use different categories (e.g., one uses a scale of 1 to 3, another uses a scale of 1 to 5).\n",
    "- Tables that purport to categorize ranges of kappa as \"good,\" \"fair,\" \"poor\" etc. are inappropriate; do not use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6] F1 Score (also F-score or F-measure)\n",
    "\n",
    "In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results, and r is the number of correct positive results divided by the number of positive results that should have been returned. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "\n",
    "Calculation:\n",
    "\n",
    "--T------F\n",
    "  \n",
    "T a(20)  b(5)\n",
    "\n",
    "F c(10)  d(15)\n",
    "\n",
    "precision = a / (a+b)\n",
    "\n",
    "Recall = a / (a+c)\n",
    "\n",
    "F-Score = 2 * {precision * Recall} * {precision + Recall} --- 0<->1\n",
    "\n",
    "Measurement:\n",
    "\n",
    "- More than 0.7 (Good)\n",
    "- Less than 0.3 (Bad)\n",
    "\n",
    "Keep in mind:\n",
    "\n",
    "- The F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance.[3] Earlier works focused primarily on the F1 score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall[4] and so {\\displaystyle F_{\\beta }} F_{\\beta } is seen in wide application.\n",
    "\n",
    "- The F-score is also used in machine learning.[5] Note, however, that the F-measures do not take the true negatives into account, and that measures such as the Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.[2]\n",
    "\n",
    "- The F-score has been widely used in the natural language processing literature, such as the evaluation of named entity recognition and word segmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------- Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
