{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In machine learning, instance-based learning (sometimes called memory-based learning[1]) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.\n",
    "\n",
    "It is called instance-based because it constructs hypotheses directly from the training instances themselves.[2] This means that the hypothesis complexity can grow with the data:[2] in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. Instance-based learners may simply store a new instance or throw an old instance away.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### K-Nearest Neighbour (KNN)\n",
    "\n",
    "Pros: Simple to implement; Flexible to feature / distance choices; Naturally handles multi-class cases; Can do well in practice with enough representative data; Robust to outlier; No assumptions;\n",
    "\n",
    "Cons: Computation intensive; Storage of data; must know meaningful distance; Sensitive to local patterns; Totally based on your training on your entire train data; Unstable in high dimensional data;\n",
    "\n",
    "#### Learning Vector Quantization (LVQ)\n",
    "\n",
    "Pros: Support binary and multiple class predict; The algorithm complexity can be adjusted during training as needed (# of nodes); Non-parametric model more accurate; Supervised Learning;\n",
    "\n",
    "Cons: Choose meaningful distance metrics; Computational intensive; \n",
    "\n",
    "#### Self-Organizing Map (SOM) \n",
    "\n",
    "Pros: Unsupervised learning; Robust to missing/partial data; Good for visualize high-dimensional data; Dimension reduction method; Good to use as process data before clustering (Preserve structure or topology of input data); Easy to understand (close = similar)\n",
    "\n",
    "Cons: Very computationally expensive (More dimensions, more K = more slow); Contradict when organize data while preserve structure – may blur important relationship in data; Need right data which you need a value for each dimension of each member of samples in order to generate a map; every SOM is different and finds different similarites among the sample vectors so a lot of maps need to be constructed in order to get one final good map;\n",
    "\n",
    "#### Locally Weighted Learning (LWL)\n",
    "\n",
    "Pros: Highly accurate prediction (When no good para for global function approximation); Less computation than global model; Quickly adaptive to new data; Usually used in robotic application or online real-time system; LWPR - Good for high-dimensional data; \n",
    "\n",
    "Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- K-Nearest Neighbour (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "k nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups.\n",
    "In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n",
    "In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n",
    "Both for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.[2]\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "\n",
    "#### Initial Parameters:\n",
    "K – nearest neighbors (Trade-off between linear or fit)\n",
    "#### Cost Function:\n",
    "Calculate Euclidean Distance\n",
    "#### Process Flow:\n",
    "The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n",
    "In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n",
    "A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has also been employed with correlation coefficients such as Pearson and Spearman.[3] Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.\n",
    "A drawback of the basic \"majority voting\" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number.[4] One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM.\n",
    "#### Tips:\n",
    "Choosing the number of nearest neighbors i.e. determining the value of k plays a significant role in determining the efficacy of the model. Thus, selection of k will determine how well the data can be utilized to generalize the results of the kNN algorithm. A large k value has benefits which include reducing the variance due to the noisy data; the side effect being developing a bias due to which the learner tends to ignore the smaller patterns which may have useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R\n",
    "\n",
    "# Classification\n",
    "# https://www.analyticsvidhya.com/blog/2015/08/learning-concept-knn-algorithms-programming/\n",
    "prc <- read.csv(\"Prostate_Cancer.csv\",stringsAsFactors = FALSE)    #This command imports the required data \n",
    "prc <- prc[-1]  #removes the first variable(id) from the data set. set and saves it to the prc data frame.\n",
    "prc$diagnosis <- factor(prc$diagnosis_result, levels = c(\"B\", \"M\"), labels = c(\"Benign\", \"Malignant\"))\n",
    "round(prop.table(table(prc$diagnosis)) * 100, digits = 1)  # it gives the result in the percentage form rounded of to 1 decimal place( and so it’s digits = 1)\n",
    "normalize <- function(x) {\n",
    "return ((x - min(x)) / (max(x) - min(x))) }\n",
    "prc_n <- as.data.frame(lapply(prc[2:9], normalize))\n",
    "prc_train <- prc_n[1:65,]\n",
    "prc_test <- prc_n[66:100,]\n",
    "prc_train_labels <- prc[1:65, 1]\n",
    "prc_test_labels <- prc[66:100, 1]   #This code takes the diagnosis factor in column 1 of the prc data frame and on turn creates prc_train_labels and prc_test_labels data frame.\n",
    "install.packages(“class”)\n",
    "library(class)\n",
    "prc_test_pred <- knn(train = prc_train, test = prc_test,cl = prc_train_labels, k=10)\n",
    "install.packages(\"gmodels\")\n",
    "library(gmodel)\n",
    "CrossTable(x=prc_test_labels, y=prc_test_pred, prop.chisq=FALSE)\n",
    "\n",
    "# Regression\n",
    "# https://artax.karlin.mff.cuni.cz/r-help/library/FNN/html/knn.reg.html\n",
    "require(chemometrics)\n",
    "data(PAC);\n",
    "pac.knn<- knn.reg(PAC$X, y=PAC$y, k=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Pythom\n",
    "\n",
    "# Classification\n",
    "# load lib {numpy, sklearn} \n",
    "from sklearn import datasets \n",
    "import numpy as np \n",
    "# loading datasets \n",
    "isir = datasets.load_iris() \n",
    "X = iris.data[:,[2,3]] \n",
    "y = iris.target \n",
    "# Spliting datasets \n",
    "from sklearn.cross_validation import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n",
    "# Scaling data \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() # - define scaler object \n",
    "sc.fit(X_train) # fit the object with data to get meansure \n",
    "X_train_std = sc.transform(X_train) # scale data \n",
    "X_test_std = sc.transform(X_test) # scale data \n",
    "# KNN Classifier \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski') # p = 1 {manhatten Dist} : p = 2 {Euclidean}  \n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "# Regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "for i, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n",
    "    y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "    plt.subplot(2, 1, i + 1)\n",
    "    plt.scatter(X, y, c='k', label='data')\n",
    "    plt.plot(T, y_, c='g', label='prediction')\n",
    "    plt.axis('tight')\n",
    "    plt.legend()\n",
    "    plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors,\n",
    "                                                                weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Learning Vector Quantization (LVQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "https://www.researchgate.net/publication/259486415_A_review_of_learning_vector_quantization_classifiers\n",
    "http://www.cs.rug.nl/~biehl/Preprints/wsom07lvq.pdf\n",
    "\n",
    "In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.\n",
    "#### Input Data:\n",
    "X(Numeric)\n",
    "\n",
    "#### Initial Parameters:\n",
    "Initial weights; Number of output nodes; K(default = 1, competitive learning); Learning rate(Update steps); Distance Metrics;\n",
    "#### Cost Function:\n",
    "D(X,J) – Only single X *Competitive, winner-takes-all*\n",
    "#### Process Flow:\n",
    "http://ccy.dd.ncu.edu.tw/~chen/course/neural/ch4/index.htm\n",
    "First initialize the M weights (initial M feature values) for J output nodes (Randomly assign J output classes to J nodes) -> For each (Like KNN but use K=1 so evaluate each) training example X with M feature values (Total X * n) -> Find the Jth node that minimize the D(X,J) *Usually Euclidean distance* -> Update the M weights for that Jth node by IF[True X class == Jth node class, update the M weights to move the Jth node close to X] ELSE [True X class != Jth node class, update the M weights to move the Jth node away from X] given the update scale based on “learning rate”. -> Repeat the process literately throughout the training set repeatly-> Stop if stopping criteria meets or max repeat time meets  \n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R\n",
    "\n",
    "# Classification\n",
    "# Find LVQ1,2,3 in https://cran.r-project.org/web/packages/class/class.pdf\n",
    "# http://astrostatistics.psu.edu/su07/R/html/class/html/lvq3.html\n",
    "# LVQ3\n",
    "data(iris3)\n",
    "train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])\n",
    "test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])\n",
    "cl <- factor(c(rep(\"s\",25), rep(\"c\",25), rep(\"v\",25)))\n",
    "cd <- lvqinit(train, cl, 10)\n",
    "lvqtest(cd, train)\n",
    "cd0 <- olvq1(train, cl, cd)\n",
    "lvqtest(cd0, train)\n",
    "cd3 <- lvq3(train, cl, cd0)\n",
    "lvqtest(cd3, train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Python\n",
    "\n",
    "# http://mnemstudio.org/ai/nn/lvq_python_ex1.txt\n",
    "# Another – https://pythonhosted.org/neurolab/ex_newlvq.html\n",
    "\"\"\"\n",
    "Example of use LVQ network\n",
    "==========================\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import neurolab as nl\n",
    "\n",
    "# Create train samples\n",
    "input = np.array([[-3, 0], [-2, 1], [-2, -1], [0, 2], [0, 1], [0, -1], [0, -2], \n",
    "                                                        [2, 1], [2, -1], [3, 0]])\n",
    "target = np.array([[1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], \n",
    "                                                        [1, 0], [1, 0], [1, 0]])\n",
    "\n",
    "# Create network with 2 layers:4 neurons in input layer(Competitive)\n",
    "# and 2 neurons in output layer(liner)\n",
    "net = nl.net.newlvq(nl.tool.minmax(input), 4, [.6, .4])\n",
    "# Train network\n",
    "error = net.train(input, target, epochs=1000, goal=-1)\n",
    "\n",
    "# Plot result\n",
    "import pylab as pl\n",
    "xx, yy = np.meshgrid(np.arange(-3, 3.4, 0.2), np.arange(-3, 3.4, 0.2))\n",
    "xx.shape = xx.size, 1\n",
    "yy.shape = yy.size, 1\n",
    "i = np.concatenate((xx, yy), axis=1)\n",
    "o = net.sim(i)\n",
    "grid1 = i[o[:, 0]>0]\n",
    "grid2 = i[o[:, 1]>0]\n",
    "\n",
    "class1 = input[target[:, 0]>0]\n",
    "class2 = input[target[:, 1]>0]\n",
    "\n",
    "pl.plot(class1[:,0], class1[:,1], 'bo', class2[:,0], class2[:,1], 'go')\n",
    "pl.plot(grid1[:,0], grid1[:,1], 'b.', grid2[:,0], grid2[:,1], 'gx')\n",
    "pl.axis([-3.2, 3.2, -3, 3])\n",
    "pl.xlabel('Input[:, 0]')\n",
    "pl.ylabel('Input[:, 1]')\n",
    "pl.legend(['class 1', 'class 2', 'detected class 1', 'detected class 2'])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Self-Organizing Map (SOM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "http://www.cs.bham.ac.uk/~jxb/NN/l16.pdf\n",
    "The principal goal of an SOM is to transform an incoming signal pattern of arbitrary\n",
    "dimension into a one or two dimensional discrete map, and to perform this transformation\n",
    "adaptively in a topologically ordered fashion. A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space. \n",
    "This makes SOMs useful for visualizing low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network.[1][2] The Kohonen net is a computationally convenient abstraction building on work on biologically neural models from the 1970s[3] and morphogenesis models dating back to Alan Turing in the 1950s.[4]\n",
    "Like most artificial neural networks, SOMs operate in two modes: training and mapping. \"Training\" builds the map using input examples (a competitive process, also called vector quantization), while \"mapping\" automatically classifies a new input vector.\n",
    "A self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. The usual arrangement of nodes is a two-dimensional regular spacing in a hexagonal or rectangular grid. The self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space. The procedure for placing a vector from data space onto the map is to find the node with the closest (smallest distance metric) weight vector to the data space vector.\n",
    "#### Input Data:\n",
    "X(Numeric)\n",
    "\n",
    "#### Initial Parameters:\n",
    "Initial weights; Number of output nodes; K(default = 1, competitive learning); Learning rate(Update steps); Distance Metrics;\n",
    "#### Cost Function:\n",
    "D(X,J) – Only single X *Competitive, winner-takes-all*\n",
    "#### Process Flow:\n",
    "Process: Continuous high dimensional input space map to a corresponding discrete low dimensional output space (2 dimensional map)\n",
    "1.Randomize the map's nodes' weight vectors\n",
    "2.Grab an input vector D ( t )   \n",
    "3.Traverse each node in the map 1.Use the Euclidean distance formula to find the similarity between the input vector and the map's node's weight vector\n",
    "2.Track the node that produces the smallest distance (this node is the best matching unit, BMU)\n",
    "\n",
    "4.Update the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector \n",
    "1.W v   ( s + 1 ) = W v   ( s ) + θ ( u , v , s ) ⋅ α ( s ) ⋅ ( D ( t ) − W v   ( s ) )  \n",
    "5.Increase s and repeat from step 2 while s < λ  \n",
    "\n",
    "A variant algorithm:\n",
    "1.Randomize the map's nodes' weight vectors\n",
    "2.Traverse each input vector in the input data set 1.Traverse each node in the map 1.Use the Euclidean distance formula to find the similarity between the input vector and the map's node's weight vector\n",
    "2.Track the node that produces the smallest distance (this node is the best matching unit, BMU)\n",
    "\n",
    "2.Update the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector \n",
    "\n",
    "1.W v   ( s + 1 ) = W v   ( s ) + θ ( u , v , s ) ⋅ α ( s ) ⋅ ( D ( t ) − W v   ( s ) )   \n",
    "\n",
    "3.Increase s and repeat from step 2 while s < λ  \n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R\n",
    "\n",
    "# https://www.r-bloggers.com/self-organising-maps-for-customer-segmentation-using-r/\n",
    "# Load the kohonen package \n",
    "require(kohonen)\n",
    "\n",
    "# Create a training data set (rows are samples, columns are variables\n",
    "# Here I am selecting a subset of my variables available in \"data\"\n",
    "data_train <- data[, c(2,4,5,8)]\n",
    "\n",
    "# Change the data frame with training data to a matrix\n",
    "# Also center and scale all variables to give them equal importance during\n",
    "# the SOM training process. \n",
    "data_train_matrix <- as.matrix(scale(data_train))\n",
    "\n",
    "# Create the SOM Grid - you generally have to specify the size of the \n",
    "# training grid prior to training the SOM. Hexagonal and Circular \n",
    "# topologies are possible\n",
    "som_grid <- somgrid(xdim = 20, ydim=20, topo=\"hexagonal\")\n",
    "\n",
    "# Finally, train the SOM, options for the number of iterations,\n",
    "# the learning rates, and the neighbourhood are available\n",
    "som_model <- som(data_train_matrix, \n",
    "                 grid=som_grid, \n",
    "                 rlen=100, \n",
    "                 alpha=c(0.05,0.01), \n",
    "                 keep.data = TRUE,\n",
    "                 n.hood=“circular” )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Python\n",
    "\n",
    "# http://peterwittek.com/somoclu-in-python.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import somoclu\n",
    "%matplotlib inline  \n",
    "c1 = np.random.rand(50, 3)/5\n",
    "c2 = (0.6, 0.1, 0.05) + np.random.rand(50, 3)/5\n",
    "c3 = (0.4, 0.1, 0.7) + np.random.rand(50, 3)/5\n",
    "data = np.float32(np.concatenate((c1, c2, c3)))\n",
    "colors = [\"red\"] * 50\n",
    "colors.extend([\"green\"] * 50)\n",
    "colors.extend([\"blue\"] * 50)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=colors)\n",
    "labels = range(150)\n",
    "\n",
    "n_rows, n_columns = 100, 160\n",
    "som = somoclu.Somoclu(n_columns, n_rows, data=data)\n",
    "%time som.train()\n",
    "som.view_component_planes()\n",
    "# More.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Locally Weighted Learning (LWL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "http://www.ias.informatik.tu-darmstadt.de/uploads/Teaching/AutonomousLearningSystems/Englert_ALS_2012.pdf\n",
    "\n",
    "The key concept here is to approximate nonlinear functions by means of piecewise linear models. Locally Weighted Learning is a class of function approximation techniques, where a prediction is done by using an approximated local model around the current point of interest. The goal of function approximation and regression is to find the underlying relationship between input and output. In a supervised learning problem training data, where each input is associated to one output, is used to create a model that predicts values which come close to the true function.\n",
    "One basic approach for solving this problem is to build a global model out of labeled training data. Examples for global methods are Support Vector Machine, Neural Networks and Gaussian Process Regression. All of these methods have in common that they use the complete training data for creating a global function. This approximated function is used to predict values that come close to the corresponding true value of the original function. A disadvantage of global methods is that sometimes no parameter values can provide a sufficient good approximation. Furthermore, the computational costs are for some tasks very high, i.e., if the task needs many predictions in short time or the model is extended incrementally. An alternative to global function approximation is Locally Weighted Learning (LWL) [2]. LWL methods are non-parametric and the current prediction is done by local functions which are using only a subset of the data. The basic idea behind LWL is that instead of building a global model for the whole function space, for each point of interest a local model is created based on neighboring data of the query point (cf. figure 1). For this purpose each data point becomes a weighting factor which expresses the influence of the data point for the prediction.\n",
    "In general, data points which are in the close neighborhood to the current query point are receiving a higher weight than data points which are far away. LWL is also called lazy learning, because the processing of the training data is shifted until a query point needs to be answered. This approach makes LWL a very accurate function approximation method where it is easy to add new training points.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "\n",
    "#### Initial Parameters:\n",
    "\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "http://www.ias.informatik.tu-darmstadt.de/uploads/Teaching/AutonomousLearningSystems/Englert_ALS_2012.pdf\n",
    "\n",
    "Memory-Based Locally Weighted Regression (LWR) – memory-based LWL methods where all training data is kept in memory.\n",
    "Locally Weighted Projection Regression (LWPR) – purely incremental LWL methods that do not need to remember any data explicitly.(computation efficient with high dimensional data / use PLS reduce D locally – robust to redundant, irrelevant data)\n",
    "\n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R\n",
    "\n",
    "# https://gist.github.com/felixr/5564929\n",
    "install.packages(\"rdyncall\")\n",
    "\n",
    "library(rdyncall)\n",
    "\n",
    "dynbind(\"lwpr\", \"\n",
    "        lwpr_init_model(*<LWPR_Model>iiZ)i;\n",
    "        lwpr_duplicate_mode(*<LWPR_Model>*<LWPR_Model>)i;\n",
    "        lwpr_set_init_alpha(*<LWPR_Model>d)i;\n",
    "        lwpr_set_init_D(*<LWPR_Model>*dd)i;\n",
    "        lwpr_set_init_D_diagonal(*<LWPR_Model>*d)i;\n",
    "        lwpr_set_init_D_spherical(*<LWPR_Model>d)i;\n",
    "        lwpr_update(*<LWPR_Model>*d*d*d*d)i;\n",
    "        lwpr_predict(*<LWPR_Model>*dd*d*d*d)v;\n",
    "        lwpr_write_xml(*<LWPR_Model>Z)i;\n",
    "        lwpr_read_xml(*<LWPR_Model>Z*i)i;\n",
    "        \")\n",
    "\n",
    "parseStructInfos(\"LWPR_Model{iiii*d*d*ciidd*d*d*d*d*ddddddddiipp*d*d*d}nIn nInStore nOut n_data mean_x var_x name diag_only meta meta_rate penalty init_alpha norm_in norm_out init_D init_M w_gen w_prune init_lambda final_lambda tau_lambda init_S2 add_threshold kernel update_D sub ws storage xn yn\")\n",
    "\n",
    "testfunc = function(x) {\n",
    "  10 * sin(7.8*log(1+x)) / (1 + 0.1*x**2)\n",
    "}\n",
    "\n",
    "Ntr = 500\n",
    "Xtr = 10 * runif(Ntr)\n",
    "Ytr = sapply(Xtr, testfunc) + 0.1 * rnorm(Ntr) * Xtr\n",
    "\n",
    "model = new.struct(LWPR_Model)\n",
    "lwpr_init_model(model, 1, 1, \"Tutorial\") \n",
    "\n",
    "lwpr_set_init_D(model, 20, 0)\n",
    "model$update_D = 1\n",
    "model$diag_only = TRUE\n",
    "model$penalty = 0.0001\n",
    "lwpr_set_init_alpha(model, 40)\n",
    "\n",
    "for (i in 1:20) {\n",
    "  idx = sample(Ntr,Ntr)\n",
    "  for (j in idx) {\n",
    "    yp=0\n",
    "    lwpr_update(model, Xtr[j], Ytr[j], yp, NULL)\n",
    "  }\n",
    "}\n",
    "\n",
    "Ntest = 500;\n",
    "Xtest = seq(0,10,length.out=Ntest)\n",
    "Ytest = numeric(Ntest)\n",
    "Conf = numeric(Ntest) \n",
    "for (k in 1:Ntest) {\n",
    "  yp = 0\n",
    "  conf = 0\n",
    "  lwpr_predict(model, Xtest[k], 0.0, yp, conf, NULL);\n",
    "  Ytest[k] = yp\n",
    "  Conf[k]  = conf\n",
    "}\n",
    "\n",
    "plot(Ytr ~ Xtr, pch=19, cex=0.5, col='red')\n",
    "lines(Xtest , Ytest, col=\"blue\") \n",
    "lines(Xtest , Ytest + Conf, col=\"cyan\") \n",
    "lines(Xtest , Ytest - Conf, col=\"cyan\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Python\n",
    "\n",
    "# http://www.rueckstiess.net/research/snippets/show/9bd4b418\n",
    "from numpy import *\n",
    "from matplotlib import pyplot as plt\n",
    "from lwpr import LWPR\n",
    " \n",
    "def testfunc(x):\n",
    "    return 10*sin(7.8*log(1+x)) / (1 + 0.1*x**2)\n",
    " \n",
    " \n",
    "Ntr = 500\n",
    "Xtr = 10 * random.random((Ntr, 1))\n",
    "Ytr = 5 + testfunc(Xtr) + 0.1 * random.normal(0, 1, (Ntr, 1)) * Xtr\n",
    " \n",
    "# initialize the LWPR model\n",
    "model = LWPR(1, 1)     \n",
    "model.init_D = 20 * eye(1)\n",
    "model.update_D = True\n",
    "model.init_alpha = 40 * eye(1)\n",
    "model.meta = False\n",
    " \n",
    "# train the model\n",
    "for k in range(20):\n",
    "    ind = random.permutation(Ntr)\n",
    "    mse = 0\n",
    "     \n",
    "    for i in range(Ntr):\n",
    "        yp = model.update(Xtr[ind[i]], Ytr[ind[i]])\n",
    "        mse = mse + (Ytr[ind[i], :] - yp)**2\n",
    " \n",
    "    nMSE = mse/Ntr/var(Ytr)\n",
    "    print \"#Data: %5i  #RFs: %3i  nMSE=%5.3f\"%(model.n_data, model.num_rfs, nMSE)\n",
    "             \n",
    "             \n",
    "# test the model with unseen data   \n",
    "Ntest = 500\n",
    "Xtest = linspace(0, 10, Ntest)\n",
    " \n",
    "Ytest = zeros((Ntest,1))\n",
    "Conf = zeros((Ntest,1))\n",
    " \n",
    "for k in range(500):\n",
    "    Ytest[k,:], Conf[k,:] = model.predict_conf(array([Xtest[k]]))\n",
    " \n",
    "plt.plot(Xtr, Ytr, 'r.')\n",
    "     \n",
    "plt.plot(Xtest,Ytest,'b-')\n",
    "plt.plot(Xtest,Ytest+Conf,'c-', linewidth=2)\n",
    "plt.plot(Xtest,Ytest-Conf,'c-', linewidth=2)\n",
    " \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
