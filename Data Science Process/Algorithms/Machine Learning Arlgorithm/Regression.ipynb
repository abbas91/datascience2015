{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In statistical modeling, regression analysis is a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. In all cases, the estimation target is a function of the independent variables called the regression function. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution.\n",
    "\n",
    "Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer causal relationships between the independent and dependent variables. However this can lead to illusions or false relationships, so caution is advisable;[2] for example, correlation does not imply causation.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "##### Ordinary Least Squares Regression (OLSR) - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "##### Linear Regression - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "##### Binary Logistic Regression - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "##### Multinomial Logistic Regression - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "##### Stepwise Regression - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "##### Multivariate Adaptive Regression Splines (MARS) - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "##### Locally Estimated Scatterplot Smoothing (LOSS) - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "##### Jackknife Regression - \n",
    "\n",
    "Pros: \n",
    "\n",
    "Cons: \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Ordinary Least Squares Regression (OLSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "http://www.theanalysisfactor.com/interpreting-regression-coefficients/\n",
    "https://en.wikipedia.org/wiki/Linear_regression\n",
    "\n",
    "In statistics, ordinary least squares (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, with the goal of minimizing the sum of the squares of the differences between the observed responses in the given dataset and those predicted by a linear function of a set of explanatory variables (visually this is seen as the sum of the vertical distances between each data point in the set and the corresponding point on the regression line – the smaller the differences, the better the model fits the data).\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "OLS (Ordinary Least Squared) Estimator\n",
    "#### Process Flow: \n",
    "Adjust parameters to minimize the residuals (Y-Ypred)^2; Fitting a straight line\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html\n",
    "lm(formula = y ~ x1 + x2, data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "# http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=0ahUKEwjAzf6q39jRAhXK5IMKHbsDDjIQFggaMAA&url=http%3A%2F%2Fscikit-learn.org%2Fstable%2Fauto_examples%2Flinear_model%2Fplot_ols.html&usg=AFQjCNGznqftMY9pFJOOT9Agq2t9HqMubw&bvm=bv.144686652,d.amc\n",
    "# Estimate coefficient of a regression model \n",
    "from sklearn.linear_model import LinearRegression \n",
    "slr = LinearRegression() \n",
    "slr.fit(x, y) \n",
    "slr.coef_ # all parameters \n",
    "slr.intercept_ # intercept\n",
    "\n",
    "# How to evaluate a regression model \n",
    "# -- Residual plots -> non-linearity / outliers (Nonlinear pattern? center by y = 0?) \n",
    "plt.scatter(Y_train_pred, Y_train_pred - Y_train,  \n",
    " \t        c='bule', marker='o', label='Training data') \n",
    "plt.scatter(Y_test_pred, Y_test_pred - Y_test,  \n",
    " \t        c='lightgreen', marker='s', label='Test data') \n",
    "plt.xlabel('Predicted Values') \n",
    "plt.ylabel('Residuals') \n",
    "plt.legend(loc='upper left') \n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red') \n",
    "plt.xlim([-10, 50]) \n",
    "plt.show() \n",
    " \n",
    "# -- Use MSE(Mean Squared Error) \n",
    "from sklearn.metrics import mean_squared_error \n",
    "mean_squared_error(Y_train, Y_train_pred) # Trainning MSE \n",
    "mean_squared_error(Y_test, Y_test_pred) # Test MSE \n",
    "\n",
    "# -- R square \n",
    "from sklearn.metrics import r2_score \n",
    "r2_score(Y_train, Y_train_pred) \n",
    "r2_score(Y_test, Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "http://www.theanalysisfactor.com/interpreting-regression-coefficients/\n",
    "https://en.wikipedia.org/wiki/Linear_regression\n",
    "\n",
    "On the other hand, linear regression is a statistical inference problem. The \"y values\" take on the interpretation of data you wish to model, and the \"x values\" take on the interpretation of extra information you have about each data point that might be helpful in predicting their \"y values\". You are trying to build a probabilistic model that describes \"y\" while taking into account \"x\", and a linear model is one of many ways to do this. A linear model assumes that \"y\" has a different mean for each possible value of \"x\", and that these means happen to follow a straight line with a certain intercept and a certain slope. As with any statistical inference problem, you estimate the unknown parameters using maximum likelihood estimation. But since in this case the unknown parameters are an intercept and a slope, the end result of maximum likelihood estimation is basically that you are choosing a straight line that fits the observed data best, so this essentially becomes the curve fitting problem discussed above.\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "Maximum Likelihood Estimator\n",
    "#### Process Flow: \n",
    "When solving the statistical linear regression problem, a very common modeling assumption is that for every possible value of \"x\", the quantity \"y\" is normally distributed with a mean that is linear in \"x\". Therefore, the likelihood function is essentially a product of PDFs of the normal distribution. As stated above, you estimate the unknown parameters (and therefore find the best fitting line) by maximizing the likelihood function. If you look at what the product of normal PDFs looks like, you will notice that maximizing this expression happens to be equivalent to... you guessed it... minimizing the sum of squared errors. That is, the line you get performing curve fitting via least squares is equivalent to the line you get performing linear regression using a normal model.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# Same Above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# Same Above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics, logistic regression, or logit regression, or logit model[1] is a regression model where the dependent variable (DV) is categorical. This article covers the case of a binary dependent variable—that is, where it can take only two values, \"0\" and \"1\", which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick. Binary Logistic Regression is a special type of regression where binary response variable is related to a set of explanatory variables, which can be discrete and/or continuous. The important point here to note is that in linear regression, the expected values of the response variable are modeled based on combination of values taken by the predictors. In logistic regression Probability or Odds of the response taking a particular value is modeled based on combination of values taken by the predictors. Like regression (and unlike log-linear models that we will see later), we make an explicit distinction between a response variable and one or more predictor (explanatory) variables.\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "OLS (Ordinary Least Squared) Estimator\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://stat.ethz.ch/R-manual/R-patched/library/stats/html/glm.html\n",
    "train <- data[1:800,]\n",
    "test <- data[801:889,]\n",
    "model <- glm(Survived ~.,family=binomial(link='logit'),data=train)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=0ahUKEwiYl5aW4djRAhVs3IMKHXMkB80QFggaMAA&url=http%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fgenerated%2Fsklearn.linear_model.LogisticRegression.html&usg=AFQjCNGpSyUzpbaClG8IQEPJmB63CQZlrg&bvm=bv.144686652,d.eWE\n",
    "# load lib {numpy, sklearn} \n",
    "from sklearn import datasets \n",
    "import numpy as np \n",
    " \n",
    "# loading datasets \n",
    "isir = datasets.load_iris() \n",
    "X = iris.data[:,[2,3]] \n",
    "y = iris.target \n",
    "# Spliting datasets \n",
    "from sklearn.cross_validation import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n",
    "# Scaling data \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() # - define scaler object \n",
    "sc.fit(X_train) # fit the object with data to get meansure \n",
    "X_train_std = sc.transform(X_train) # scale data \n",
    "X_test_std = sc.transform(X_test) # scale data \n",
    "# Fitting Model \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "lr = LogisticRegression(C=1000.0, random_state=0) # C {penality parameter} \n",
    "lr.fit(X_train_std, y_train) \n",
    "lr.predict_proba(X_test_std[0,:]) # P() of predict on one sample \n",
    "\n",
    "\" array([[ 0.000, 0.063, 0.937 ]]) \" # three classes p()s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
    "\n",
    "In statistics, multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes.[1] That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.). Multinomial logistic regression is used when the dependent variable in question is nominal (equivalently categorical, meaning that it falls into any one of a set of categories which cannot be ordered in any meaningful way) and for which there are more than two categories. \n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=0ahUKEwims-Xa4djRAhWH3oMKHXyIApEQFggaMAA&url=http%3A%2F%2Fwww.ats.ucla.edu%2Fstat%2Fr%2Fdae%2Fmlogit.htm&usg=AFQjCNH4Bx0iybTQM0u6mYPcfypfB050Eg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=0ahUKEwiYl5aW4djRAhVs3IMKHXMkB80QFggaMAA&url=http%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fgenerated%2Fsklearn.linear_model.LogisticRegression.html&usg=AFQjCNGpSyUzpbaClG8IQEPJmB63CQZlrg&bvm=bv.144686652,d.eWE\n",
    "# load lib {numpy, sklearn} \n",
    "from sklearn import datasets \n",
    "import numpy as np \n",
    " \n",
    "# loading datasets \n",
    " isir = datasets.load_iris() \n",
    "X = iris.data[:,[2,3]] \n",
    "y = iris.target \n",
    "# Spliting datasets \n",
    "from sklearn.cross_validation import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n",
    "# Scaling data \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() # - define scaler object \n",
    "sc.fit(X_train) # fit the object with data to get meansure \n",
    "X_train_std = sc.transform(X_train) # scale data \n",
    "X_test_std = sc.transform(X_test) # scale data \n",
    "# Fitting Model \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "lr = LogisticRegression(C=1000.0, random_state=0) # C {penality parameter}  / Find ‘multinomial’ option\n",
    "lr.fit(X_train_std, y_train) \n",
    "lr.predict_proba(X_test_std[0,:]) # P() of predict on one sample \n",
    "\" array([[ 0.000, 0.063, 0.937 ]]) \" # three classes p()s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Multivariate Adaptive Regression Splines (MARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991.[1] It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "MARS builds a model in two phases: the forward and the backward pass. This two-stage approach is the same as that used by recursive partitioning trees.\n",
    "The forward pass:\n",
    "MARS starts with a model which consists of just the intercept term (which is the mean of the response values). MARS then repeatedly adds basis function in pairs to the model. At each step it finds the pair of basis functions that gives the maximum reduction in sum-of-squares residual error (it is a greedy algorithm). The two basis functions in the pair are identical except that a different side of a mirrored hinge function is used for each function. Each new basis function consists of a term already in the model (which could perhaps be the intercept term) multiplied by a new hinge function. A hinge function is defined by a variable and a knot, so to add a new basis function, MARS must search over all combinations of the following:\n",
    "\n",
    "1 existing terms (called parent terms in this context)\n",
    "\n",
    "2 all variables (to select one for the new basis function)\n",
    "\n",
    "3 all values of each variable (for the knot of the new hinge function).\n",
    "\n",
    "To calculate the coefficient of each term MARS applies a linear regression over the terms.\n",
    "This process of adding terms continues until the change in residual error is too small to continue or until the maximum number of terms is reached. The maximum number of terms is specified by the user before model building starts.\n",
    "The search at each step is done in a brute force fashion, but a key aspect of MARS is that because of the nature of hinge functions the search can be done relatively quickly using a fast least-squares update technique. Actually, the search is not quite brute force. The search can be sped up with a heuristic that reduces the number of parent terms to consider at each step (\"Fast MARS\" [4]).\n",
    "\n",
    "The backward pass:\n",
    "The forward pass usually builds an overfit model. (An overfit model has a good fit to the data used to build the model but will not generalize well to new data.) To build a model with better generalization ability, the backward pass prunes the model. It removes terms one by one, deleting the least effective term at each step until it finds the best submodel. Model subsets are compared using the GCV criterion described below.\n",
    "\n",
    "The backward pass has an advantage over the forward pass: at any step it can choose any term to delete, whereas the forward pass at each step can only see the next pair of terms.\n",
    "The forward pass adds terms in pairs, but the backward pass typically discards one side of the pair and so terms are often not seen in pairs in the final model. A paired hinge can be seen in the equation for y ^ in the first MARS example above; there are no complete pairs retained in the ozone example.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://cran.r-project.org/web/packages/earth/earth.pdf\n",
    "Earth(y ~ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# https://github.com/scikit-learn-contrib/py-earth\n",
    "git clone git://github.com/scikit-learn-contrib/py-earth.git\n",
    "cd py-earth\n",
    "sudo python setup.py install\n",
    "\n",
    "import numpy\n",
    "from pyearth import Earth\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Create some fake data\n",
    "numpy.random.seed(0)\n",
    "m = 1000\n",
    "n = 10\n",
    "X = 80*numpy.random.uniform(size=(m,n)) - 40\n",
    "y = numpy.abs(X[:,6] - 4.0) + 1*numpy.random.normal(size=m)\n",
    "\n",
    "#Fit an Earth model\n",
    "model = Earth()\n",
    "model.fit(X,y)\n",
    "\n",
    "#Print the model\n",
    "print(model.trace())\n",
    "print(model.summary())\n",
    "\n",
    "#Plot the model\n",
    "y_hat = model.predict(X)\n",
    "pyplot.figure()\n",
    "pyplot.plot(X[:,6],y,'r.')\n",
    "pyplot.plot(X[:,6],y_hat,'b.')\n",
    "pyplot.xlabel('x_6')\n",
    "pyplot.ylabel('y')\n",
    "pyplot.title('Simple Earth Example')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Locally Estimated Scatterplot Smoothing (LOSS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "https://en.wikipedia.org/wiki/Local_regression\n",
    "\n",
    "LOESS and LOWESS (locally weighted scatterplot smoothing) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model. \"LOESS\" is a later generalization of LOWESS; although it is not a true initialism, it may be understood as standing for \"LOcal regrESSion\".\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "LOESS and LOWESS thus build on \"classical\" methods, such as linear and nonlinear least squares regression. They address situations in which the classical procedures do not perform well or cannot be effectively applied without undue labor. LOESS combines much of the simplicity of linear least squares regression with the flexibility of nonlinear regression. It does this by fitting simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point. In fact, one of the chief attractions of this method is that the data analyst is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data.\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n",
    "# https://stat.ethz.ch/R-manual/R-devel/library/stats/html/loess.html\n",
    "loess(formula, data, weights, subset, na.action, model = FALSE,\n",
    "      span = 0.75, enp.target, degree = 2,\n",
    "      parametric = FALSE, drop.square = FALSE, normalize = TRUE,\n",
    "      family = c(\"gaussian\", \"symmetric\"),\n",
    "      method = c(\"loess\", \"model.frame\"),\n",
    "      control = loess.control(...), ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n",
    "# http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=0ahUKEwijq5245djRAhWk0YMKHdnjA9MQFggmMAI&url=http%3A%2F%2Fstackoverflow.com%2Fquestions%2F36252434%2Fpredicting-on-new-data-using-locally-weighted-regression-loess-lowess&usg=AFQjCNEcjlI4A3t8r06qIerqkcQn--gmpA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------- Jackknife Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation: \n",
    "In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size N   {\\displaystyle N}  N, the jackknife estimate is found by aggregating the estimates of each N-1-sized sub-sample. Perform OLS regression on those estimators.\n",
    "#### Input Data: \n",
    "X(Numeric)/X(Categorical)\n",
    "#### Initial Parameters: \n",
    "NA\n",
    "#### Cost Function: \n",
    "\n",
    "#### Process Flow: \n",
    "\n",
    "#### Evaluation Methods: \n",
    "\n",
    "#### Tips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- R Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------- Python Code\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
