{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "xxxxxx\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### K-Nearest Neighbour (KNN)\n",
    "\n",
    "Pros: Simple to implement; Flexible to feature / distance choices; Naturally handles multi-class cases; Can do well in practice with enough representative data; Robust to outlier; No assumptions;\n",
    "\n",
    "Cons: Computation intensive; Storage of data; must know meaningful distance; Sensitive to local patterns; Totally based on your training on your entire train data;\n",
    "\n",
    "#### Learning Vector Quantization (LVQ)\n",
    "\n",
    "Pros: Support binary and multiple class predict; The algorithm complexity can be adjusted during training as needed (# of nodes); Non-parametric model more accurate; \n",
    "\n",
    "Cons: Choose meaningful distance metrics; Computational intensive; \n",
    "\n",
    "#### Self-Organizing Map (SOM) \n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n",
    "#### Locally Weighted Learning (LWL)\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cons:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- K-Nearest Neighbour (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "k nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups.\n",
    "In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n",
    "In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n",
    "Both for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.[2]\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "\n",
    "#### Initial Parameters:\n",
    "K – nearest neighbors (Trade-off between linear or fit)\n",
    "#### Cost Function:\n",
    "Calculate Euclidean Distance\n",
    "#### Process Flow:\n",
    "The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n",
    "In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n",
    "A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has also been employed with correlation coefficients such as Pearson and Spearman.[3] Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.\n",
    "A drawback of the basic \"majority voting\" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number.[4] One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM.\n",
    "#### Tips:\n",
    "Choosing the number of nearest neighbors i.e. determining the value of k plays a significant role in determining the efficacy of the model. Thus, selection of k will determine how well the data can be utilized to generalize the results of the kNN algorithm. A large k value has benefits which include reducing the variance due to the noisy data; the side effect being developing a bias due to which the learner tends to ignore the smaller patterns which may have useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R\n",
    "\n",
    "# Classification\n",
    "# https://www.analyticsvidhya.com/blog/2015/08/learning-concept-knn-algorithms-programming/\n",
    "prc <- read.csv(\"Prostate_Cancer.csv\",stringsAsFactors = FALSE)    #This command imports the required data \n",
    "prc <- prc[-1]  #removes the first variable(id) from the data set. set and saves it to the prc data frame.\n",
    "prc$diagnosis <- factor(prc$diagnosis_result, levels = c(\"B\", \"M\"), labels = c(\"Benign\", \"Malignant\"))\n",
    "round(prop.table(table(prc$diagnosis)) * 100, digits = 1)  # it gives the result in the percentage form rounded of to 1 decimal place( and so it’s digits = 1)\n",
    "normalize <- function(x) {\n",
    "return ((x - min(x)) / (max(x) - min(x))) }\n",
    "prc_n <- as.data.frame(lapply(prc[2:9], normalize))\n",
    "prc_train <- prc_n[1:65,]\n",
    "prc_test <- prc_n[66:100,]\n",
    "prc_train_labels <- prc[1:65, 1]\n",
    "prc_test_labels <- prc[66:100, 1]   #This code takes the diagnosis factor in column 1 of the prc data frame and on turn creates prc_train_labels and prc_test_labels data frame.\n",
    "install.packages(“class”)\n",
    "library(class)\n",
    "prc_test_pred <- knn(train = prc_train, test = prc_test,cl = prc_train_labels, k=10)\n",
    "install.packages(\"gmodels\")\n",
    "library(gmodel)\n",
    "CrossTable(x=prc_test_labels, y=prc_test_pred, prop.chisq=FALSE)\n",
    "\n",
    "# Regression\n",
    "# https://artax.karlin.mff.cuni.cz/r-help/library/FNN/html/knn.reg.html\n",
    "require(chemometrics)\n",
    "data(PAC);\n",
    "pac.knn<- knn.reg(PAC$X, y=PAC$y, k=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Pythom\n",
    "\n",
    "# Classification\n",
    "# load lib {numpy, sklearn} \n",
    "from sklearn import datasets \n",
    "import numpy as np \n",
    "# loading datasets \n",
    "isir = datasets.load_iris() \n",
    "X = iris.data[:,[2,3]] \n",
    "y = iris.target \n",
    "# Spliting datasets \n",
    "from sklearn.cross_validation import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n",
    "# Scaling data \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() # - define scaler object \n",
    "sc.fit(X_train) # fit the object with data to get meansure \n",
    "X_train_std = sc.transform(X_train) # scale data \n",
    "X_test_std = sc.transform(X_test) # scale data \n",
    "# KNN Classifier \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski') # p = 1 {manhatten Dist} : p = 2 {Euclidean}  \n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "# Regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "for i, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n",
    "    y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "    plt.subplot(2, 1, i + 1)\n",
    "    plt.scatter(X, y, c='k', label='data')\n",
    "    plt.plot(T, y_, c='g', label='prediction')\n",
    "    plt.axis('tight')\n",
    "    plt.legend()\n",
    "    plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors,\n",
    "                                                                weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Learning Vector Quantization (LVQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "https://www.researchgate.net/publication/259486415_A_review_of_learning_vector_quantization_classifiers\n",
    "http://www.cs.rug.nl/~biehl/Preprints/wsom07lvq.pdf\n",
    "\n",
    "In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "\n",
    "#### Initial Parameters:\n",
    "Initial weights; Number of output nodes; K(default = 1, competitive learning); Learning rate(Update steps); \n",
    "#### Cost Function:\n",
    "D(X,J) – Only single X *Competitive, winner-takes-all*\n",
    "#### Process Flow:\n",
    "http://ccy.dd.ncu.edu.tw/~chen/course/neural/ch4/index.htm\n",
    "First initialize the M weights (initial M feature values) for J output nodes (Randomly assign J output classes to J nodes) -> For each (Like KNN but use K=1 so evaluate each) training example X with M feature values (Total X * n) -> Find the Jth node that minimize the D(X,J) *Usually Euclidean distance* -> Update the M weights for that Jth node by IF[True X class == Jth node class, update the M weights to move the Jth node close to X] ELSE [True X class != Jth node class, update the M weights to move the Jth node away from X] given the update scale based on “learning rate”. -> Repeat the process literately throughout the training set repeatly-> Stop if stopping criteria meets or max repeat time meets  \n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R\n",
    "\n",
    "# Classification\n",
    "# Find LVQ1,2,3 in https://cran.r-project.org/web/packages/class/class.pdf\n",
    "# http://astrostatistics.psu.edu/su07/R/html/class/html/lvq3.html\n",
    "# LVQ3\n",
    "data(iris3)\n",
    "train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])\n",
    "test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])\n",
    "cl <- factor(c(rep(\"s\",25), rep(\"c\",25), rep(\"v\",25)))\n",
    "cd <- lvqinit(train, cl, 10)\n",
    "lvqtest(cd, train)\n",
    "cd0 <- olvq1(train, cl, cd)\n",
    "lvqtest(cd0, train)\n",
    "cd3 <- lvq3(train, cl, cd0)\n",
    "lvqtest(cd3, train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Python\n",
    "\n",
    "# http://mnemstudio.org/ai/nn/lvq_python_ex1.txt\n",
    "# Another – https://pythonhosted.org/neurolab/ex_newlvq.html\n",
    "\"\"\"\n",
    "Example of use LVQ network\n",
    "==========================\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import neurolab as nl\n",
    "\n",
    "# Create train samples\n",
    "input = np.array([[-3, 0], [-2, 1], [-2, -1], [0, 2], [0, 1], [0, -1], [0, -2], \n",
    "                                                        [2, 1], [2, -1], [3, 0]])\n",
    "target = np.array([[1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], \n",
    "                                                        [1, 0], [1, 0], [1, 0]])\n",
    "\n",
    "# Create network with 2 layers:4 neurons in input layer(Competitive)\n",
    "# and 2 neurons in output layer(liner)\n",
    "net = nl.net.newlvq(nl.tool.minmax(input), 4, [.6, .4])\n",
    "# Train network\n",
    "error = net.train(input, target, epochs=1000, goal=-1)\n",
    "\n",
    "# Plot result\n",
    "import pylab as pl\n",
    "xx, yy = np.meshgrid(np.arange(-3, 3.4, 0.2), np.arange(-3, 3.4, 0.2))\n",
    "xx.shape = xx.size, 1\n",
    "yy.shape = yy.size, 1\n",
    "i = np.concatenate((xx, yy), axis=1)\n",
    "o = net.sim(i)\n",
    "grid1 = i[o[:, 0]>0]\n",
    "grid2 = i[o[:, 1]>0]\n",
    "\n",
    "class1 = input[target[:, 0]>0]\n",
    "class2 = input[target[:, 1]>0]\n",
    "\n",
    "pl.plot(class1[:,0], class1[:,1], 'bo', class2[:,0], class2[:,1], 'go')\n",
    "pl.plot(grid1[:,0], grid1[:,1], 'b.', grid2[:,0], grid2[:,1], 'gx')\n",
    "pl.axis([-3.2, 3.2, -3, 3])\n",
    "pl.xlabel('Input[:, 0]')\n",
    "pl.ylabel('Input[:, 1]')\n",
    "pl.legend(['class 1', 'class 2', 'detected class 1', 'detected class 2'])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Self-Organizing Map (SOM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "\n",
    "#### Initial Parameters:\n",
    "\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "\n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- Locally Weighted Learning (LWL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki Definitation:\n",
    "\n",
    "#### Input Data:\n",
    "X(Numeric)/X(Categorical)\n",
    "\n",
    "#### Initial Parameters:\n",
    "\n",
    "#### Cost Function:\n",
    "\n",
    "#### Process Flow:\n",
    "\n",
    "#### Tips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------- P"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
